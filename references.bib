
@article{DworkAlgorithmicFoundationsDifferential2014,
  title = {The Algorithmic Foundations of Differential Privacy},
  volume = {9},
  issn = {1551-305X, 1551-3068},
  doi = {10.1561/0400000042},
  abstract = {The Algorithmic Foundations of Differential Privacy},
  language = {English},
  number = {3\textendash{}4},
  journal = {Foundations and Trends\textregistered{} in Theoretical Computer Science},
  author = {Dwork, Cynthia and Roth, Aaron},
  month = aug,
  year = {2014},
  pages = {211-407},
  file = {/home/roshan/Documents/Zotero/storage/3F3JEV8K/privacybook.pdf;/home/roshan/Documents/Zotero/storage/TIR3JKCH/TCS-042.html}
}

@article{VershyninRandomMatrices2010,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1011.3027},
  primaryClass = {cs, math},
  title = {Introduction to the Non-Asymptotic Analysis of Random Matrices},
  abstract = {This is a tutorial on some basic non-asymptotic methods and concepts in random matrix theory. The reader will learn several tools for the analysis of the extreme singular values of random matrices with independent rows or columns. Many of these methods sprung off from the development of geometric functional analysis since the 1970's. They have applications in several fields, most notably in theoretical computer science, statistics and signal processing. A few basic applications are covered in this text, particularly for the problem of estimating covariance matrices in statistics and for validating probabilistic constructions of measurement matrices in compressed sensing. These notes are written particularly for graduate students and beginning researchers in different areas, including functional analysts, probabilists, theoretical statisticians, electrical engineers, and theoretical computer scientists.},
  journal = {arXiv:1011.3027 [cs, math]},
  author = {Vershynin, Roman},
  month = nov,
  year = {2010},
  keywords = {60B20; 46B09,Computer Science - Numerical Analysis,Mathematics - Functional Analysis,Mathematics - Probability},
  file = {/home/roshan/Documents/Zotero/storage/IJHH6KJN/Vershynin - 2010 - Introduction to the non-asymptotic analysis of ran.pdf;/home/roshan/Documents/Zotero/storage/H52RCB9Z/1011.html}
}

@book{TaoRandomMatrixTheory2012,
  title = {Topics in Random Matrix Theory},
  volume = {132},
  publisher = {{American Mathematical Society Providence, RI}},
  author = {Tao, Terence},
  year = {2012},
  file = {/home/roshan/Documents/Zotero/storage/MSPU8UY4/Tao - 2012 - Topics in random matrix theory.pdf}
}

@article{SheffetPrivateApproxRegression2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1507.00056},
  primaryClass = {cs},
  title = {Private Approximations of the 2nd-Moment Matrix Using Existing Techniques in Linear Regression},
  abstract = {We introduce three differentially-private algorithms that approximates the 2nd-moment matrix of the data. These algorithm, which in contrast to existing algorithms output positive-definite matrices, correspond to existing techniques in linear regression literature. Specifically, we discuss the following three techniques. (i) For Ridge Regression, we propose setting the regularization coefficient so that by approximating the solution using Johnson-Lindenstrauss transform we preserve privacy. (ii) We show that adding a small batch of random samples to our data preserves differential privacy. (iii) We show that sampling the 2nd-moment matrix from a Bayesian posterior inverse-Wishart distribution is differentially private provided the prior is set correctly. We also evaluate our techniques experimentally and compare them to the existing "Analyze Gauss" algorithm of Dwork et al.},
  journal = {arXiv:1507.00056 [cs]},
  author = {Sheffet, Or},
  month = jun,
  year = {2015},
  keywords = {Computer Science - Data Structures and Algorithms},
  file = {/home/roshan/Documents/Zotero/storage/RYD8W43B/Sheffet - 2015 - Private Approximations of the 2nd-Moment Matrix Us.pdf;/home/roshan/Documents/Zotero/storage/QWBQPVQU/1507.html}
}

@inproceedings{AbbasiYadkoriImprovedAlgorithmsLinear2011,
  title = {Improved Algorithms for Linear Stochastic Bandits},
  volume = {24},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  publisher = {{Curran Associates, Inc.}},
  author = {Abbasi-Yadkori, Yasin and P{\'a}l, D{\'a}vid and Szepesv{\'a}ri, Csaba},
  editor = {Shawe-Taylor, J. and Zemel, R. S. and Bartlett, P. L. and Pereira, F. and Weinberger, K. Q.},
  year = {2011},
  pages = {2312--2320},
  file = {/home/roshan/Documents/Zotero/storage/JMZWP5VH/Abbasi-Yadkori et al. - 2011 - Improved Algorithms for Linear Stochastic Bandits.pdf;/home/roshan/Documents/Zotero/storage/HB5KANLU/4417-improved-algorithms-for-linear-stochastic-bandits.html}
}

@inproceedings{MishraNearlyOptimalDPBandits2015,
  address = {Arlington, Virginia, United States},
  series = {UAI'15},
  title = {({{Nearly}}) Optimal Differentially Private Stochastic Multi-Arm Bandits},
  isbn = {978-0-9966431-0-8},
  abstract = {We study the problem of private stochastic multi-arm bandits. Our notion of privacy is the same as some of the earlier works in the general area of private online learning [13, 17, 24]. We design algorithms that are i) differentially private, and ii) have regret guarantees that (almost) match the regret guarantees for the best non-private algorithms (e.g., upper confidence bound sampling and Thompson sampling). Moreover, through our experiments, we empirically show the effectiveness of our algorithms.},
  booktitle = {Proceedings of the {{Thirty}}-{{First Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  publisher = {{AUAI Press}},
  author = {Mishra, Nikita and Thakurta, Abhradeep},
  year = {2015},
  pages = {592--601},
  file = {/home/roshan/Documents/Zotero/storage/DV2BH6X3/Supplementary Material.pdf;/home/roshan/Documents/Zotero/storage/KZNE9W7Q/Mishra and Thakurta - 2015 - (Nearly) Optimal Differentially Private Stochastic.pdf}
}

@article{TossouAchievingPrivacyAdversarial2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.04222},
  primaryClass = {cs},
  title = {Achieving Privacy in the Adversarial Multi-Armed Bandit},
  abstract = {In this paper, we improve the previously best known regret bound to achieve \$$\backslash$epsilon\$-differential privacy in oblivious adversarial bandits from \$$\backslash$mathcal\{O\}\{(T\^\{2/3\}/$\backslash$epsilon)\}\$ to \$$\backslash$mathcal\{O\}\{($\backslash$sqrt\{T\} $\backslash$ln T /$\backslash$epsilon)\}\$. This is achieved by combining a Laplace Mechanism with EXP3. We show that though EXP3 is already differentially private, it leaks a linear amount of information in \$T\$. However, we can improve this privacy by relying on its intrinsic exponential mechanism for selecting actions. This allows us to reach \$$\backslash$mathcal\{O\}\{($\backslash$sqrt\{$\backslash$ln T\})\}\$-DP, with a regret of \$$\backslash$mathcal\{O\}\{(T\^\{2/3\})\}\$ that holds against an adaptive adversary, an improvement from the best known of \$$\backslash$mathcal\{O\}\{(T\^\{3/4\})\}\$. This is done by using an algorithm that run EXP3 in a mini-batch loop. Finally, we run experiments that clearly demonstrate the validity of our theoretical analysis.},
  journal = {arXiv:1701.04222 [cs]},
  author = {Tossou, Aristide C. Y. and Dimitrakakis, Christos},
  month = jan,
  year = {2017},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security,Computer Science - Learning},
  file = {/home/roshan/Documents/Zotero/storage/RK4KKWYY/Tossou and Dimitrakakis - 2017 - Achieving Privacy in the Adversarial Multi-Armed B.pdf;/home/roshan/Documents/Zotero/storage/GIPIES8V/1701.html}
}

@inproceedings{SmithThakurtaPrivateOnlineLearning2013,
  title = {({{Nearly}}) Optimal Algorithms for Private Online Learning in Full-Information and Bandit Settings},
  booktitle = {Advances in {{Neural Information Processing Systems}} 26},
  publisher = {{Curran Associates, Inc.}},
  author = {Smith, Adam and Thakurta, Abhradeep},
  editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
  year = {2013},
  pages = {2733--2741},
  file = {/home/roshan/Documents/Zotero/storage/LJEC97M3/Guha Thakurta and Smith - 2013 - (Nearly) Optimal Algorithms for Private Online Lea.pdf;/home/roshan/Documents/Zotero/storage/ZH57Z28C/5012-nearly-optimal-algorithms-for-private-online-learning-in-full-information-and-bandit-setti.html}
}

@inproceedings{LattimoreEndOptimism2017,
  title = {The End of Optimism? {{An}} Asymptotic Analysis of Finite-Armed Linear Bandits},
  shorttitle = {The {{End}} of {{Optimism}}?},
  abstract = {Stochastic linear bandits are a natural and simple generalisation of finite-armed bandits with numerous practical applications. Current approaches focus on generalising existing techniques for fini...},
  language = {en},
  booktitle = {{{PMLR}}},
  author = {Lattimore, Tor and Szepesvari, Csaba},
  month = apr,
  year = {2017},
  pages = {728-737},
  file = {/home/roshan/Documents/Zotero/storage/MM4IPVDW/Lattimore and Szepesvari - 2017 - The End of Optimism An Asymptotic Analysis of Fin.pdf;/home/roshan/Documents/Zotero/storage/YWE3FQ9M/lattimore17a.html}
}

@inproceedings{KearnsMechanismDesign2014,
  title = {Mechanism Design in Large Games: Incentives and Privacy},
  isbn = {978-1-4503-2698-8},
  shorttitle = {Mechanism Design in Large Games},
  doi = {10.1145/2554797.2554834},
  language = {en},
  publisher = {{ACM Press}},
  author = {Kearns, Michael and Pai, Mallesh and Roth, Aaron and Ullman, Jonathan},
  year = {2014},
  pages = {403-410},
  file = {/home/roshan/Documents/Zotero/storage/9L39FJ8H/Kearns et al. - 2014 - Mechanism design in large games incentives and pr.pdf}
}

@article{KarwaVadhanFiniteSampleDP2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.03908},
  primaryClass = {cs, math, stat},
  title = {Finite Sample Differentially Private Confidence Intervals},
  abstract = {We study the problem of estimating finite sample confidence intervals of the mean of a normal population under the constraint of differential privacy. We consider both the known and unknown variance cases and construct differentially private algorithms to estimate confidence intervals. Crucially, our algorithms guarantee a finite sample coverage, as opposed to an asymptotic coverage. Unlike most previous differentially private algorithms, we do not require the domain of the samples to be bounded. We also prove lower bounds on the expected size of any differentially private confidence set showing that our the parameters are optimal up to polylogarithmic factors.},
  journal = {arXiv:1711.03908 [cs, math, stat]},
  author = {Karwa, Vishesh and Vadhan, Salil},
  month = nov,
  year = {2017},
  keywords = {Computer Science - Cryptography and Security,Mathematics - Statistics Theory},
  file = {/home/roshan/Documents/Zotero/storage/7GQIQHH4/Karwa and Vadhan - 2017 - Finite Sample Differentially Private Confidence In.pdf;/home/roshan/Documents/Zotero/storage/7D2N9BV8/1711.html}
}

@book{ZhangMatrixTheory2011,
  address = {New York},
  edition = {2nd},
  series = {Universitext},
  title = {Matrix Theory: Basic Results and Techniques},
  isbn = {978-1-4614-1098-0},
  lccn = {QA188 .Z47 2011},
  shorttitle = {Matrix Theory},
  publisher = {{Springer}},
  author = {Zhang, Fuzhen},
  year = {2011},
  keywords = {Matrices},
  file = {/home/roshan/Documents/Zotero/storage/YM8YMGLV/Zhang - 2011 - Matrix theory basic results and techniques.pdf}
}

@inproceedings{DworkDifferentialPrivacy2006,
  series = {Lecture Notes in Computer Science},
  title = {Differential Privacy},
  isbn = {978-3-540-35907-4 978-3-540-35908-1},
  doi = {10.1007/11787006_1},
  abstract = {In 1977 Dalenius articulated a desideratum for statistical databases: nothing about an individual should be learnable from the database that cannot be learned without access to the database. We give a general impossibility result showing that a formalization of Dalenius' goal along the lines of semantic security cannot be achieved. Contrary to intuition, a variant of the result threatens the privacy even of someone not in the database. This state of affairs suggests a new measure, differential privacy, which, intuitively, captures the increased risk to one's privacy incurred by participating in a database. The techniques developed in a sequence of papers [8, 13, 3], culminating in those described in [12], can achieve any desired level of privacy under this measure. In many cases, extremely accurate information about the database can be provided while simultaneously ensuring very high levels of privacy.},
  language = {en},
  booktitle = {Automata, {{Languages}} and {{Programming}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Dwork, Cynthia},
  month = jul,
  year = {2006},
  pages = {1-12},
  file = {/home/roshan/Documents/Zotero/storage/QUKF48ZZ/Dwork - 2006 - Differential Privacy.pdf;/home/roshan/Documents/Zotero/storage/7HR82UTF/11787006_1.html}
}

@inproceedings{DworkCalibratingNoiseSensitivity2006,
  series = {Lecture Notes in Computer Science},
  title = {Calibrating Noise to Sensitivity in Private Data Analysis},
  isbn = {978-3-540-32731-8 978-3-540-32732-5},
  doi = {10.1007/11681878_14},
  abstract = {We continue a line of research initiated in [10,11]on privacy-preserving statistical databases. Consider a trusted server that holds a database of sensitive information. Given a query function f mapping databases to reals, the so-called true answer is the result of applying f to the database. To protect privacy, the true answer is perturbed by the addition of random noise generated according to a carefully chosen distribution, and this response, the true answer plus noise, is returned to the user.Previous work focused on the case of noisy sums, in which f = $\sum$ i g(x i ), where x i denotes the ith row of the database and g maps database rows to [0,1]. We extend the study to general functions f, proving that privacy can be preserved by calibrating the standard deviation of the noise according to the sensitivity of the function f. Roughly speaking, this is the amount that any single argument to f can change its output. The new analysis shows that for several particular applications substantially less noise is needed than was previously understood to be the case.The first step is a very clean characterization of privacy in terms of indistinguishability of transcripts. Additionally, we obtain separation results showing the increased value of interactive sanitization mechanisms over non-interactive.},
  language = {en},
  booktitle = {Theory of {{Cryptography}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Dwork, Cynthia and McSherry, Frank and Nissim, Kobbi and Smith, Adam},
  month = mar,
  year = {2006},
  pages = {265-284},
  file = {/home/roshan/Documents/Zotero/storage/ZFCASH3G/Dwork et al. - 2006 - Calibrating Noise to Sensitivity in Private Data A.pdf;/home/roshan/Documents/Zotero/storage/Q53U2C83/11681878_14.html}
}

@inproceedings{TossouAlgDPBandits2016,
  title = {Algorithms for Differentially Private Multi-Armed Bandits},
  copyright = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys' fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author's personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author's employer, and then only on the author's or the employer's own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author's or the employer's creation (including tables of contents with links to other papers) without AAAI's written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
  abstract = {We present differentially private algorithms for the stochastic Multi-Armed Bandit (MAB) problem. This is a problem for applications such as adaptive clinical trials, experiment design, and user-targeted advertising where private information is connected to individual rewards. Our major contribution is to show that there exist ($\epsilon$,$\delta$) differentially private variants of Upper Confidence Bound algorithms which have optimal regret, O($\epsilon$-1 + log T ). This is a significant improvement over previous results, which only achieve poly-log regret O($\epsilon$-2 log3 T), because of our use of a novel interval based mechanism. We also substantially improve the bounds of previous family of algorithms which use a continual release mechanism. Experiments clearly validate our theoretical bounds.},
  language = {en},
  booktitle = {Thirtieth {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Tossou, Aristide C. Y. and Dimitrakakis, Christos},
  month = mar,
  year = {2016},
  file = {/home/roshan/Documents/Zotero/storage/IEP5DCFK/Tossou and Dimitrakakis - 2016 - Algorithms for Differentially Private Multi-Armed .pdf;/home/roshan/Documents/Zotero/storage/3Y6LSDUC/11906.html;/home/roshan/Documents/Zotero/storage/P78WAVKI/1511.html}
}

@article{JainDPMatrixCompletion2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1712.09765},
  primaryClass = {cs},
  title = {Differentially Private Matrix Completion, Revisited},
  abstract = {We study the problem of privacy-preserving collaborative filtering where the objective is to reconstruct the entire users-items preference matrix using a few observed preferences of users for some of the items. Furthermore, the collaborative filtering algorithm should reconstruct the preference matrix while preserving the privacy of each user. We study this problem in the setting of joint differential privacy where each user computes her own preferences for all the items, without violating privacy of other users' preferences. We provide the first provably differentially private algorithm with formal utility guarantees for this problem. Our algorithm is based on the Frank-Wolfe (FW) method, and consistently estimates the underlying preference matrix as long as the number of users \$m\$ is \$$\backslash$omega(n\^\{5/4\})\$, where \$n\$ is the number of items, and each user provides her preference for at least \$$\backslash$sqrt\{n\}\$ randomly selected items. We also empirically evaluate our FW-based algorithm on a suite of datasets, and show that our method provides nearly same accuracy as the state-of-the-art non-private algorithm, and outperforms the state-of-the-art private algorithm by as much as 30\%.},
  journal = {arXiv:1712.09765 [cs]},
  author = {Jain, Prateek and Thakkar, Om and Thakurta, Abhradeep},
  month = dec,
  year = {2017},
  keywords = {Computer Science - Learning},
  file = {/home/roshan/Documents/Zotero/storage/IX255GNW/Jain et al. - 2017 - Differentially Private Matrix Completion, Revisite.pdf;/home/roshan/Documents/Zotero/storage/CPRI9CZS/1712.html}
}

@inproceedings{BunConcentratedDifferentialPrivacy2016,
  series = {Lecture Notes in Computer Science},
  title = {Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds},
  isbn = {978-3-662-53640-7 978-3-662-53641-4},
  shorttitle = {Concentrated {{Differential Privacy}}},
  doi = {10.1007/978-3-662-53641-4_24},
  abstract = {``Concentrated differential privacy'' was recently introduced by Dwork and Rothblum as a relaxation of differential privacy, which permits sharper analyses of many privacy-preserving computations. We present an alternative formulation of the concept of concentrated differential privacy in terms of the R{\'e}nyi divergence between the distributions obtained by running an algorithm on neighboring inputs. With this reformulation in hand, we prove sharper quantitative results, establish lower bounds, and raise a few new questions. We also unify this approach with approximate differential privacy by giving an appropriate definition of ``approximate concentrated differential privacy''.},
  language = {en},
  booktitle = {Theory of {{Cryptography}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Bun, Mark and Steinke, Thomas},
  month = nov,
  year = {2016},
  pages = {635-658},
  file = {/home/roshan/Documents/Zotero/storage/W49SREV7/Bun and Steinke - 2016 - Concentrated Differential Privacy Simplifications.pdf;/home/roshan/Documents/Zotero/storage/6SWX5EK6/10.html}
}

@inproceedings{BassilyPrivateEmpiricalRisk2014,
  address = {Washington, DC, USA},
  series = {FOCS '14},
  title = {Private Empirical Risk Minimization: Efficient Algorithms and Tight Error Bounds},
  isbn = {978-1-4799-6517-5},
  shorttitle = {Private {{Empirical Risk Minimization}}},
  doi = {10.1109/FOCS.2014.56},
  abstract = {Convex empirical risk minimization is a basic tool in machine learning and statistics. We provide new algorithms and matching lower bounds for differentially private convex empirical risk minimization assuming only that each data point's contribution to the loss function is Lipschitz and that the domain of optimization is bounded. We provide a separate set of algorithms and matching lower bounds for the setting in which the loss functions are known to also be strongly convex.Our algorithms run in polynomial time, and in some cases even match the optimal non-private running time (as measured by oracle complexity). We give separate algorithms (and lower bounds) for ($\epsilon$, 0)- and ($\epsilon$,$\delta$)-differential privacy, perhaps surprisingly, the techniques used for designing optimal algorithms in the two cases are completely different. Our lower bounds apply even to very simple, smooth function families, such as linear and quadratic functions. This implies that algorithms from previous work can be used to obtain optimal error rates, under the additional assumption that the contributions of each data point to the loss function is smooth. We show that simple approaches to smoothing arbitrary loss functions (in order to apply previous techniques) do not yield optimal error rates. In particular, optimal algorithms were not previously known for problems such as training support vector machines and the high-dimensional median.},
  booktitle = {Proceedings of the 2014 {{IEEE}} 55th {{Annual Symposium}} on {{Foundations}} of {{Computer Science}}},
  publisher = {{IEEE Computer Society}},
  author = {Bassily, Raef and Smith, Adam and Thakurta, Abhradeep},
  year = {2014},
  keywords = {learning (artificial intelligence),Optimization,(ε; 0)-differential privacy,(ε; δ)-differential privacy,Algorithm design and analysis,arbitrary loss function smoothing,computational complexity,Convex functions,convex programming,Lipschitz loss function,machine learning,minimisation,Noise measurement,optimal nonprivate running time,oracle complexity,polynomial time,Privacy,private convex empirical risk minimization,Risk management,smooth function families,statistics,Support vector machines},
  pages = {464--473},
  file = {/home/roshan/Documents/Zotero/storage/ZLCD7CND/Bassily et al. - 2014 - Private Empirical Risk Minimization Efficient Alg.pdf;/home/roshan/Documents/Zotero/storage/ZRGSD3SK/6979031.html}
}

@article{ChaudhuriDPERM2011,
  title = {Differentially Private Empirical Risk Minimization},
  volume = {12},
  issn = {1532-4435},
  abstract = {Privacy-preserving machine learning algorithms are crucial for the increasingly common setting in which personal data, such as medical or financial records, are analyzed. We provide general techniques to produce privacy-preserving approximations of classifiers learned via (regularized) empirical risk minimization (ERM). These algorithms are private under the $\epsilon$-differential privacy definition due to Dwork et al. (2006). First we apply the output perturbation ideas of Dwork et al. (2006), to ERM classification. Then we propose a new method, objective perturbation, for privacy-preserving machine learning algorithm design. This method entails perturbing the objective function before optimizing over classifiers. If the loss and regularizer satisfy certain convexity and differentiability criteria, we prove theoretical results showing that our algorithms preserve privacy, and provide generalization bounds for linear and nonlinear kernels. We further present a privacy-preserving technique for tuning the parameters in general machine learning algorithms, thereby providing end-to-end privacy guarantees for the training process. We apply these results to produce privacy-preserving analogues of regularized logistic regression and support vector machines. We obtain encouraging results from evaluating their performance on real demographic and benchmark data sets. Our results show that both theoretically and empirically, objective perturbation is superior to the previous state-of-the-art, output perturbation, in managing the inherent tradeoff between privacy and learning performance.},
  journal = {J. Mach. Learn. Res.},
  author = {Chaudhuri, Kamalika and Monteleoni, Claire and Sarwate, Anand D.},
  month = jul,
  year = {2011},
  pages = {1069--1109},
  file = {/home/roshan/Documents/Zotero/storage/8ZR2LZJ2/Chaudhuri et al. - 2011 - Differentially Private Empirical Risk Minimization.pdf}
}

@inproceedings{JainDPOnlineLearning2012,
  title = {Differentially Private Online Learning},
  abstract = {In this paper, we consider the problem of preserving privacy in the context of online learning. Online learning involves learning from data in real-time, due to which the learned model as well as i...},
  language = {en},
  booktitle = {Conference on {{Learning Theory}}},
  author = {Jain, Prateek and Kothari, Pravesh and Thakurta, Abhradeep},
  month = jun,
  year = {2012},
  pages = {24.1-24.34},
  file = {/home/roshan/Documents/Zotero/storage/FXG54F4K/Jain et al. - 2012 - Differentially Private Online Learning.pdf;/home/roshan/Documents/Zotero/storage/W65BSRDG/jain12.html}
}

@inproceedings{DworkBoosting2010,
  title = {Boosting and Differential Privacy},
  doi = {10.1109/FOCS.2010.12},
  abstract = {Boosting is a general method for improving the accuracy of learning algorithms. We use boosting to construct improved privacy-pre serving synopses of an input database. These are data structures that yield, for a given set Q of queries over an input database, reasonably accurate estimates of the responses to every query in Q, even when the number of queries is much larger than the number of rows in the database. Given a base synopsis generator that takes a distribution on Q and produces a "weak" synopsis that yields "good" answers for a majority of the weight in Q, our Boosting for Queries algorithm obtains a synopsis that is good for all of Q. We ensure privacy for the rows of the database, but the boosting is performed on the queries. We also provide the first synopsis generators for arbitrary sets of arbitrary low-sensitivity queries, i.e., queries whose answers do not vary much under the addition or deletion of a single row. In the execution of our algorithm certain tasks, each incurring some privacy loss, are performed many times. To analyze the cumulative privacy loss, we obtain an O($\epsilon$2) bound on the expected privacy loss from a single e-differentially private mechanism. Combining this with evolution of confidence arguments from the literature, we get stronger bounds on the expected cumulative privacy loss due to multiple mechanisms, each of which provides e-differential privacy or one of its relaxations, and each of which operates on (potentially) different, adaptively chosen, databases.},
  booktitle = {2010 {{IEEE}} 51st {{Annual Symposium}} on {{Foundations}} of {{Computer Science}}},
  author = {Dwork, C. and Rothblum, G. N. and Vadhan, S.},
  month = oct,
  year = {2010},
  keywords = {learning (artificial intelligence),data structures,Privacy,Accuracy,algorithm theory,boosting,Boosting,cumulative privacy loss,Data privacy,data structure,Data structures,database,Databases,differential privacy,expected privacy loss,Generators,learning algorithm,query algorithm,query processing,synopsis generator},
  pages = {51-60},
  file = {/home/roshan/Documents/Zotero/storage/3L29MSNK/Dwork et al. - 2010 - Boosting and Differential Privacy.pdf;/home/roshan/Documents/Zotero/storage/39U2Y3PV/5670947.html}
}

@inproceedings{DworkOurData2006,
  series = {Lecture Notes in Computer Science},
  title = {Our Data, Ourselves: Privacy via Distributed Noise Generation},
  isbn = {978-3-540-34546-6 978-3-540-34547-3},
  shorttitle = {Our {{Data}}, {{Ourselves}}},
  doi = {10.1007/11761679_29},
  abstract = {In this work we provide efficient distributed protocols for generating shares of random noise, secure against malicious participants. The purpose of the noise generation is to create a distributed implementation of the privacy-preserving statistical databases described in recent papers [14,4,13]. In these databases, privacy is obtained by perturbing the true answer to a database query by the addition of a small amount of Gaussian or exponentially distributed random noise. The computational power of even a simple form of these databases, when the query is just of the form $\sum$ i f(d i ), that is, the sum over all rows i in the database of a function f applied to the data in row i, has been demonstrated in [4]. A distributed implementation eliminates the need for a trusted database administrator.The results for noise generation are of independent interest. The generation of Gaussian noise introduces a technique for distributing shares of many unbiased coins with fewer executions of verifiable secret sharing than would be needed using previous approaches (reduced by a factor of n). The generation of exponentially distributed noise uses two shallow circuits: one for generating many arbitrarily but identically biased coins at an amortized cost of two unbiased random bits apiece, independent of the bias, and the other to combine bits of appropriate biases to obtain an exponential distribution.},
  language = {en},
  booktitle = {Advances in {{Cryptology}} - {{EUROCRYPT}} 2006},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Dwork, Cynthia and Kenthapadi, Krishnaram and McSherry, Frank and Mironov, Ilya and Naor, Moni},
  month = may,
  year = {2006},
  pages = {486-503},
  file = {/home/roshan/Documents/Zotero/storage/GSHJT2FC/Dwork et al. - 2006 - Our Data, Ourselves Privacy Via Distributed Noise.pdf;/home/roshan/Documents/Zotero/storage/IV8EM67I/11761679_29.html}
}

@inproceedings{DworkDifferentialPrivacyNewSettings2010,
  address = {Philadelphia, PA, USA},
  series = {SODA '10},
  title = {Differential Privacy in New Settings},
  isbn = {978-0-89871-698-6},
  abstract = {Differential privacy is a recent notion of privacy tailored to the problem of statistical disclosure control: how to release statistical information about a set of people without compromising the the privacy of any individual [7]. We describe new work [10, 9] that extends differentially private data analysis beyond the traditional setting of a trusted curator operating, in perfect isolation, on a static dataset. We ask \textbullet{} How can we guarantee differential privacy, even against an adversary that has access to the algorithm's internal state, eg, by subpoena? An algorithm that achives this is said to be pan-private. \textbullet{} How can we guarantee differential privacy when the algorithm must continually produce outputs? We call this differential privacy under continual observation. We also consider these requirements in conjunction.},
  booktitle = {Proceedings of the {{Twenty}}-First {{Annual ACM}}-{{SIAM Symposium}} on {{Discrete Algorithms}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  author = {Dwork, Cynthia},
  year = {2010},
  pages = {174--183},
  file = {/home/roshan/Documents/Zotero/storage/LQRZK2TV/Dwork - 2010 - Differential Privacy in New Settings.pdf}
}

@inproceedings{ChanPrivateContinualRelease2010,
  series = {Lecture Notes in Computer Science},
  title = {Private and Continual Release of Statistics},
  isbn = {978-3-642-14161-4 978-3-642-14162-1},
  doi = {10.1007/978-3-642-14162-1_34},
  abstract = {We ask the question \textendash{} how can websites and data aggregators continually release updated statistics, and meanwhile preserve each individual user's privacy? Given a stream of 0's and 1's, we propose a differentially private continual counter that outputs at every time step the approximate number of 1's seen thus far. Our counter construction has error that is only poly-log in the number of time steps. We can extend the basic counter construction to allow websites to continually give top-k and hot items suggestions while preserving users' privacy.},
  language = {en},
  booktitle = {Automata, {{Languages}} and {{Programming}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Chan, T.-H. Hubert and Shi, Elaine and Song, Dawn},
  month = jul,
  year = {2010},
  pages = {405-417},
  file = {/home/roshan/Documents/Zotero/storage/PBST4MI9/Chan et al. - 2010 - Private and Continual Release of Statistics.pdf;/home/roshan/Documents/Zotero/storage/2YJ8EP94/978-3-642-14162-1_34.html}
}

@inproceedings{DworkComplexityDPRelease2009,
  address = {New York, NY, USA},
  series = {STOC '09},
  title = {On the Complexity of Differentially Private Data Release: Efficient Algorithms and Hardness Results},
  isbn = {978-1-60558-506-2},
  shorttitle = {On the {{Complexity}} of {{Differentially Private Data Release}}},
  doi = {10.1145/1536414.1536467},
  abstract = {We consider private data analysis in the setting in which a trusted and trustworthy curator, having obtained a large data set containing private information, releases to the public a "sanitization" of the data set that simultaneously protects the privacy of the individual contributors of data and offers utility to the data analyst. The sanitization may be in the form of an arbitrary data structure, accompanied by a computational procedure for determining approximate answers to queries on the original data set, or it may be a "synthetic data set" consisting of data items drawn from the same universe as items in the original data set; queries are carried out as if the synthetic data set were the actual input. In either case the process is non-interactive; once the sanitization has been released the original data and the curator play no further role. For the task of sanitizing with a synthetic dataset output, we map the boundary between computational feasibility and infeasibility with respect to a variety of utility measures. For the (potentially easier) task of sanitizing with unrestricted output format, we show a tight qualitative and quantitative connection between hardness of sanitizing and the existence of traitor tracing schemes.},
  booktitle = {Proceedings of the {{Forty}}-First {{Annual ACM Symposium}} on {{Theory}} of {{Computing}}},
  publisher = {{ACM}},
  author = {Dwork, Cynthia and Naor, Moni and Reingold, Omer and Rothblum, Guy N. and Vadhan, Salil},
  year = {2009},
  keywords = {differential privacy,cryptography,exponential mechanism,privacy,traitor tracing},
  pages = {381--390},
  file = {/home/roshan/Documents/Zotero/storage/A8BCVAU5/Dwork et al. - 2009 - On the complexity of differentially private data r 2.pdf}
}

@article{LaurentAdaptiveEstimation2000,
  title = {Adaptive Estimation of a Quadratic Functional by Model Selection},
  volume = {28},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1015957395},
  abstract = {We consider the problem of estimating $\Vert$s$\Vert$2$\Vert$s$\Vert$2$\backslash$|s$\backslash$|\^2 when sss belongs to some separable Hilbert space and one observes the Gaussian process Y(t)=$\backslash$langles,t\rangle+$\backslash$sigmaL(t)Y(t)=$\backslash$langles,t\rangle+$\backslash$sigmaL(t)Y(t) = $\backslash$langles, t$\backslash$rangle + $\backslash$sigmaL(t), for all t$\epsilon\mathbb{H}$t$\epsilon$Ht $\backslash$epsilon $\backslash$mathbb\{H\},where LLL is some Gaussian isonormal process. This framework allows us in particular to consider the classical ``Gaussian sequence model'' for which $\mathbb{H}$=l2($\mathbb{N}{_\ast}$)H=l2(N${_\ast}$)$\backslash$mathbb\{H\} = l\_2($\backslash$mathbb\{N\}*) and L(t)=$\sum\lambda\geq$1t$\lambda\epsilon\lambda$L(t)=$\sum\lambda\geq$1t$\lambda\epsilon\lambda$L(t) = $\backslash$sum\_\{$\backslash$lambda$\backslash$geq1\}t\_\{$\backslash$lambda\}$\backslash$varepsilon\_\{$\backslash$lambda\}, where ($\epsilon\lambda$)$\lambda\geq$1($\epsilon\lambda$)$\lambda\geq$1($\backslash$varepsilon\_\{$\backslash$lambda\})\_\{$\backslash$lambda$\backslash$geq1\} is a sequence of i.i.d. standard normal variables. Our approach consists in considering some at most countable families of finite-dimensional linear subspaces of $\mathbb{H}$H$\backslash$mathbb\{H\} (the models) and then using model selection via some conveniently penalized least squares criterion to build new estimators of $\Vert$s$\Vert$2$\Vert$s$\Vert$2$\backslash$|s$\backslash$|\^2. We prove a general nonasymptotic risk bound which allows us to show that such penalized estimators are adaptive on a variety of collections of sets for the parameter sss, depending on the family of models from which they are built.In particular, in the context of the Gaussian sequence model, a convenient choice of the family of models allows defining estimators which are adaptive over collections of hyperrectangles, ellipsoids, lplpl\_p-bodies or Besov bodies.We take special care to describe the conditions under which the penalized estimator is efficient when the level of noise $\sigma\sigma\backslash$sigma tends to zero. Our construction is an alternative to the one by Efro{\"\i}movich and Low for hyperrectangles and provides new results otherwise.},
  language = {en},
  number = {5},
  journal = {The Annals of Statistics},
  author = {Laurent, B. and Massart, P.},
  month = oct,
  year = {2000},
  keywords = {model selection,$l_p$-bodies,Adaptive estimation,Besov bodies,efficient estimation,Gaussian sequence model,quadratic functionals},
  pages = {1302-1338},
  file = {/home/roshan/Documents/Zotero/storage/28RHKBY5/Laurent and Massart - 2000 - Adaptive estimation of a quadratic functional by m.pdf;/home/roshan/Documents/Zotero/storage/3JGHM4LI/1015957395.html}
}

@article{RusmevichientongLinearlyParameterizedBandits2010,
  title = {Linearly Parameterized Bandits},
  volume = {35},
  issn = {0364-765X},
  doi = {10.1287/moor.1100.0446},
  abstract = {We consider bandit problems involving a large (possibly infinite) collection of arms, in which the expected reward of each arm is a linear function of an r-dimensional random vector Z $\in$ $\mathbb{R}$r, where r $\geq$ 2. The objective is to minimize the cumulative regret and Bayes risk. When the set of arms corresponds to the unit sphere, we prove that the regret and Bayes risk is of order $\Theta$(r $\surd$T), by establishing a lower bound for an arbitrary policy, and showing that a matching upper bound is obtained through a policy that alternates between exploration and exploitation phases. The phase-based policy is also shown to be effective if the set of arms satisfies a strong convexity condition. For the case of a general set of arms, we describe a near-optimal policy whose regret and Bayes risk admit upper bounds of the form O(r $\surd$T log3/2T).},
  number = {2},
  journal = {Mathematics of Operations Research},
  author = {Rusmevichientong, Paat and Tsitsiklis, John N.},
  month = apr,
  year = {2010},
  pages = {395-411},
  file = {/home/roshan/Documents/Zotero/storage/PWIAFXYS/Rusmevichientong and Tsitsiklis - 2010 - Linearly Parameterized Bandits.pdf;/home/roshan/Documents/Zotero/storage/R5S8I4G8/moor.1100.html}
}

@inproceedings{DaniStochasticLinearOptimization2008,
  title = {Stochastic Linear Optimization under Bandit Feedback},
  abstract = {In the classical stochastic k-armed bandit problem, in each of a sequence of T rounds, a decision maker chooses one of k arms and incurs a cost chosen from an unknown distribution associated with that arm. The goal is to minimize regret, defined as the difference between the cost incurred by the algorithm and the optimal cost. In the linear optimization version of this problem (first considered by Auer [2002]), we view the arms as vectors in Rn, and require that the costs be linear functions of the chosen vector. As before, it is assumed that the cost functions are sampled independently from an unknown distribution. In this setting, the goal is to find algorithms whose running time and regret behave well as functions of the number of rounds T and the dimensionality n (rather than the number of arms, k, which may be exponential in n or even infinite).

We give a nearly complete characterization of this problem in terms of both upper and lower bounds for the regret. In certain special cases (such as when the decision region is a polytope), the regret is polylog(T). In general though, the optimal regret is $\Theta{_\ast}$ ( $\surd$ T) \textemdash{} our lower bounds rule out the possibility of obtaining polylog(T) rates in general.

We present two variants of an algorithm based on the idea of ``upper confidence bounds.'' The first, due to Auer [2002], but not fully analyzed, obtains regret whose dependence on n and T are both essentially optimal, but which may be computationally intractable when the decision set is a polytope. The second version can be efficiently implemented when the decision set is a polytope (given as an intersection of half-spaces), but gives up a factor of $\surd$ n in the regret bound. Our results also extend to the setting where the set of allowed decisions may change over time.},
  booktitle = {21st {{Annual Conference}} on {{Learning Theory}}},
  author = {Dani, Varsha and Hayes, Thomas and Kakade, Sham},
  month = jan,
  year = {2008},
  pages = {355-366},
  file = {/home/roshan/Documents/Zotero/storage/4MAHCIS8/Dani et al. - 2008 - Stochastic Linear Optimization Under Bandit Feedba.pdf;/home/roshan/Documents/Zotero/storage/MMYG3UTX/101.html}
}

@inproceedings{DworkContinualObservation2010,
  address = {New York, NY, USA},
  series = {STOC '10},
  title = {Differential Privacy under Continual Observation},
  isbn = {978-1-4503-0050-6},
  doi = {10.1145/1806689.1806787},
  abstract = {Differential privacy is a recent notion of privacy tailored to privacy-preserving data analysis [11]. Up to this point, research on differentially private data analysis has focused on the setting of a trusted curator holding a large, static, data set; thus every computation is a "one-shot" object: there is no point in computing something twice, since the result will be unchanged, up to any randomness introduced for privacy. However, many applications of data analysis involve repeated computations, either because the entire goal is one of monitoring, e.g., of traffic conditions, search trends, or incidence of influenza, or because the goal is some kind of adaptive optimization, e.g., placement of data to minimize access costs. In these cases, the algorithm must permit continual observation of the system's state. We therefore initiate a study of differential privacy under continual observation. We identify the problem of maintaining a counter in a privacy preserving manner and show its wide applicability to many different problems.},
  booktitle = {Proceedings of the {{Forty}}-Second {{ACM Symposium}} on {{Theory}} of {{Computing}}},
  publisher = {{ACM}},
  author = {Dwork, Cynthia and Naor, Moni and Pitassi, Toniann and Rothblum, Guy N.},
  year = {2010},
  keywords = {privacy,private data analysis},
  pages = {715--724},
  file = {/home/roshan/Documents/Zotero/storage/7IVE235R/Dwork et al. - 2010 - Differential Privacy Under Continual Observation.pdf}
}

@inproceedings{HardtTalwarGeometryDP2010,
  address = {New York, NY, USA},
  series = {STOC '10},
  title = {On the Geometry of Differential Privacy},
  isbn = {978-1-4503-0050-6},
  doi = {10.1145/1806689.1806786},
  abstract = {We consider the noise complexity of differentially private mechanisms in the setting where the user asks d linear queries f:Rn -$>$ R non-adaptively. Here, the database is represented by a vector in R and proximity between databases is measured in the l1-metric. We show that the noise complexity is determined by two geometric parameters associated with the set of queries. We use this connection to give tight upper and lower bounds on the noise complexity for any d $\leq$ n. We show that for d random linear queries of sensitivity 1, it is necessary and sufficient to add l2-error $\Theta$(min d$\surd$d/$\epsilon$,d$\surd$(log (n/d))/$\epsilon$) to achieve $\epsilon$-differential privacy. Assuming the truth of a deep conjecture from convex geometry, known as the Hyperplane conjecture, we can extend our results to arbitrary linear queries giving nearly matching upper and lower bounds. Our bound translates to error \$O(min d/$\epsilon$,$\surd$(d log(n/d)/$\epsilon$)) per answer. The best previous upper bound (Laplacian mechanism) gives a bound of O(min (d/$\epsilon$,$\surd$n/$\epsilon$)) per answer, while the best known lower bound was $\Omega$($\surd$d/$\epsilon$). In contrast, our lower bound is strong enough to separate the concept of differential privacy from the notion of approximate differential privacy where an upper bound of O($\surd$\{d\}/$\epsilon$) can be achieved.},
  booktitle = {Proceedings of the {{Forty}}-Second {{ACM Symposium}} on {{Theory}} of {{Computing}}},
  publisher = {{ACM}},
  author = {Hardt, Moritz and Talwar, Kunal},
  year = {2010},
  keywords = {differential privacy,privacy,complexity,geometry,histogram,statistical data analysis},
  pages = {705--714},
  file = {/home/roshan/Documents/Zotero/storage/R63TTRJN/Hardt and Talwar - 2010 - On the Geometry of Differential Privacy.pdf}
}

@inproceedings{DworkAnalyzeGauss2014,
  address = {New York, NY, USA},
  series = {STOC '14},
  title = {Analyze {{Gauss}}: Optimal Bounds for Privacy-Preserving Principal Component Analysis},
  isbn = {978-1-4503-2710-7},
  shorttitle = {Analyze {{Gauss}}},
  doi = {10.1145/2591796.2591883},
  abstract = {We consider the problem of privately releasing a low dimensional approximation to a set of data records, represented as a matrix A in which each row corresponds to an individual and each column to an attribute. Our goal is to compute a subspace that captures the covariance of A as much as possible, classically known as principal component analysis (PCA). We assume that each row of A has $\mathscr{l}$2 norm bounded by one, and the privacy guarantee is defined with respect to addition or removal of any single row. We show that the well-known, but misnamed, randomized response algorithm, with properly tuned parameters, provides nearly optimal additive quality gap compared to the best possible singular subspace of A. We further show that when ATA has a large eigenvalue gap -- a reason often cited for PCA -- the quality improves significantly. Optimality (up to logarithmic factors) is proved using techniques inspired by the recent work of Bun, Ullman, and Vadhan on applying Tardos's fingerprinting codes to the construction of hard instances for private mechanisms for 1-way marginal queries. Along the way we define a list culling game which may be of independent interest. By combining the randomized response mechanism with the well-known following the perturbed leader algorithm of Kalai and Vempala we obtain a private online algorithm with nearly optimal regret. The regret of our algorithm even outperforms all the previously known online non-private algorithms of this type. We achieve this better bound by, satisfyingly, borrowing insights and tools from differential privacy!},
  booktitle = {Proceedings of the {{Forty}}-Sixth {{Annual ACM Symposium}} on {{Theory}} of {{Computing}}},
  publisher = {{ACM}},
  author = {Dwork, Cynthia and Talwar, Kunal and Thakurta, Abhradeep and Zhang, Li},
  year = {2014},
  pages = {11--20},
  file = {/home/roshan/Documents/Zotero/storage/K3LKSJ9H/Dwork et al. - 2014 - Analyze Gauss Optimal Bounds for Privacy-preservi.pdf}
}


