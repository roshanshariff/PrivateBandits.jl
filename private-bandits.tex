\documentclass{article}
\usepackage[utf8]{inputenc}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018
\PassOptionsToPackage{round,colon}{natbib}

% ready for submission
\usepackage{nips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
% \usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\title{Differentially Private Contextual Linear Bandits}
\author{
  Roshan Shariff
  \And
  Or Sheffet
}

\usepackage{newtxtext}
%\usepackage[semibold]{libertine} % a bit lighter than Times--no osf in math
\usepackage[T1]{fontenc} % best for Western European languages
%\usepackage{textcomp} % required to get special symbols
%\usepackage[varqu,varl]{inconsolata} % a typewriter font must be defined
\usepackage{amsmath,mathtools}
\usepackage{amsthm,thmtools,thm-restate}
\usepackage{amssymb} % loads amsfonts
\usepackage{nicefrac}
%\usepackage[libertine,cmintegrals,bigdelims,vvarbb]{newtxmath} %
%replaces some amssymb symbols
\usepackage{newtxmath}
\usepackage[scr=rsfso]{mathalfa} % Use rsfso to provide mathscr
\usepackage{dsfont}
%\usepackage{upgreek}
\usepackage{bm} % load after all math to give access to bold math

\usepackage{microtype}
\usepackage[inline,shortlabels]{enumitem}
\usepackage{setspace}
\usepackage{tikz-cd}
\usepackage{booktabs}
\usepackage{algpseudocode,algorithm}
\usepackage{multicol}
%\usepackage{fullpage}
\usepackage{color}
\newcommand{\os}[1]{\textcolor{red}{Or's comment:~\textbf{#1}}}
%\usepackage{dblfloatfix}

%% Bibliography/References
%\usepackage[round,colon]{natbib}

%% Cross-references
\usepackage{varioref}
\usepackage[hidelinks]{hyperref}
\usepackage[capitalise]{cleveref}

%% To-do notes
\usepackage[obeyFinal]{todonotes}

% \newcommand{\tinytodo}[2][]{\todo[size=\tiny]{#2}}
\newcommand{\tinytodo}[2][]{\todo[size=\tiny, #1]{\begin{spacing}{1.0}#2\end{spacing}}}
\newcommand{\RStodo}[2][]{\tinytodo[color=red!20, #1]{R:\@#2}} % Roshan
\newcommand{\OStodo}[2][]{\tinytodo[color=blue!20, #1]{Cs: #2}} % Or
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%% Macros

\renewcommand{\vec}[1]{\bm{#1}}
\newcommand{\wildcard}{\mathinner{\,{\cdot}\,}}
\newcommand{\defeq}{\coloneq}
\newcommand{\eqdef}{\eqcolon}
\newcommand{\inv}[1]{#1^{-1}}
\newcommand{\Real}{\mathds{R}}
\newcommand{\Nat}{\mathds{N}}
\newcommand{\Int}{\mathds{Z}}
\newcommand{\mgf}{\mathrm{mgf}}
\newcommand{\UCB}{\operatorname{UCB}}
\renewcommand\mid{\mathinner{\vert}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\rank}{rank}

\newcommand\given[1][\delimsize]{%
  \providecommand{\delimsize}{}
  \nonscript\:#1\vert\allowbreak\nonscript\:\mathopen{}
}

\DeclarePairedDelimiter{\abs}||
\DeclarePairedDelimiter{\paren}()
\DeclarePairedDelimiter{\brck}{[}{]}
\DeclarePairedDelimiterX{\set}[1]\lbrace\rbrace{#1}
\DeclarePairedDelimiter{\floor}\lfloor\rfloor
\DeclarePairedDelimiter{\ceil}\lceil\rceil
\DeclarePairedDelimiterX{\innerp}[2]\langle\rangle{#1,#2}
\DeclarePairedDelimiterXPP{\Prob}[1]{\mathds{P}}(){}{#1}
\DeclarePairedDelimiterXPP{\PrSet}[1]{\mathds{P}}\{\}{}{#1}
\DeclarePairedDelimiterXPP{\Ex}[1]{\mathds{E}}{[}{]}{}{#1}
\DeclarePairedDelimiterXPP{\Exx}[2]{\mathds{E}_{#1}}{[}{]}{}{#2}
\DeclarePairedDelimiterXPP{\Var}[1]{\mathrm{Var}}{[}{]}{}{#1}
\DeclarePairedDelimiterXPP{\One}[1]{\mathds{1}}\{\}{}{#1}
\DeclarePairedDelimiterXPP{\norm}[2]{}\Vert\Vert{_{#1}}{#2}

%% Other symbols
\newcommand{\A}{\mathcal{A}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\E}{\mathcal{E}}
\providecommand\transp{\top}
\let\transpsymbol\transp
\renewcommand{\transp}[1]{#1^\transpsymbol}
\newcommand{\Aset}[1]{\mathcal{A}_{#1}}
\newcommand{\Dset}[1]{\mathcal{D}_{#1}}
\newcommand{\Cset}[1]{\mathcal{C}_{#1}}
\newcommand{\Eset}[1]{\mathcal{E}_{#1}}
\newcommand{\scrF}{\mathscr{F}}
\newcommand{\Wishart}{\mathcal{W}}
\newcommand{\Normal}{\mathcal{N}}
\newcommand{\Eye}[1]{\bm{I}_{#1 \times #1}}
\newcommand{\XtX}[1]{\transp{#1}{#1}}

% Theorem environments

\declaretheorem[style=plain]{theorem}
\declaretheorem[style=plain,sibling=theorem]{lemma}
\declaretheorem[style=plain,sibling=theorem]{corollary}
\declaretheorem[style=plain,sibling=theorem]{proposition}
\declaretheorem[style=remark,sibling=theorem]{claim}
\declaretheorem[style=definition]{definition}
\declaretheorem[style=definition]{assumption}
\declaretheorem[style=remark,numbered=no]{remark}

\declaretheorem[style=definition,numbered=no,name=Assumptions]{assumptions*}

\newenvironment{assumptions}[2][]{%
  \begin{assumptions*}[#1]
    #2
    \begin{enumerate}[nosep]
      \setcounter{enumi}{\theassumption}
      \newcommand{\assume}[1][]{\item\label[assumption]{##1}}
    }{
      \setcounter{assumption}{\theenumi}
    \end{enumerate}
  \end{assumptions*}%
}

\Crefname{assumption}{Assumption}{Assumptions}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\onehalfspacing

\begin{document}

\maketitle

\begin{abstract}
  We study the contextual linear bandit problem, where an algorithm
  sequentially selects actions that maximize an associated reward,
  which may depend on a provided per-round \emph{context}.  In
  particular, the reward is assumed to be a linear function of a
  \emph{feature vector} that encodes the context and selected action,
  with added stochastic noise.

  In many real-world applications, it is important to maintain the
  privacy of the provided contexts and rewards.  Specifically, the
  algorithm's choice of actions should not reveal too much information
  about the previous contexts and rewards it has learned from.  We
  address this concern by providing a \emph{differentially private}
  algorithm for the contextual linear bandit problem, along with
  bounds on its performance.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

The \emph{stochastic multi-armed bandit} (MAB) is a well-known
sequential decision-making task: in every round, a learner chooses an
action (or arm) and receives a corresponding reward drawn i.i.d.\ from
an unknown distribution associated with the chosen action action.  The
objective is to maximize the cumulative reward by learning from the
past rewards associated with the different actions.  The
\emph{contextual} bandit problem is an extension of MAB, where in each
round we are also given an arbitrary context, and the reward is now
drawn from a distribution that depends on \emph{both} the context and
the selected action.

% Since the agent only observes the reward for actions it takes, this
% is an online learning task with partial information.  Learning the
% rewards associated with all the arms requires an agent to
% \emph{explore} them.  Too much exploration, however, would be
% counter-productive; eventually the algorithm should \emph{exploit}
% the best arms it has found.  The main challenge of bandit strategies
% is to balance exploration and exploitation.

As a motivating example, consider the problem of online
advertisements.  The user provides a context (composed of query word,
past history of clicks, website visits etc), and in response the site
surfaces a potential ad and receives a reward if the user clicks the
ad.  Ignoring the context and modeling the problem as a standard MAB
(with each possible ad as an action) suffers from the drawback of
ignoring the variety of users' preferences; whereas mode-ling the
problem as an independent learning task per user prevents
generalization between users.  As a result, it is common to model the problem as a
contextual linear-bandit problem.  Based on the user-given context,
each ad (action) is mapped to some feature vector; yet the
click-probability is modeled the same way across all users --- it is
assumed to be proportional to the inner-product between the action's
feature vector and some target vector $\vec\theta^*$.

The above example motivates the need for privacy in the contextual bandit setting: users' interests, web site visits, and clicks are
sensitive personal information, yet they are all required for making a reasonable prediction.
% An algorithm may pick an ad for a
% user based on their own information, and it may even learn from
% aggregated information to choose better in the future, but an
% adversary analyzing these ad choices must not be able to learn too
% much about any individual user.
In this work, we give the first upper- and lower-bounds for the
problem of (joint) \emph{differentially private} contextual linear
bandits.
\emph{Differential privacy}, the de-facto gold-standard of
privacy preserving data-analysis in both academia and industry,
states that the algorithm's performance has a very limited dependency on any interaction with one single user (one context and one
reward).  However, as we later illustrate, in the contextual
bandit setting adhering to the standard notion of differential privacy (under continual observation) implies we must essentially ignore
each day's context and thus incur linear regret.  We therefore adopt
the more relaxed notion of \emph{joint differential privacy}
\citep{KearnsMechanismDesign2014} which allows for the output of round
$t$ to depend arbitrarily on the context of round $t$, yet have only
limited dependency with the context or reward of any other round
$t'$.  Intuitively, we allow the $t$-th user to observe ads
corresponding to her preferences, without revealing (much) about those
preferences to all users at times $t'>t$.\footnote{The guarantee of
  differential privacy under continuous observation assures that the
  entire sequence of actions and rewards preserve privacy.  Hence, even
  if all later user collude in an effort to learn user $t$'s context,
  they still have very limited advantage over a random guess.}

\subsection{Background and Problem Formulation}
\label{subsec:intro_background}

\subsubsection{Stochastic Contextual Linear Bandits.}%
\label{sec:contextual-bandits}

The classic MAB problem is a sequential decision making task in which, at
every round $t$, an algorithm selects an action $a_t$ from a fixed set $\A$.
%Usually, $\A = \set{1,\dotsc,K}$ (the $K$-armed bandit) but we do not
%require this; our algorithms can handle other action sets that may be
%infinite or uncountable.
The algorithm then receives a \emph{reward}
corresponding to the action it chose.  In the (stationary)
\emph{stochastic multi-armed bandit}, the reward is of the form $r_t =
\mu_{a_t} + \eta_t$, where $\mu_a$ is the expected reward of action $a$
and $\eta_t$ is zero-mean noise.
%In other words, the reward is sampled independently from a distribution that depends only on the selected action.

In the contextual bandit problem, the algorithm also receives a
\emph{context} $c_t\in\C$ at the beginning of each round, and the
reward is assumed to depend on both the context and the choset action $a_t$.  To
take advantage of the context while choosing actions, we will assume
that the context affects the reward in a linear way, in the following
sense.  We assume the existence of a known \emph{mapping}
$\phi:\Cset{}\times\Aset{}\to\Real^d$ that maps every context-action
pair to a $d$-dimensional \emph{feature vector}.  We will assume that
the reward is a linear function of the feature vector, with some added
noise: $r_t = \innerp{\vec\theta^*}{\phi(c_t,a_t)} + \eta_t$ for some
unknown vector $\vec\theta^*\in\Real^d$. In fact, we simplify the
problem statement to include the \emph{decision set}
$\Dset{t} \defeq \set{\phi(c_t,a)\given a\in\Aset{}} \subset \Real^d$
(where the context is implicitly encoded in the decision set), and so,
rather than choosing an action out of $\Aset{}$ the algorithm chooses
some $x_t\in\Dset{t}$.  Thus, the contextual stochastic linear bandit
framework is composed of repeated rounds in which
\begin{enumerate}
\item The learner receives an arbitrarily chosen \emph{decision set} $\mathcal{D}_t \subset
  \Real^d$.
\item The learner chooses an \emph{action} $\vec x_t \in \mathcal{D}_t$.
\item The learner receives a stochastic \emph{reward}
  $y_t = \innerp{\vec\theta^*}{\vec x_t} + \eta_t$.
\end{enumerate}
The vector $\vec\theta^*\in\Real^d$ is the key unknown parameter of the
environment which the agent aims to learn so that she can maximize reward. The goal of the learner is thus to minimize her expected regret...\os{Define the objective: minimizing expected regret --- perhaps justify why we chose this objective (standard, implies similar objectives etc)}

\subsubsection{Joint Differential Privacy.}\label{sec:joint-dp}

As discussed above, the context and reward may be considered private
information about the users which we wish to keep private from all \emph{other} users. We thus introduce the notion of joint differentially private learners under continuous observation \citep[a combination of the two definitions given in][]{KearnsMechanismDesign2014,DworkContinualObservation2010}. First, we say two sequences $S = \langle (\mathcal{D}_1, y_1), (\mathcal{D}_2, y_2), ..., (\mathcal{D}_n, y_n) \rangle$ and $S' = \langle (\mathcal{D}'_1, y'_1), ..., (\mathcal{D}'_n, y'_n) \rangle$ are \emph{$t$-neighbors} if for all $t'\neq t$ it holds that $(\mathcal{D}_{t'},y_{t'}) = (\mathcal{D}'_{t'}, y'_{t'})$.
%\os{maybe omit?} We say $S$ and $S'$ are \emph{neighbors} if there exists a $t$ such that the two sequences are $t$-neighbors.
% For example, a
% search engine might have a context consisting of a user's search
% query, identity, interests, and physical location, while the reward
% indicates which search result the user clicked on.  The search engine
% should, of course, use the context to answer each query; furthermore,
% it should learn from the reward to better respond to future queries
% from other users.  However, it should also maintain privacy: its
% responses to queries should not reveal \emph{too much} information
% about the context and rewards it has learned from.  More precisely, we
% want algorithms that are \emph{jointly differentially private} in the
% following sense:

\begin{definition}
\label{def:joint-dp}
  A randomized algorithm $A$ for the contextual bandit problem is
  \emph{$(\varepsilon,\delta)$-jointly differentially private} (JDP) under continual observation if for any $t$ and any pair of $t$-neighboring sequences $S$ and $S'$, and any subset ${\cal S}_{>t} \subset \mathcal{D}_{t+1} \times \mathcal{D}_{t+2} \times \cdots \times \mathcal{D}_{T}$ of sequence of actions ranging from day $t+1$ to the end of the sequence, it holds that $\Pr[A(S)\in \mathcal{S}_{>t}] \leq e^\varepsilon\Pr[A(S')\in \mathcal{S}'_{>t}] +\delta$.
\end{definition}
Note that the standard definition of differential privacy requires the distribution proximity to hold for any subset of sequences of actions ranging from day $t$ to the end of sequence. However, in our problem formulation, with given decision sets, this notion isn't not even well-defined --- as the decision set of day $t$ is different under $S$ and under $S'$. Therefore, when we discuss the impossibility of regret-minimization under standard differential privacy, we result back to the setting of different contexts with fixed action set. See further details in Section~\ref{sec:lower_bounds}.

\subsection{Our Contribution and Paper Organization}
\label{subsec:contributions}

In this work, in addition to formulating the definition of JDP under
continual observation, we also present a framework for implementing
JDP algorithms for the contextual bandit problem. Not surprisingly,
our framework combines the tree-based algorithm~\cite{DworkContinualObservation2010}  with the linear
upper-confidence bound (LinUCB) algorithm~\cite{???????}.  However, for modularity,
we choose to analyze a family of linear UCB algorithms where in each
day one uses a different regularizer, under the premise that all
regularizers' singular values are bounded. \os{above and below?!?}
Moreover, we repeat our analysis twice. Once in a general framework,
obtaining an upper bound on the regret which is proportional to
$\sqrt n$; and once in an instance dependent setting, where in each
day there's a significant gap of at least $\Delta$ between the best arm of each round and any other
arms, obtaining a regret upper bound of $O(\log(n)/\Delta)$.\footnote{The analysis carries through to $k-1$ gaps between the $i$th
arm and the leading one, yet its result is too cumbersome so we omit
this analysis.} Our leading application of course is privacy, though
one could postulate other reasons why such a change in regularizers
would be useful (e.g., updating the regularizer when an initial upper
bound of a parameter turns out to be wrong). This analysis is given in Section~\ref{}. We then plug in two
particular regularizers into our scheme: one based on additive Wishart
noise \citep{SheffetPrivateApproxRegression2015} which always results
in a PSD; and one based on additive Gaussian
noise \citep{DworkAnalyzeGauss2014} which doesn't have to result in a PSD, so
we shift it by $\rho I$ so that w.h.p.\ (over all days in the sequence)
the shifted eigenvalues of the noise are all positive. The details of the two techniques are given in Section~\ref{}. We also present
empirical evidence showing our analysis is in fact optimal, presented in Section~\ref{}.

We also give lower bound for the $\varepsilon$-differentially private
MAB problem. Whereas all previous work on the private MAB problem uses
the standard (non-private) bounds, we show that the regret of
\emph{any} private algorithm must incur \emph{an additional} cost of
$\log(T)/\varepsilon$. This resembles the lower bound of the
adversarial setting, however, the proof technique cannot rely on the
standard packing arguments \citep[see, for
example,][]{HardtTalwarGeometryDP2010} seeing as the input for the
problem is stochastic rather than adversarial.  Instead, we apply a
new result of \citet{KarwaVadhanFiniteSampleDP2017} showing, in essence,
that the effective group-privacy between two possible inputs of $n$
samples drawn iid from distributions $P$ and $Q$ respectively is about
$n\cdot d_{\rm TV}(P,Q)$. Details appear in Section~\ref{sec:lowerbound}.



\todo[inline]{Complete this section.}
\section{Preliminaries, Background and Related Works}
\label{sec:background}

\paragraph{Differential Privacy.} Differential privacy was first introduced by
\citet{DworkCalibratingNoiseSensitivity2006, DworkKenthrapdai...2006}, and it is a rigorous mathermatical notion that determines that the probability of any event changes very little with the change of a single datum. (We omit the definition as the Joint-DP was already defined.) A classic result~\cite{DworkKenthrapdai...2006} states that if $f$ is a function of the input whose $L_2$-norm cannot change by more than $B$ with a change of a single datum, then adding Gaussian noise to $f$ of $0$-mean and vairance $B^2\log(\nicefrac 1 \delta)/\varepsilon^2$ preserves $(\varepsilon,\delta)$-differential privacy. Among its many elegant traits is the notion of group privacy: should $k$ datums change the data, the change to any every is still limited, by (roughly) $k$ times the magnitude of a single entry's change. A different property of differential privacy is composition~\cite{DworkRothblumVadhan2010}: The combination of $k$ $(\varepsilon,\delta)$-differentially private algorithms is $\left( O(k\varepsilon^2 + 2\sqrt{k\log(\nicefrac 1 {\delta'})}), k\delta+\delta'  \right)$-differentially private for any $\delta'>0$.

The notion of differential privacy under continual observation was first defined in~\cite{DworkContinualObservation2010} have also been defined, using the \textbf{tree-based algorithm} (originally appearing in~\cite{ChanPrivateContinualRelease2010}). This algorithm maintains a binary tree whose $n$ leafs correspond to the $n$ entries in the input sequence. Each node in the tree maintains a noisy (privacy-preserving) count of the sum of the input entries in its subtree, thus enabling us to maintain a running count of the sum of inputs thus far by combining no more than $\log(n)$ noisy counts. This algorithm is the key ingredient in a variety of works the deal with privacy in an noline setting, including counts~\cite{DworkContinualObservation2010}, online convex optimization~\cite{Jain,someone,thakurta,colt2012/3}, and regret minimization~\cite{SmithThakurta2013},\cite{MishraNearlyOptimalDPBandits2015} and~\cite{TussouDimitrikakis twice} in the adversarial and stoachastic setting.

In the offline setting, there have been numerous works studying differentially private linear regression~\cite{ChaudhuriMonteleoniSarwate2011, BassilySmithThakurta2014}. In this work we rely on two techniques that maintain a private version of the $2^{\rm nd}$-moment matrix of the data (the Gram matrix) --- the work of~\cite{DworkTTZ14} which uses additive Gaussian noise (which doesn't necessarily result in a PSD output) and the work of~\cite{SheffetPrivateApproxRegression2015} which uses additive Wishart noise (and is always PSD).

\begin{multicols}{2}[\subsection{Notation}\label{sec:notation}]
  \nolinenumbers
  \begin{description}[style=sameline,leftmargin=5em]
  \item[$n$] horizon, i.e. number of rounds
  \item[$s,t$] indices of rounds
  \item[$d$] dimensionality of action space
  \item[$\Dset{t}\subset\Real^d$] decision set at round $t$
  \item[$\vec x_t \in \Dset{t}$] action at round $t$
  \item[$y_t \in \Real$] reward at round $t$
  \item[$\vec X_t \in \Real^{t\times d}$] matrix with $\vec X_s = \transp{\vec
      x_s}$ for $s\in[t]$
  \item[$\vec y_t \in \Real^t$] vector of rewards $\vec y_s = y_s$
  \item[$\vec\theta^* \in \Real^d$] unknown parameter vector
  \item[$\norm{V}{\vec x}$] $\defeq \sqrt{\transp{\vec x} V \vec x}$
  \end{description}
  \linenumbers
\end{multicols}

\section{Differentially Private Linear UCB}

Our differentially private contextual linear bandit algorithm is based
on the well-studied LinUCB algorithm, an application of the Upper
Confidence Bound (UCB) idea to stochastic linear bandits
\citep{DaniStochasticLinearOptimization2008,RusmevichientongLinearlyParameterizedBandits2010,AbbasiYadkoriImprovedAlgorithmsLinear2011}.
At every round $t$, LinUCB constructs a \emph{confidence set} $\E_t$
containing the unknown parameter vector $\vec\theta^*$ with high
probability.  It then computes an upper confidence bound on the reward
of each action in the decision set $\Dset{t}$, and ``optimistically''
chooses the action with the highest UCB:
\begin{align}\label{eq:def-ucb}
  \vec x_t &\gets \argmax_{\vec x\in\Dset{t}} \UCB_t(\vec x),
  &\text{where }
  \UCB_t(\vec x) &\defeq \max_{\vec\theta\in\E_t} \innerp{\vec\theta}{\vec x}.
\end{align}
We assume the rewards are linear with added subgaussian noise (i.e.,
$y_s = \innerp{\vec\theta^*}{\vec x_s} + \eta_s$ for $s<t$), so it is
natural to center the confidence set $\E_t$ on the (regularized)
linear regression estimate:
\begin{align*}
  \hat{\vec\theta}_t
  &\defeq \min_{\hat{\vec\theta} \in \Real^d} \norm{}{\vec X_{t-1} \hat{\vec\theta}
    - \vec y_{t-1}}^2 + \norm{H_t}{\hat{\vec\theta}}^2
    = \inv{(G_t + H_t)} \transp{\vec X_{t-1}} \vec y_{t-1}.
    &\text{where } G_t \defeq \XtX{\vec X_{t-1}}
\end{align*}
The matrix $V_t \defeq G_t + H_t \in \Real^{d\times d}$ is a regularized
version of the Gram matrix $G_t$.  Whenever the learner chooses an
action vector, the corresponding reward gives it some information
about the projection of $\vec\theta^*$ onto that vector.  In other
words, the estimate $\hat{\vec\theta}$ is probably closer to
$\vec\theta^*$ along directions of $\Real^d$ where many actions have
been taken.  This motivates the use of ellipsoidal confidence sets
that are smaller in these directions, inversely related to the
corresponding eigenvalues of $G_t$ (and ideally $V_t$). The ellipsoid
is uniformly scaled by $\beta_t$ to achieve the desired confidence
level, as prescribed by \cref{prop:calc-beta}.
\begin{align}\label{eq:def-ellip}
  \E_t &\defeq \set{\vec\theta\in\Real^d \given
        \norm{V_t}{\vec\theta-\hat{\vec\theta}_t} \le \beta_t},
  &\text{ for which }
    \UCB_t(\vec x) &= \innerp{\hat{\vec\theta}}{\vec x} + \beta_t\norm{\inv{V_t}}{\vec x}.
\end{align}
Just as the changing regularizer $H_t$ perturbs the Gram matrix $G_t$,
our algorithm allows for the vector
$\vec u_t \defeq \transp{\vec X_t} \vec y_t$ to be perturbed by a
changing $\vec h_t$ to get
$\tilde{\vec u}_t \defeq \vec u_t + \vec h_t$.  The estimate
$\hat{\vec\theta}_t$ is replaced by
$\tilde{\vec\theta}_t \defeq \inv{V_t}\tilde{\vec u}_t$.

\begin{algorithm}
  \caption{Linear UCB with Changing Perturbations}\label{alg:linucb}
  \begin{algorithmic}
    \State \textbf{Initialize:} $G_1 \gets 0_{d\times d}$,
    $u_1\gets \vec 0_{d}$.
    \For{each round $t = 1,2,\dotsc,n$}
    \State Receive $\Dset{t} \gets{}$ decision set ${} \subset \Real^d$.
    \State Receive regularized $V_t \gets G_t + H_t$.
    \State Receive perturbed $\tilde{\vec u}_t \gets \vec u_t + \vec h_t$
    \State Compute regressor $\tilde{\vec\theta}_{t} \gets \inv{V_{t}}\tilde{\vec u}_{t}$
    \State Pick action $\vec x_t \gets \argmax_{\vec x\in\Dset{t}}
    \innerp{\tilde{\vec \theta}_t}{\vec x} +
    \beta_t\norm{\inv{V_t}}{\vec x}$.
    %\Comment{Choose action.}
    \State Observe $y_t \gets {}$ reward for action $\vec x_t$
    %\Comment{Observe reward.}
    \State Update: $G_{t+1} \gets G_{t} + \vec x_t \transp{\vec x_t},
    \quad \vec u_{t+1} \gets \vec u_{t} + \vec x_t y_t$
    %\Comment{Record in privacy-preserving history.}
    \EndFor
  \end{algorithmic}
\end{algorithm}

We now collect our assumptions about the environment and inputs to the
algorithm; we will indicate where each of these is used.
\begin{assumptions}[\Cref{alg:linucb}]{%
    For all rounds $t=1,\dotsc,n$ and actions $\vec x\in\Dset{t}$:}
  \assume[ass:x-bound] $\norm{}{\vec x} \le L$.
  \assume[ass:meanreward-bound] $\abs{\innerp{\vec\theta^*}{\vec x}}
  \le B$.
  \assume[ass:thetastar-bound] $\norm{}{\vec\theta^*} \le S$.
  \assume[ass:linear-reward] $y_t = \innerp{\vec\theta^*}{\vec x_t} + \eta_t$
  \assume[ass:psd-reg] $H_t \succ 0$ is symmetric.
    \assume[ass:subgaussian] $\sigma^2$-conditionally subgaussian reward
  noise $\eta_t$ given actions and previous rewards:
  \begin{align*}
    \Ex{\exp(\lambda\eta_t) \given \vec x_1, y_1, \dotsc, \vec x_{t-1}, y_{t-1}, \vec x_t}
    &\le \exp(\lambda^2\sigma^2/2), &\text{for all } \lambda\in\Real.
  \end{align*}
\end{assumptions}
\cref{ass:x-bound,ass:meanreward-bound} are not needed for the
correctness of the algorithm itself, only for the regret bounds in
\cref{sec:regret-bounds}.  Conversely, it is important to note that of
these, only \cref{ass:x-bound} is needed for the algorithm to be
differentially private (see \cref{sec:alg-dp}).

The algorithm relies upon the confidence ellipsoids to be accurate but
nevertheless tightly concentrated around the unknown $\vec\theta^*$.
In other words, the $\beta_t$ should be as small as possible while
accounting for the noise $\eta_t$ and the perturbations caused by
$H_t$ and $\vec h_t$.

\begin{definition}[Accurate $(\beta_t)_t$]\label{def:accurate-beta}
  A sequence $(\beta_t)_{t=1}^n$ is $(\alpha, n)$-\emph{accurate} for
  $(H_t)_{t=1}^n$ and $(\vec h_t)_{t=1}^n$ if, with probability at
  least $1-\alpha$, it satisfies
  $\norm{V_t}{\vec\theta^* - \tilde{\vec\theta}_t} \le \beta_t$
  for all rounds $t=1,\dotsc,n$ simultaneously.
\end{definition}

\begin{definition}[Accurate $\rho_{\min}$, $\rho_{\max}$, and
  $\gamma$]\label{def:accurate-params}
  The tail bounds $0 < \rho_{\min} \le \rho_{\max}$ and $\gamma$ are
  $\alpha$-\emph{accurate} for some $H_t$ and $\vec h_t$ if the
  following hold with probability at least $1-\alpha$:
  \begin{align*}
    \norm{}{H_t} &\le \rho_{\max},
    &\norm{}{\inv{H_t}} &\le \rho_{\min},
    &\norm{\inv{H_t}}{\vec h_t} &\le \gamma.
  \end{align*}
\end{definition}

\begin{restatable}[Calculating $\beta_t$]{proposition}{CalcBeta}%
  \label{prop:calc-beta}
  Suppose
  \cref{ass:thetastar-bound,ass:linear-reward,ass:psd-reg,ass:subgaussian}
  hold, i.e.\ $\norm{}{\vec\theta^*} \le S$, $H_t \succeq 0$, and the
  rewards are linear with $\sigma^2$-subgaussian noise.  For some
  $\alpha\in(0,1)$, horizon $n$, and sequences $(H_t)_{t=1}^n$ and
  $(\vec h_t)_{t=1}^n$, let $0 < \rho_{\min} \le \rho_{\max}$ and
  $\gamma$ be $\alpha/(2n)$-accurate tail bounds according to
  \cref{def:accurate-params} for each round separately.  Then the
  sequence $(\beta_t)_{t=1}^n$ is $(\alpha,n)$-accurate according to
  \cref{def:accurate-beta}:
  \begin{align*}
    \beta_t &\defeq \sigma\sqrt{2\log(2/\alpha) + \log\det V_t - d\log\rho_{\min}}
             + S\sqrt{\rho_{\max}} + \gamma. \\
    \shortintertext{Furthermore, if all $\vec x_s \le L$
    (\cref{ass:x-bound}), then}
    \beta_t &\le \sigma\sqrt{2\log\frac{2}{\alpha} + d\log\paren*{1 + \frac{tL^2}{d\rho_{\min}}}}
             + S\sqrt{\rho_{\max}} + \gamma.
  \end{align*}
\end{restatable}

\subsection{Achieving Differential Privacy}
\label{sec:alg-dp}

Notice that \cref{alg:linucb} uses its history of actions and rewards
up to round $t$ only via the confidence set $\E_t$, which is to say
the Gram matrix $G_t\defeq\XtX{\vec X_{t-1}}$ and the vector
$\vec u_t \defeq \transp{\vec X_t} \vec y_t$; these also determine
$\beta_t$.  By recording this history with differential privacy, we
obtain a Linear UCB algorithm that is jointly differentially private
(\cref{def:joint-dp}) because it simply post-processes $G_t$ and
$\vec u_t$
\citep[see][Proposition~2.1]{DworkAlgorithmicFoundationsDifferential2014}.
\begin{claim}
  If the sequence $(V_t,\tilde{\vec u}_t)_{t=1}^{n-1}$ is
  $(\varepsilon,\delta)$-differentially private with respect to
  $(\vec x_t, y_t)_{t=1}^{n-1}$, then \cref{alg:linucb} is
  $(\varepsilon,\delta)$-jointly differentially private.
\end{claim}

\begin{remark}
  \Cref{alg:linucb} is only jointly differentially private even though
  the history maintains full differential privacy.  This is because
  the chosen action depends not only on the past contexts $c_s$
  ($s < t$, via the differentially private $\vec X_{t-1}$) but also on
  the current context $c_t$ via the decision set $\Dset{t}$.  Since
  this use of $c_t$ is not differentially private, it is revealed by
  the algorithm's chosen $\vec x_t$ (as discussed in
  \cref{sec:joint-dp}).
\end{remark}

To make the following exposition simpler, define the matrix
$A \defeq \begin{bmatrix} \vec X_n & \vec y_n \end{bmatrix} \in
\Real^{n\times(d+1)}$, with $A_t$ being the top $t-1$ rows of $A$
(and $A_1 = \vec 0_{1\times(d+1)}$).  Then the top-left $d\times d$
submatrix of $M_t \defeq \XtX{A_t}$ is the Gram matrix $G_t$ and the
first $d$ entries of its last column are
$\vec u_t = \transp{\vec X_{t-1}}\vec y_{t-1}$.  It therefore suffices
to maintain a differentially private approximation of $M_t$.

\begin{assumption}\label{ass:bounded-data}
  Each row of $A$ is bounded:
  $\sqrt{\norm{}{\vec x_t}^2 + y_t^2} \le \tilde L$ for all rounds
  $t$.  In particular, this holds if $\vec x_t \le L$
  (\cref{ass:x-bound}) and the actual rewards are bounded (not just
  their means as in \cref{ass:meanreward-bound}):
  $\abs{y_t} \le \tilde B$.
\end{assumption}

\subsection{Pan-Privacy with Tree-Based Aggregation}
\label{sec:tree-mechanism}

\todo[inline]{Expand this section, keeping the results below.}  For a horizon
of $n$, the pan-private tree mechanism composes $m = \ceil{1+\log_2n}$
mechanisms.  Suppose we want to achieve a target of
$(\varepsilon,\delta)$-differential privacy after composing these $m$
mechanisms.  Then it is sufficient for each mechanism to be
$(\varepsilon',\delta')$-differentially private where:
\begin{align*}
  \varepsilon'' &= \frac{\varepsilon}{2\sqrt{2m\ln\frac{1}{\delta - m\delta'}}} \\
  \delta' &< \delta/m.
\end{align*}

\subsection{Differential Privacy via Additive Wishart Noise}
\label{sec:dp-wishart}

\begin{theorem}%
  \label{thm:wishart-cont-dp}
  Fix $\varepsilon$ and a sufficiently small $\delta$, let $A\in\Real^{n\times p}$ be a
  matrix with each row having $\norm{}{A_{t,:}} \le \tilde L$, and let
  $m \defeq 1 + \ceil{\log_2n}$.  Generate the sequence of matrices
  $(W_t)_{t=1}^n$ using the tree mechanism (\cref{sec:tree-mechanism})
  with Wishart noise distributed according to \todo{Make sure $k$ below is correct!}
  \begin{align*}
    &\Wishart_p(\tilde L^2\Eye{p}, k),
    &\text{ where } k
    &\defeq p + \floor*{\frac{224}{\varepsilon^2} \cdot m\ln^2\paren*{\frac{4m+1}{\delta}}}.
  \end{align*}
  Then releasing the sequence $(\XtX{A_{1:t,:}} + W_t)_{t=1}^n$ is
  $(\varepsilon,\delta)$-differentially private with respect to
  changing a single row of $A$.

  Furthermore, each $W_t$ is distributed identically, but not
  independently, according to $\Wishart_p(\tilde L^2\Eye{p}, mk)$.
\end{theorem}

\todo[inline]{Be explicit about how to get $H_t$ and $\vec h_t$ from $W_t$.}

\begin{corollary}%
  \label{cor:wishart-alg}
  \Cref{alg:linucb} is $(\varepsilon,\delta)$-jointly differentially
  private when run using the differentially private histories
  described in \cref{thm:wishart-cont-dp}.  Now fix
  $\alpha\in(0,1)$ and define
  \begin{align*}
    H
    &\defeq \tilde L^2\paren*{\sqrt{mk} - \sqrt{d} - \sqrt{2\ln\frac{4n}{\alpha}}}^2\Eye{d} \\
    \beta_t
    &\defeq \sigma\sqrt{2\ln\frac{n}{\alpha} + \log\frac{\det V_t}{\det H}}
      + S \tilde L \paren*{\sqrt{mk} + \sqrt{d} + \sqrt{2\ln\frac{4n}{\alpha}}}
      + \tilde L \sqrt{d + 2\sqrt{d\ln\frac{2n}{\alpha}} + 2\ln\frac{2n}{\alpha}}.
  \end{align*}
  With probability at least $1-\alpha$, the following hold for rounds
  $t = 1,\dotsc,n-1$: $H_t \succeq H$ and
  $\norm{V_t}{\vec\theta^* - \tilde{\vec\theta}_t} \le
  \beta_t$.
\end{corollary}

\subsection{Differential Privacy via Additive Gaussian Noise}
\label{sec:dp-gauss}

\begin{theorem}%
  \label{thm:analyze-gauss}
  Fix $\varepsilon\in(0,1)$ and $\delta\in(0,1/e)$.  Let
  $A\in\Real^{n\times p}$ be a matrix whose rows have $l_2$-norm
  bounded by $\tilde L$.  Let $N\in\Real^{p\times p}$ be a symmetric
  random Gaussian matrix with each entry having variance
  $\tilde L^4\ln(1/\delta)/\varepsilon^2$;
  i.e.\,$N_{i,j}\sim\Normal(0,\tilde L^4\ln(1/\delta)/\varepsilon^2)$
  independently for $i \le j$ and $N_{i,j} = N_{j,i}$ for $i > j$.
  Then outputting $\XtX{A} + N$ is
  $(\varepsilon,\delta)$-differentially private with respect to
  changing a single row of $A$.
\end{theorem}

\begin{theorem}%
  \label{thm:analyze-gauss-cont}
  Fix $\varepsilon$ and a sufficiently small $\delta$, let
  $A\in\Real^{n\times p}$ be a matrix with each row having
  $\norm{}{A_{t,:}} \le \tilde L$, and let
  $m \defeq 1 + \ceil{\log_2n}$.  Generate the sequence of matrices
  $(N_t)_{t=1}^n$ using the tree mechanism (\cref{sec:tree-mechanism})
  with symmetric Gaussian noise; i.e., having equal $(i,j)$ and
  $(j,i)$ entries independently drawn (for each $i \le j$) from the
  distribution\todo{Make sure the variance below is correct!}
  \begin{align*}
    &\Normal(0, \sigma^2),
    &\text{ where } \sigma^2
    &\defeq \frac{\tilde L^4 \ln(1/\delta)}{\varepsilon^2}.
  \end{align*}
  Then releasing the sequence $(\XtX{A_{1:t,:}} + N_t)_{t=1}^n$ is
  $(\varepsilon,\delta)$-differentially private with respect to
  changing a single row of $A$.

  Furthermore, each entry of $N_t$ is distributed identically, but not
  independently, according to $\Normal(0, m\sigma^2)$.
\end{theorem}

\subsection{Regret Bounds}
\label{sec:regret-bounds}

We start by giving a ``generic'' regret bound
that holds if the confidence sets are accurate.

\begin{restatable}[Regret of \Cref{alg:linucb}]{theorem}{ThmLinUCBRegret}
  \label{thm:linucb-regret}%
  Suppose the mean rewards are bounded:
  $\abs{\innerp{\vec\theta^*}{\vec x}} \le B$ for all $\vec x \in \bigcup_t\Dset{t}$;
  and $\bar\beta_n \ge \max\set{\beta_0,\dotsc,\beta_{n-1},1}$.  In
  the event that for all rounds $t\in\set{1,\dotsc,n}$ the confidence
  sets $\Eset{t}$ contain $\vec\theta^*$ (i.e.,
  $\norm{V_{t-1}}{\vec\theta^* - \tilde{\vec\theta}_{t-1}} \le
  \beta_{t-1}$) and each $V_{t-1} = G_{t-1} + H_{t-1}$ for some
  symmetric $H,H_0,\dotsc,H_{n-1} \in \Real^{d\times d}$ with all
  $H_{t-1} \succeq H \succ 0$, the pseudo-regret of \cref{alg:linucb}
  \begin{align*}
    \widehat R_n
    &\defeq \sum_{t=1}^n \max_{\vec x\in\Dset{t}} \innerp{\vec x}{\vec\theta^*}
      - \innerp{\vec x_t}{\vec\theta*} \\
    \shortintertext{satisfies}
    \widehat R_n
    &\le 2B\bar\beta_n \sqrt{2n \log\frac{\det V_n}{\det H}}. \\
    \shortintertext{Furthermore, if each $\norm{}{\vec x_t} \le L$}
    \widehat R_n
    &\le 2B\bar\beta_n \sqrt{2dn \log\frac{\tr H + nL^2}{d\det^{1/d} H}}.
  \end{align*}
\end{restatable}

\subsection{Instance-Dependent Regret Bounds}%
\label{sec:prob-dep-regret}

\todo[inline]{Work in progress}

\begin{theorem}
  Suppose that at every round $t$ the decision set has a unique optimal
  action and every other action incurs a regret at least $\Delta_t$.
  Let $\Delta$ be a lower bound on all the $\Delta_t$.  We now give a
  \emph{problem-dependent} regret bound that depends on the $\Delta$.
\end{theorem}

\begin{proof}
  Consider the set of rounds up to $t$ on which the learner chooses a
  sub-optimal action: $B_t = \set{s \le t \given r_t > 0}$ and let
  $b_t = |B_t|$ be the number of times this happened.  Let $F_t$ be
  the Gram matrix for just the actions on those rounds:
  \begin{align*}
    F_t &\defeq \sum_{s\in B_t} \XtX{\vec x_s}.
  \end{align*}
  Now $V_t = G_t + H_t \succeq G_t + H \succeq E_t + H \eqdef U_t$, and
  $\inv{V_t} \preceq \inv{U_t}$.  We now decompose the regret into the
  sum of per-round regrets $r_t$ but in a different way than we did in
  \cref{lemma:linucb-regret}:
  \begin{align*}
    \widehat R_n
    &= \sum_{t\in B_n} r_t \le \sum_{t\in B_n} \frac{r_t^2}{\Delta}
    &\text{since } \frac{r_t}{\Delta} \ge 1 \text{ for } t\in B_n \\
    &\le \frac{4\sqrt{\bar\beta_n}}{\Delta} \sum_{t\in B_n} \min\set{B^2, \norm{\inv{V_t}}{\vec x_t}^2}
    &\text{from \eqref{eq:regret-bound-oneround} in \cref{lemma:linucb-regret}} \\
    &\le \frac{4\sqrt{\bar\beta_n}}{\Delta} \sum_{t\in B_n} \min\set{B^2, \norm{\inv{U_t}}{\vec x_t}^2} \\
    &\le \frac{8B^2\sqrt{\bar\beta_n}}{\Delta} \log\frac{\det(E_n+H)}{\det H}
    &\text{\Cref{lemma:elliptical-potential} with the sequence } (x_t, U_t)_{t\in B_n} \\
    \intertext{Now we have $\norm{}{E_n} \le \tr E_n \le L^2b_t$, and since $H =
    \rho_{\min}\Eye{d}$, the regret is bounded by}
    \widehat R_n &\le \frac{8B^2\sqrt{\bar\beta_n}}{\Delta} d\log\frac{L^2b_t + \rho_{\min}}{\rho_{\min}}.
  \end{align*}

\end{proof}


\section{Lower Bounds}
\label{sec:lower_bounds}

In this section, we present lower bounds for two versions of the problem we deal with in this work. The first, and probably the more obvious of the two, deals with the impossibility of obtaining sub-linear regret for the contextual bandit problem under the standard notion of differential privacy (under continual observation). Here, we assume user $t$ provides a context $c_t$ which actually determines the mapping of the arms into feature vectors: $\phi(c_t, a) \in \Real^d$. The sequence of choice thus made by the learner is $a_1,..., a_n \in \Aset{}^n$ which we aim to maintain private. Formally, two sequences $S = \langle (c_1, y_1),..., (c_n,y_n)\rangle$ and $S' = \langle (c'_1, y'_1),..., (c'_n,y'_n)\rangle$ are called neighbors if there exists a single $t$ such that for any $t'\neq t$ we have $(c_{t'},y_{t'}) = (c'_{t'},y'_{t'})$); and an algorithm $A$ is $(\varepsilon,\delta)$-differentially private if for any two neigboring sequences $S$ and $S'$ and any subsets of sequences of actions ${\cal S}\subset \Aset{}^n$ it holds that $\Pr[A(S)\in\mathcal{S}] \leq e^\varepsilon \Pr[A(S')\in \mathcal{S}] +\delta$.

\begin{claim}
\label{clm:standard_contextual_DP_implies_linear_regret}
For any $\varepsilon < \ln(2)$ and $\delta < 0.25$, any $(\varepsilon,\delta)$-differentially private algorithm $A$ for the contextual bandit problem must incur pseudo-regret of $\Omega(n)$.
\end{claim}
\begin{proof}
We consider a setting with two arms $\Aset{}=\{a^1,a^2\}$ and two possible contexts: $c^1$ which maps $a^1\mapsto \vec\theta^*$ and $a^2 \mapsto -\vec\theta^*$; and $c^2$ which flips the mapping. Assuming $\|\vec\theta^*\|=1$ it is evident we incur a pseudo-regret of $2$ when pulling arm $a^1$ is under context $c^2$ or pulling arm $a^2$ under $c^1$. Fix a day $t$ and a history of previous inputs and arm pulls $H_{t-1}$. Consider a pair of neighboring sequences that agree on the history $H_{t-1}$ and differ just on day $t$ --- in $S$ the context $c_t =c^1$ whereas in $S'$ it is set as $c_t =c^2$. Denote $\mathcal{S}$ as the subset of action sequences that are fixed on the first $t-1$ days according to $H_{t-1}$, have the $t$-th action be $a^1$ and on days $>t$ may have any action. Thus, applying the guarantee of differential privacy w.r.t to $\mathcal{S}$ we get that $\Pr[a_t = a^1 |~S] = \Pr[A(S)\in \mathcal{S}] \leq e^\varepsilon \Pr[a_t=a^1 |~S'] + \delta$. Consider an adversary that sets the context of day $t$ to be either $c^1$ or $c^2$ uniformly at random and independently of other days. The pseudo-regret incurred on day $t$ is thus: $2\cdot \tfrac 1 2 \left( \Pr[a_t=a^2|~S] + \Pr[a_t=a^1|~S'] \right)  \geq (1- \Pr[a_t=a^1|~S]) + e^{-\varepsilon}(\Pr[a_t=a^1|~S]-\delta) = 1 + (e^{-\varepsilon}-1)\Pr[a_t=a^2|~S] - \delta > 1 - 1\cdot \tfrac 1 2 - \tfrac 1 4 = \tfrac 1 4$. As the above applies to any day $t$,  the algorithm's pseudo-regret is $\geq \tfrac n 4$ against such random adversary.
\end{proof}

The second lower bound we show is more challenging. We show that any $\varepsilon$-differentially private algorithm for the classic MAB problem must incur \emph{an additional} pseudo-regret of $\Omega( k\log(n)/\epsilon)$ on top of the standard (non-private) regret bounds.  We consider an instance of the MAB where the leading arm is $a^1$, the rewards are drawn from a distribution over $\{-1,1\}$, and the gap between the means of arm $a^1$ and arm $a\neq a^1$ is $\Delta_a$. Simple calculation shows that for such distributions, the total-variation distance between two distributions whose means are $\mu$ and $\mu-\Delta$ is $\Delta/2$. Fix $\Delta_2, \Delta_3,...,\Delta_k$ as some small constants, and we now argue the following.
\begin{claim}
\label{clm:LB_pulling_any_arms_many_times}
Let $A$ be any $\varepsilon$-differentially private algorithm for the MAB problems with $k$ arms whose expected regret is at most $n^{3/4}$. Fix any arm $a\neq a^1$, whose difference between it and the optimal arm $a^1$ is $\Delta_a$. Then, for sufficiently large $n$s, $A$ pulls arm $a$ at least $\tfrac {\log(n)}{100\varepsilon\Delta_a}$ many times w.p. $\geq \nicefrac 1 2$.
\end{claim}
We comment that the bound $n^{3/4}$ was chosen arbitrarily, and we only require a regret upper bound of $n^{1-c}$ for some $c>0$. Of course, we could have used standard assumptions, where the regret is asymptotically smaller than \emph{any} polynomial; or discuss algorithms of regret $\tilde O(\sqrt n)$ (best min-max regret). Aiming to separate the standard lower-bounds on regret from the private bounds, we decided to use $n^{3/4}$.
As an immediate corollary we obtain the following \emph{private} regret bound:
\begin{corollary}
\label{cor:LB_private_MAB}
The expected pseudo-regret of any $\varepsilon$-differentially private algorithm from the MAB is $\Omega(k\log(n)/\varepsilon)$. Combined with the non-private bound of $\Omega\left( \sum_{a\neq a^1} \tfrac{\log(n)}{\Delta_a}\right)$ we get that the private regret bound of the $\max$ of the two terms, i.e.: $\Omega\left( \tfrac {k\log(n)}{\varepsilon}+\sum\limits_{a\neq a^1} \tfrac{\log(n)}{\Delta_a}\right)$.
\end{corollary}
\begin{proof}
Based on Claim~\ref{clm:LB_pulling_any_arms_many_times}, the expected pseudo-regret is $\geq \sum\limits_{a\neq a^1} \tfrac 1 2 \cdot \Delta_a \tfrac{\log(n)}{100 \varepsilon\Delta_a} = \tfrac{(k-1)\log(n)}{200\varepsilon}$.
\end{proof}
\begin{proof}[Proof of Claim~\ref{clm:LB_pulling_any_arms_many_times}]
Fix arm $a$. Let $\bar P$ be the vector of the $k$-probability distributions associated with the $k$ arms. Denote $E$ as the event that arm $a$ is pulled $<\tfrac{\log(n)}{100\varepsilon\Delta_a} := t_a$ many times. Our goal is to show that $\Pr_{A;~{\rm rewards}\sim \bar P}[E]<\nicefrac 1 2$.

To that end, we postulate a different distribution for the rewards of arm $a$ -- a new distribution whose mean is \emph{greater} by $\Delta_a$ than the mean reward of arm $a^1$. The total-variation distance between the given distribution and the postulated distribution is $\Delta_a$. Denote $\bar Q$ as the vector of distributions of arm-rewards (where only $P_a \neq Q_a$). We now argue that should the rewards be drawn from $\bar Q$, then the event $E$ is  very unlikely: $\Pr_{A;~{\rm rewards}\sim \bar Q}[E] \leq \tfrac 2{\Delta_a}n^{-1/4}$. Indeed, the argument is based on a standard Markov-like argument: the expected pseudo-regret of $A$ is $<n^{3/4}$, yet it is $\geq \Pr_{A;~{\rm rewards}\sim \bar Q}[E]\cdot (n-t_a) \Delta_a \geq n\tfrac{\Delta_a}2\Pr_{A;~{\rm rewards}\sim \bar Q}[E]$, for sufficiently large $n$.


We now apply a beautiful result of~\cite{KarwaVadhanFiniteSampleDP2017} (Lemma 6.1), stating that the ``effective'' group privacy between the case where the $n$ datums of the inputs are drawn iid from either distribution $P$ or from distribution $Q$ is proportional to $\varepsilon n\cdot d_{\rm TV}(P,Q)$. In our case, the key point is that we only consider this change \emph{under the event $E$}, thus the number of samples we need to redraw from the distribution $P_a$ rather than $Q_a$ is strictly smaller than $t_a$, and the elegant coupling arguming of~\cite{KarwaVadhanFiniteSampleDP2017} reduces it to $6\Delta_a \cdot t_a$. To better illustrate the argument, consider a version of the algorithm described in~\cite{KarwaVadhanFiniteSampleDP2017}: it generates a collection of $t_a$ \emph{pairs} of points, the left ones are iid samples from $P_a$ and the right ones are iid samples from $Q_a$, and, in expectation, in $(1-\Delta_a)$ fraction of the pairs the right- and the left-samples are identical. This algorithm declares {\tt success} when it never runs out of samples and {\tt failure} otherwise. Due to group privacy, oracle invocations coming from $A$ to this algorithm that either always ask for the left sample or always ask for the right sample should therefore trigger {\tt success} with similar probability, different only up to a factor of $\exp(\epsilon \Delta_a t_a)$. And seeing as {\tt success} is quite unlikely when asked only for right-samples, it is only fairly unlikely when asked only for left-samples.

Formaly, we conclude the proof by applying the result of~\cite{KarwaVadhanFiniteSampleDP2017} to infer that $\Pr_{A;~{\rm rewards}\sim \bar P}[E] \leq \exp(6\varepsilon\Delta_a t_a)\Pr_{A;~{\rm rewards}\sim \bar Q}[E] \leq \exp(0.06 \log(n)) \cdot \tfrac 2{\Delta_a}n^{-1/4} = n^{-0.19}\tfrac 2{\Delta_a} \leq \nicefrac 1 2$ for sufficiently large $n$s, proving the required.
\end{proof}

\section{Conclusions and Future Work}
\label{sec:conclusions}

\subsection{Future Work}
\label{sec:future-work}

\todo[inline]{Consider non-contextual bandits; only the rewards are
  private.  Does this help with the counter-example against optimism
  in Tor's paper?}


\bibliographystyle{plainnat}
\bibliography{references,zotero-references}

\newpage

\appendix

\section{Proof of \Cref{prop:calc-beta}}

\CalcBeta*

\begin{proof}
  By definition, $\tilde{\vec\theta}_t = \inv{V_t} \tilde{\vec u}_t$,
  $\tilde{\vec u}_t = \vec u_t + \vec h_t$, and $\vec u_t = \transp{\vec
    X_t} \vec y_t$, so that
  \begin{align*}
    \vec\theta^* - \tilde{\vec\theta}_t
    &= \vec\theta^* - \inv{V_t}(\transp{\vec X_t} \vec y_t + \vec h_t) \\
    &= \vec\theta^* - \inv{V_t}(\transp{\vec X_t} \vec X_t \vec\theta^*
      + \transp{\vec X_t} \vec\eta_t + \vec h_t)
    &\text{since } \vec y_t = \vec X_t \vec\theta^* + \vec\eta_t \\
    &= \vec\theta^* - \inv{V_t}(V_t \vec\theta^* - H_t \vec\theta^* + \vec z_t + \vec h_t)
    &\text{defining } \vec z_t \defeq \transp{\vec X_t} \vec\eta_t \\
    &= \inv{V_t}(H_t\vec\theta^* - \vec z_t - \vec h_t) \\
    \shortintertext{Multiplying both sides by $V_t^{1/2}$ gives}
    V_t^{1/2}(\vec\theta^* - \tilde{\vec\theta}_t)
    &= V_t^{-1/2}(H_t\vec\theta^* - \vec z_t - \vec h_t) \\
    \norm{V_t}{\vec\theta^* - \tilde{\vec\theta}_t}
    &= \norm{\inv{V_t}}{H_t\vec\theta^* - \vec z_t - \vec h_t}
    &\text{applying $\norm{}{\wildcard}$ to both sides}\\
    &\le \norm{\inv{V_t}}{H_t\vec\theta^*} + \norm{\inv{V_t}}{z_t}
      + \norm{\inv{V_t}}{\vec h_t} &\text{triangle inequality} \\
    &\le \norm{\inv{V_t}}{\vec z_t} + \norm{\inv{H_t}}{H_t\vec\theta^*}
      + \norm{\inv{H_t}}{\vec h_t}
    &\text{since $\inv{V_t} \preceq \inv{H_t}$ by \cref{claim:psd-matrix-props}} \\
    &= \norm{\inv{V_t}}{\vec z_t} + \norm{H_t}{\vec\theta^*} +
      \norm{\inv{H_t}}{\vec h_t}.
    &\qedhere
  \end{align*}
  We use a union bound over all $n$ rounds to bound
  $\norm{H_t}{\vec\theta^*}$ by
  $\sqrt{\norm{}{H_t}}\norm{}{\vec\theta^*} \le S\sqrt{\rho_{\max}}$
  and $\norm{\inv{H_t}}{\vec h_t}$ by $\gamma$, with probability at
  least $1-\alpha/2$.  Finally, we bound $\norm{\inv{V_t}}{\vec z_t}$
  with probability at least $1-\alpha/2$ by the ``self-normalized
  bound for vector-valued martingales'' of
  \citet[Theorem~1]{AbbasiYadkoriImprovedAlgorithmsLinear2011}.  The
  upper bound on each $\beta_t$ is another application of
  \cref{lemma:elliptical-potential}, as used in
  \cref{thm:linucb-regret}.
\end{proof}

\section{Proof of \Cref{thm:linucb-regret}}

\ThmLinUCBRegret*

\begin{lemma}\label{lemma:linucb-regret}
  Suppose the mean rewards are bounded:
  $\abs{\innerp{\vec\theta^*}{\vec x}} \le B$ for all $\vec x\in\bigcup_t\Dset{t}$;
  all the $V_t \succ 0$ are symmetric; and
  $\bar\beta_n \ge \max\set{\beta_0,\dotsc,\beta_{n-1},1}$.  In the
  event that for all rounds $t\in\set{1,\dotsc,n}$ the confidence sets
  $\Eset{t}$ contain $\vec\theta^*$ (i.e.,
  $\norm{V_{t-1}}{\vec\theta^* - \tilde{\vec\theta}_{t-1}} \le \beta_{t-1}$),
  the pseudo-regret of \cref{alg:linucb} satisfies
  \begin{align*}
    \widehat{R}_n &\le \bar\beta_n\sqrt{4n \sum_{t=1}^n \min\set{B^2,
                   \norm{\inv{V_{t-1}}}{\vec x_t}^2}}.
  \end{align*}
\end{lemma}

\begin{proof}
  At every round $t$, \cref{alg:linucb} selects the ``optimistic''
  action $\vec x_t$ satisfying
  \begin{align}\label{eq:optimistic-action}
    (\vec x_t,\bar{\vec\theta}_t)
    &= \argmax_{\vec x\in\Dset{t},\vec\theta\in\Eset{t}} \innerp{\vec\theta}{\vec x}.
  \end{align}
  Let $\vec x_t^* = \argmax_{\vec x\in\Dset{t}}\innerp{\vec\theta^*}{\vec x}$ be an optimal
  action and $r_t = \innerp{\vec\theta^*}{\vec x_t^* - \vec x_t}$ be the immediate
  pseudo-regret suffered for round $t$:
  \begin{align*}
    r_t &= \innerp{\vec\theta^*}{\vec x_t^*} - \innerp{\vec\theta^*}{\vec x_t} \\
        &\le \innerp{\bar{\vec\theta}_t}{\vec x_t} - \innerp{\vec\theta^*}{\vec x_t}
        &\text{from \eqref{eq:optimistic-action} since }
          \vec x_t^*\in\Dset{t}, \vec\theta^*\in\Eset{t} \\
        &= \innerp{\bar{\vec\theta}_t - \vec\theta^*}{\vec x_t} \\
        &= \innerp{V_{t-1}^{1/2}(\bar{\vec\theta}_t - \vec\theta^*)}{V_{t-1}^{-1/2} \vec x_t} \\
        &\le \norm{V_{t-1}}{\bar{\vec\theta}_t - \vec\theta^*}\norm{\inv{V_{t-1}}}{\vec x_t}
        &\text{by Cauchy-Schwarz}\\
        &\le \paren[\big]{\norm{V_{t-1}}{\bar{\vec\theta}_t - \tilde{\vec\theta}_{t-1}}
          + \norm{V_{t-1}}{\vec\theta^* - \tilde{\vec\theta}_{t-1}}}
          \norm{\inv{V_{t-1}}}{\vec x_t}
        &\text{by the triangle inequality}\\
        &\le 2\beta_{t-1}\norm{\inv{V_{t-1}}}{\vec x_t}
        &\text{since } \bar{\vec\theta}_t, \vec\theta^* \in \Eset{t} \\
        &\le 2\bar\beta_n\norm{\inv{V_{t-1}}}{\vec x_t}
        &\text{since } \bar\beta_n \ge \beta_{t-1}.
  \end{align*}
  From our assumptions that the mean absolute reward is bounded by $B$
  and $\bar\beta_n \ge 1$, we also get that
  $r_t \le 2B \le 2B\bar\beta_n$.  Putting these together,
  \begin{align}\label{eq:regret-bound-oneround}
    r_t &\le 2\bar\beta_n \min\set{B, \norm{\inv{V_{t-1}}}{\vec x_t}}
  \end{align}
  Now we apply Jensen's inequality as follows:
  \begin{align*}
    \widehat R_n^2 &= n^2 \paren[\Big]{\sum_{t=1}^n \frac{r_t}{n}}^2
                   \le n^2 \sum_{t=1}^n \frac{r_t^2}{n} = n\sum_{t=1}^nr_t^2 \\
    \widehat R_n &\le \sqrt{n\sum_{t=1}^n r_t^2}
                  =  2\bar\beta_n\sqrt{n\sum_{t=1}^n\min\set{B^2,
                  \norm{\inv{V_{t-1}}}{\vec x_t}^2}}.
                  \qedhere
  \end{align*}
\end{proof}

\begin{lemma}[Elliptical Potential]\label{lemma:elliptical-potential}
  Let $\vec x_1,\dotsc,\vec x_n \in \Real^d$ be bounded vectors with each
  $\norm{}{\vec x_t} \le L$, $U_0\in\Real^{d\times d}$ a positive
  definite matrix, and $B \ge 0$.  Define
  $U_t \defeq U_0 + \sum_{s=1}^t \vec x_s \transp{\vec x_s}$ for all
  $t\in\set{1,\dotsc,n}$.  Then
  \begin{align*}
    \sum_{t=1}^n\min\set{B^2, \norm{\inv{U_{t-1}}}{\vec x_t}^2}
    &\le 2B^2 \log\frac{\det U_n}{\det U_0}
      \le 2B^2d \log\frac{\tr U_0+nL^2}{d\det^{1/d} U_0}.
  \end{align*}
\end{lemma}

\begin{proof}
  We begin by deriving the first inequality with $B=1$, using the fact
  that $\min\set{1, u} \le 2\log(1+u)$ for any $u \ge 0$:
  \begin{align*}
    \sum_{t=1}^n\min\set{1, \norm{\inv{U_{t-1}}}{\vec x_t}^2}
    &\le 2 \sum_{t=1}^n \log(1 + \norm{\inv{U_{t-1}}}{\vec x_t}^2).
  \end{align*}
  We will show that this last summation is $2\log(\det U_n/\det
  U_0)$.  For $t \ge 1$ we have
  \begin{align*}
    U_t &= U_{t-1} + \vec x_t \transp{\vec x_t}
         = U_{t-1}^{1/2}
         \paren[\big]{I + U_{t-1}^{-1/2} \vec x_t \transp{\vec x_t} U_{t-1}^{-1/2}}
         U_{t-1}^{1/2} \\
    \det U_t &=\det U_{t-1}
              \det\paren[\big]{I +
              U_{t-1}^{-1/2} \vec x_t \transp{\vec x_t}
              U_{t-1}^{-1/2}}.
  \end{align*}
  Consider the eigenvectors of the matrix $I + \vec y \transp{\vec y}$
  for an arbitrary vector $\vec y \in \Real^d$.  We know that $\vec y$
  itself is an eigenvector with eigenvalue $1+\norm{}{\vec y}^2$:
  \begin{align*}
    (I + \vec y \transp{\vec y}) \vec y
    &= \vec y + \vec y \innerp{\vec y}{\vec y} = (1+\norm{}{\vec y}^2)\vec y.
  \end{align*}
  Moreover, since $I + \vec y \transp{\vec y}$ is symmetric, every
  other eigenvector $\vec u$ is orthogonal to $\vec y$, so that
  \begin{align*}
    (I + \vec y \transp{\vec y}) \vec u
    &= \vec u + \vec u \innerp{\vec y}{\vec u} = \vec u.
  \end{align*}
  Therefore the only eigenvalues of $I + \vec y \transp{\vec y}$ are
  $1+\norm{}{\vec y}^2$ (with eigenvector $\vec y$) and 1.  In our
  case $\vec y = U_{t-1}^{-1/2} \vec x_t$ and
  $\norm{}{\vec y}^2 = \transp{\vec x_t} \inv{U_{t-1}} \vec x_t =
  \norm{\inv{U_{t-1}}}{\vec x_t}^2$, so we get our first inequality:
  \begin{align*}
    \det U_n &= \det U_0 \prod_{t=1}^n(1 + \norm{\inv{U_{t-1}}}{\vec x_t}^2) \\
    2\log\frac{\det U_n}{\det U_0}
            &= 2\sum_{t=1}^n\log(1+\norm{\inv{U_{t-1}}}{\vec x_t}^2).
  \end{align*}
  For $B \neq 1$, we apply the above result with each $\vec x_t$ scaled by
  $B^{-1}$ and $U_0,\dotsc,U_n$ scaled by $B^{-2}$:
  \begin{align*}
    \sum_{t=1}^n\min\set{B^2,\norm{\inv{U_{t-1}}}{\vec x_t}^2}
    &= B^2 \sum_{t=1}^n\min\set{1,\norm{\inv{U_{t-1}}}{B^{-1} \vec x_t}^2}
      \le 2B^2 \log\frac{\det(B^{-2}U_n)}{\det(B^{-2}U_0)}.
  \end{align*}
  To get the second inequality, we apply the arithmetic-geometric
  mean inequality to the eigenvalues $\lambda_i$ of $U_n$:
  \begin{align*}
    \det U_n &= \prod_{i=1}^d \lambda_i
              \le \paren[\Big]{\frac{1}{d} \sum_{i=1}^d \lambda_i}^d
              = \paren{(1/d)\tr U_n}^d
              \le \paren{(\tr U_0 + nL^2)/d}^d \\
    2B^2 \log\frac{\det U_n}{\det U_0}
            &\le 2B^2d \log\frac{\tr U_0 + nL^2}{d\det^{1/d}U_0}
              \qedhere
  \end{align*}
\end{proof}

\begin{proof}[Proof of \Cref{thm:linucb-regret}]
  From our assumptions and \cref{lemma:linucb-regret}, we know that
  the Linear UCB regret is bounded by
  \begin{align*}
    \widehat{R}_n &\le 2\bar\beta_n\sqrt{n\sum_{t=1}^n\min\set{B^2,
                   \norm{\inv{V_{t-1}}}{\vec x_t}^2}}.
  \end{align*}
  Define $U_t \defeq G_t + H$, so that $U_t \preceq V_t$ and by
  \cref{claim:psd-matrix-props} it follows that
  $\inv{V_t} \preceq \inv{U_t}$, which means
  $\norm{\inv{V_{t-1}}}{\vec x_t}^2 \le \norm{\inv{U_{t-1}}}{\vec x_t}^2$.  We
  now apply \cref{lemma:elliptical-potential} to the sequence $U_t$ to
  get
  \begin{align*}
    \sum_{t=1}^n\min\set{B^2,\norm{\inv{V_{t-1}}}{\vec x_t}^2}
    &\le \sum_{t=1}^n\min\set{B^2,\norm{\inv{U_{t-1}}}{\vec x_t}^2} \\
    &\le 2B^2 \log\det\frac{U_n}{U_0} \\
    &\le 2B^2d \log\frac{\tr U_0 + nL^2}{d \det^{1/d} U_0}.
  \end{align*}
  To get the required bounds, it only remains to see that $U_0 = H$ and
  that $\det U_n \le \det V_n$ (again by
  \cref{claim:psd-matrix-props}).
\end{proof}

\section{Privacy Proofs}

\begin{theorem}[{\citealp[Theorem~4.1]{SheffetPrivateApproxRegression2015}}]%
  \label{thm:wishart-dp}%
  Fix $\varepsilon\in(0,1)$ and $\delta\in(0,1/e)$.  Let
  $A\in\Real^{n\times p}$ be a matrix whose rows have $l_2$-norm
  bounded by $\tilde L$.  Let $W$ be a matrix sampled from the
  $d$-dimensional Wishart distribution with $k$ degrees of freedom
  using the scale matrix $\tilde L^2\Eye{p}$ (i.e.\
  $W \sim \Wishart_p(\tilde L^2\Eye{p}, k)$) for
  $k \ge p + \floor[\big]{\frac{14}{\varepsilon^2}\cdot 2\log(4/\delta)}$.
  Then outputting $\XtX{A} + N$ is
  $(\varepsilon,\delta)$-differentially private with respect to
  changing a single row of $A$.
\end{theorem}

\section{Useful Results}

\begin{claim}[{\citealp[Theorem~7.8]{ZhangMatrixTheory2011}}]%
  \label{claim:psd-matrix-props}%
  If $A \succeq B \succeq 0$, then
  \begin{enumerate}[nolistsep]
  \item $\rank(A) \ge \rank(B)$
  \item $\det A \ge \det B$
  \item $\inv{B} \succeq \inv{A}$ if $A$ and $B$ are nonsingular.
  \end{enumerate}
\end{claim}

\begin{claim}[{\citealp[Corollary to
    Lemma~1,][p.~1325]{LaurentAdaptiveEstimation2000}}]%
  \label{claim:chi2-tails}
  If $U\sim\chi^2(d)$ and $\alpha\in(0,1)$,
  \begin{align*}
    \Prob[\bigg]{U \ge d + 2\sqrt{d\ln\frac{1}{\alpha}} + 2\ln\frac{1}{\alpha}} &\le \alpha,
    & \Prob[\bigg]{U \le d - 2\sqrt{d\ln\frac{1}{\alpha}}} &\le \alpha.
  \end{align*}
\end{claim}

\begin{claim}[{\citealp[Adaptation
    of][Corollary~5.35]{VershyninRandomMatrices2010}}]%
  \label{claim:gaussian-matrix-tails}
  Let $A$ be an $n\times d$ matrix whose entries are independent
  standard normal variables.  Then for every $\alpha\in(0,1)$, with
  probability at least $1-\alpha$ it holds that
  \begin{align*}
    \sigma_{\min}(A), \sigma_{\max}(A) &\in \sqrt{n} \pm (\sqrt{d} + \sqrt{2\ln(2/\alpha)})
  \end{align*}
\end{claim}

\begin{claim}[{\citealp[Lemma~A.3]{SheffetPrivateApproxRegression2015}}]%
  \label{claim:wishart-tails}
  Fix $\alpha\in(0,1/e)$ and let $W\sim\Wishart_d(V, k)$ with $\sqrt{m}
  > \sqrt{d} + \sqrt{2\ln(2/\alpha)}$.  Then with probability at least
  $1-\alpha$ it holds that for every $j = 1,2,\dotsc,d$:
  \begin{align*}
    \sigma_j(W) &\in \paren*{\sqrt{m} \pm \paren[\Big]{\sqrt{d} + \sqrt{2\ln(2/\alpha)}}}^2 \sigma_j(V).
  \end{align*}
\end{claim}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\todo[inline]{Remove following material from final version.}

\section{Ellipsoidal Confidence Sets}
\label{sec:ellips-conf-bounds}

Throughout this section we will fix some particular round $t \le n$
and suppress the subscripted $t$.

We will consider confidence sets for the value of $\theta^*$ that are
ellipsoidal with centre $\hat{\theta}$, of the form
$\set{\theta\in\Real^d \given \norm{V}{\theta-\hat{\theta}}^2 \le
  \beta}$.  Then the upper confidence bound of an action $x$ is
\begin{align*}
  \UCB(x) &= \max_{\theta: \norm{V}{\theta-\hat\theta}^2 \le \beta} \innerp{x}{\theta}
\end{align*}
We solve this constrained optimization problem by the method of
Lagrange multipliers:
\begin{align*}
  \mathcal{L}(\theta, \lambda) &= \innerp{x}{\theta} - \lambda(\norm{V}{\theta-\hat\theta}^2 - \beta) \\
  \nabla_\theta\mathcal{L}(\theta, \lambda) &= \transp{x} - \lambda\transp{(\theta-\hat\theta)}V = 0 \\
  \theta &= \hat\theta + \frac{{\inv{V}x}}{\lambda}
  \shortintertext{To find $\lambda$:}
  \beta &= \norm{V}{\theta-\hat\theta}^2
          = \norm[\bigg]{V}{\frac{\inv{V}x}{\lambda}}^2
          = \frac{1}{\lambda^2}\transp{x}\inv{V}x \\
  \lambda &= \norm{\inv{V}}{x} / \sqrt\beta
  \intertext{Which we substitute into the expression above:}
  \UCB(x) &= \innerp[\bigg]{x}{\hat\theta + \frac{\sqrt\beta}{\norm{\inv{V}}{x}}\inv{V}x} \\
          &= \innerp{x}{\hat\theta} + \sqrt\beta \norm{\inv{V}}{x}.
\end{align*}
We have therefore shown that if we construct an ellipsoidal confidence
set with an appropriate $V$ and $\beta$, then the requirements of
\cref{thm:linucb-regret} will be satisfied with the above $\UCB$
index.

Let $X_s \in \Real^d$ be the action vector selected by the algorithm at
time $s$, so that $X$ is a $t \times d$ matrix.  The \emph{Gram matrix} is given by
$G \defeq \transp{X}X \in \Real^{d \times d}$.  Let $y_s$ be the
reward received at time $s$, so that $y\in\Real^t$.

We will take $\hat{\theta}$ to be a regularized solution of the
least-squares system
\begin{align*}
  X\theta &\approx y,
\end{align*}
regularized by the symmetric positive definite matrix $H \succeq 0$:
\begin{align*}
  \hat{\theta} &= \argmin_\theta \frac{1}{2}\norm{}{X\theta - y}^2 + \frac{1}{2}\norm{H}{\theta}^2 \\
               &= \inv{\paren{\transp{X}X + H}} \transp{X}y \\
               &= \inv{\paren{\transp{X}X + H}} \transp{X} \paren{X\theta^* + \eta} \\
               &= \theta^* + \inv{\paren{\transp{X}X + H}}\paren{\transp{X}\eta - H\theta^*} \\
               &= \theta^* + \inv{V}Z - \inv{V}H\theta^*
\end{align*}
where we used the fact that $y = X\theta^* + \eta$ (and $\eta_s$ is the
noise in the reward at round $s$) and we defined $V \defeq \transp{X}X
+ H$ and $Z\defeq X^T\eta$.

\begin{lemma}\label{lemma:subgaussian-z}
  Under \cref{assumption:subgaussian-noise} and for any
$\lambda\in\Real^d$, the random variable $Z = \transp{X}\eta$ satisfies
  \begin{align*}
    \Ex{\exp(\transp{\lambda}Z)} \le \exp\paren{\norm{G}{\lambda}^2/2}.
  \end{align*}

  \begin{proof}
    We have
    \begin{align*}
      \Ex{\exp(\transp{\lambda}Z)}
      &= \Ex{\exp(\transp{\lambda}\transp{X}\eta)} \\
      &= \Ex[\Big]{\Ex[\Big]{\prod_{s=1}^t \exp\paren{\transp{\lambda}X_s\eta_s} \given \mathcal{F}_t}} \\
      &= \Ex[\Big]{\Ex[\Big]{\exp\paren{\transp{\lambda}X_t\eta_t} \given \mathcal{F}_t} \prod_{s=1}^{t-1} \exp\paren{\transp{\lambda}X_s\eta_s}} \\
      &\le \Ex[\Big]{\exp\paren{{(\transp{\lambda}X_t)}^2/2}
        \prod_{s=1}^{t-1} \exp\paren{\transp{\lambda}X_s\eta_s}}
      &\text{since $\eta_t$ is conditionally 1-subgaussian given $\mathcal{F}_t$}\\
      &\le \Ex[\Big]{\prod_{s=1}^t\exp({(\transp{\lambda}X_s)}^2/2)}
      &\text{similarly conditioning on $\mathcal{F}_{t-1},\mathcal{F}_{t-2},\dotsc$}\\
      &= \Ex[\Big]{\exp\paren[\Big]{\sum_{s=1}^t\frac{\transp{\lambda}X_s\transp{X_s}\lambda}{2}}} \\
      &= \exp\paren[\Big]{\frac{1}{2}\transp{\lambda}\transp{X}X\lambda}
        = \exp(\norm{G}{\lambda}^2/2)
      &\qedhere
    \end{align*}
  \end{proof}
\end{lemma}


\begin{lemma}\label{lemma:z-norm-bounded-whp}
  Suppose for any $\lambda\in\Real^d$ the random variable
  $Z=\transp{X}\eta$ satisfies
  $\Ex{\exp(\transp{\lambda}Z)} \le \exp(\norm{G}{\lambda}^2/2)$ (see,
  for example, \cref{lemma:subgaussian-z}).  Let
  $0 \prec H \in \Real^{d\times d}$ be any symmetric positive definite
  matrix.  Then for any $0 < \delta \le 1$, we have
  \begin{align*}
    \Prob[\Bigg]{\norm{\inv{(G+H)}}{Z} \ge \sqrt{2\log\frac{1}{\delta} + \log\frac{\det(G+H)}{\det H}}} &\le \delta.
  \end{align*}

  \begin{proof}
    We define
    \begin{align*}
      M_\lambda &\defeq \exp\paren[\Big]{\transp{\lambda}Z - \frac{1}{2}\transp{\lambda}G\lambda}.
    \end{align*}
    Now consider $\lambda\sim\mathcal{N}(0, \inv{H})$ to be a random
    variable with the density function $h(\lambda)$.  Define
    \begin{align*}
      \bar{M}
      &\defeq \int M_\lambda \,h(\lambda)\,d\lambda \\
      &= \frac{1}{\sqrt{{(2\pi)}^d \det\inv{H}}} \int\exp\paren[\Big]{\transp{\lambda}Z
        - \frac{1}{2}\transp{\lambda}G\lambda
        - \frac{1}{2}\transp{\lambda}H\lambda
        } \,d\lambda.
    \end{align*}
    We complete the square in the integrand:
    \begin{align*}
      \transp{\lambda}Z - \frac{1}{2}\transp{\lambda}G\lambda - \frac{1}{2}\transp{\lambda}H\lambda
      &= \frac{1}{2}\transp{Z}\inv{(G+H)}Z - \frac{1}{2}\transp{(\lambda - \inv{(G+H)}Z)}(G+H)(\lambda-\inv{(G+H)}Z) \\
      &= \frac{1}{2} \norm{\inv{(G+H)}}{Z}^2 - \frac{1}{2}\norm{G+H}{\lambda-\inv{(G+H)}Z}^2
    \end{align*}
    which we substitute into the previous equation to get
    \begin{align*}
      \bar{M}
      &= \frac{\exp\paren[\big]{\frac{1}{2} \norm{\inv{(G+H)}}{Z}^2}}{\sqrt{{(2\pi)}^d \det\inv{H}}}
        \int \exp\paren[\Big]{-\frac{1}{2} \norm{G+H}{\lambda-\inv{(G+H)}Z}^2} \,d\lambda \\
      &= \sqrt{\frac{\det H}{\det(G+H)}} \exp\paren[\Big]{\frac{1}{2}\norm{\inv{(G+H)}}{Z}^2}, \\
      \log\bar{M} &= \frac{1}{2}\norm{\inv{(G+H)}}{Z}^2 + \frac{1}{2}\log\frac{\det H}{\det(G+H)}.
    \end{align*}
    Our assumption gives us
    \begin{align*}
      \Ex{\exp(\transp{\lambda}Z)} &\le \exp\paren[\Big]{\frac{1}{2}\transp{\lambda}G\lambda} \\
      \Ex[\bigg]{\frac{\exp(\transp{\lambda}Z)}{\exp(\frac{1}{2}\transp{\lambda}G\lambda)}}
      &= \Ex{M_\lambda} \le 1, &\text{for all }\lambda\in\Real^d.
    \end{align*}
    Now, by Fubini's theorem we can exchange the expectation with the
    integral over $\lambda$:
    \begin{align*}
      \Ex{\bar{M}} &= \Ex[\Big]{\int M_\lambda\,h(\lambda)\,d\lambda} = \int \Ex{M_\lambda} \,h(\lambda)\,d\lambda \le 1
    \end{align*}
    and use a Chernoff bound:
    \begin{align*}
      \Prob{\log(\bar{M}) \ge u} &\le \exp(-u) \\
      \Prob[\Big]{\frac{1}{2}\norm{\inv{(G+H)}}{Z}^2 \ge u + \frac{1}{2}\log\frac{\det(G+H)}{\det H}} &\le \exp(-u) \\
      \Prob[\Big]{\norm{\inv{(G+H)}}{Z} \ge \sqrt{2u + \log\frac{\det(G+H)}{\det H}}} &\le \exp(-u).
    \end{align*}
    Substituting $u = \log(1/\delta)$ completes the proof.
  \end{proof}

\end{lemma}

%===============================================================================
% \hrule

% We see that
% \begin{align*}
%   \max_{\lambda\in\Real^d} \exp\paren[\Big]{\transp{\lambda}Z - \frac{1}{2}\transp{\lambda}G\lambda}
%   &= \exp\paren[\Big]{\max_{\lambda\in\Real^d} \transp{\lambda}Z - \frac{1}{2}\transp{\lambda}G\lambda}.
% \end{align*}
% The maximizer of this expression is given by taking the gradient and
% setting it to zero:
% \begin{align*}
%   \nabla_\lambda\brck[\Big]{\transp{\lambda}Z - \frac{1}{2}\transp{\lambda}G\lambda}_{\lambda=\lambda^*} = Z - G\lambda^* &= 0 \\
%   \lambda^* &= \inv{G}Z
% \end{align*}
% and thus
% \begin{align*}
%   \max_{\lambda\in\Real^d} \exp\paren[\Big]{\transp{\lambda}Z - \frac{1}{2}\transp{\lambda}G\lambda}
%   &= \exp\paren[\Big]{\transp{Z}\inv{G}Z - \frac{1}{2}\transp{Z}\inv{G}Z} \\
%   &= \exp\paren[\Big]{\frac{1}{2}\norm{\inv{G}}{Z}^2}.
% \end{align*}
% We define
% \begin{align*}
%   M_\lambda \defeq \exp\paren[\Big]{\transp{\lambda}Z - \frac{1}{2}\transp{\lambda}G\lambda}
% \end{align*}
% and use Chernoff's bound:
% \begin{align*}
%   \Pr\paren[\Big]{\frac{1}{2}\norm{\inv{G}}{Z}^2 > u}
%   &= \Pr\paren[\Big]{\max_{\lambda\in\Real^d}M_\lambda > u} \\
%   &\le \exp(-u) \Ex[\Big]{\max_{\lambda\in\Real^d} M_\lambda}.
% \end{align*}
% \hrule
%===============================================================================

\subsection{Ridge Regression}
\label{sec:ridge-regression}

We will now instantiate a concrete algorithm based on \emph{ridge
  regression}, i.e.\ using the regularizer $H=\rho I$ for some
$\rho>0$.  Thus we will have
\begin{align*}
  V_t &\defeq \rho I + \sum_{s=1}^t X_s\transp{X_s}
  \shortintertext{and}
  \hat{\theta}_t - \theta^* &= \inv{V_t}Z_t - \inv{V_t}H\theta^* \\
                          &= \inv{V_t}Z_t - \rho\inv{V_t}\theta^* \\
  V_t^{1/2}(\hat{\theta}_t - \theta^*) &= V_t^{-1/2}Z - \rho V_t^{-1/2}\theta^* \\
  \norm{V_t}{\hat{\theta}_t - \theta^*} &= \norm{}{V_t^{-1/2}Z - \rho V_t^{-1/2}\theta^*} \\
  &\le \norm{\inv{V_t}}{Z} + \rho\norm{\inv{V_t}}{\theta^*}.\
                          &\text{Triangle inequality} \\
  &\le \norm{\inv{V_t}}{Z} + \sqrt\rho \norm{}{\theta^*} &\text{Since } V_t \succeq \rho I.
\end{align*}
We now use \cref{lemma:subgaussian-z,lemma:z-norm-bounded-whp} in our
confidence bound:
\begin{align*}
  \Prob[\Bigg]{\norm{V_t}{\hat{\theta}_t - \theta^*} > \sqrt\rho\norm{}{\theta^*} + \sqrt{2\log\frac{1}{\delta} + \log\frac{\det V_t}{\det \rho I}}}
  &\le \delta.
\end{align*}
We assume that $\norm{}{\theta^*} \le S$.  We use the union bound,
replacing $\delta$ with $\delta/n$ to get a bound that holds uniformly
at every round:
\begin{align*}
  \sqrt{\beta_t} &= \sqrt\rho S + \sqrt{2\log\frac{n}{\delta} + \log\frac{\det V_t}{\det \rho I}}.
\end{align*}
Applying \cref{thm:linucb-regret} gives us the regret bound
\begin{align*}
  \widehat{R}_n
  &\le \sqrt{8dn\beta_{n-1}\log\frac{\tr V_0 + nL^2}{d\det^{1/d} V_0}}
    = \sqrt{8dn\beta_{n-1}\log\frac{d\rho + nL^2}{d\rho}}
  \shortintertext{where}
  \sqrt{\beta_{n-1}}
  &= \sqrt\rho S + \sqrt{2\log\frac{n}{\delta} + \log\frac{\det V_{n-1}}{\det \rho I}} \\
  &\le \sqrt\rho S + \sqrt{2\log\frac{n}{\delta} + d\log\paren*{1 + \frac{nL^2}{d\rho}}}.
\end{align*}
Choosing $\delta=1/n$ and constant $\rho$ gives $\sqrt{\beta_{n-1}} =
O(d^{1/2}\log^{1/2}(n/d))$ and thus the expected regret of Linear UCB
with ellipsoidal confidence sets satisfies
\begin{align*}
  \widehat{R}_n &= O(\beta_n^{1/2}\sqrt{dn\log(n/d)}) = O(d\log(n/d)\sqrt{n}).
\end{align*}




% \section{Facts About Random Variables}

% \begin{definition}[Subgaussian random variable]\label{def:subG}
%   A real-valued random variable $X$ is \emph{$\sigma^2$-subgaussian}
%   if its moment-generating function satisfies:
%   \begin{align*}
%     \mgf_X(s) \defeq \Ex{\exp(sX)} &\le \exp(s^2\sigma^2/2).
%   \end{align*}
% \end{definition}

% \begin{definition}[Subexponential random variable]\label{def:subExp}
%   A real-valued random variable $X$ is \emph{$\lambda$-subexponential}
%   if its moment-generating function satisfies:
%   \begin{align*}
%     \mgf_X(s) \defeq \Ex{\exp(sX)} &\le \exp(s^2\lambda^2/2),
%     &\text{for all } \abs{s}\le 1/\lambda.
%   \end{align*}
% \end{definition}

% \begin{definition}[Subgaussian/subexponential random vector]
%   A random vector $X\in\Real^b$ is subgaussian (subexponential) if the
%   random variable $\innerp{v}{X}$ is subgaussian (subexponential) for
%   any $v\in\Real^n$ with $\norm{}{v}=1$.
% \end{definition}

% \begin{definition}[Subgaussian/subexponential random matrix]
%   A random matrix $W\in\Real^{n\times m}$ is subgaussian (subexponential)
%   if the random vector $Wu\in\Real^n$ is subgaussian
%   (subexponential) for any $u\in\Real^m$ with $\norm{}{u}=1$.
% \end{definition}

\section{Other Proofs}

\begin{lemma}[{\citealp[Lemma~9]{AbbasiYadkoriImprovedAlgorithmsLinear2011}}]\label{lemma:martingale-mgf}
  Let $X_1, X_2, \dotsc \in \Real$ and
  $\sigma_1, \sigma_2, \dotsc \in \Real$ be sequences of random
  variables and let
  $\mathcal{F}_1 \subset \mathcal{F}_2 \subset \dotsb$ be a
  filtration.  Suppose that each $X_t \in \mathcal{F}_t$ and almost
  surely
  $\Ex{\exp(\lambda X_t)\given \mathcal{F}_{t-1}} \le
  \exp(\sigma_t^2\lambda^2/2)$ (i.e. each
  $\sigma_t \in \mathcal{F}_{t-1}$ and $X_t$ is conditionally
  $\sigma_t$-subgaussian given $\mathcal{F}_{t-1}$).  For any
  $\lambda\in\Real$, define
  \begin{align*}
    M_{t,\psi} &\defeq \exp\paren[\bigg]{\sum_{s=1}^t \psi X_s - \frac{1}{2}\sigma_s^2\psi^2}.
  \end{align*}
  Let $\tau$ be a stopping time with respect to
  ${(\mathcal{F}_t)}_{t=1}^\infty$.  Then $M_{\tau,\psi}$ is almost surely
  well-defined and $\Ex{M_{\tau,\psi}} \le 1$.

  \begin{proof}
    We will show that ${(M_{t,\psi})}_{t=1}^\infty$ is a
    supermartingale.  Let
    \begin{align*}
      D_{t,\psi} &\defeq \exp\paren[\bigg]{\psi X_t - \frac{1}{2}\sigma_t^2\psi^2}.
    \end{align*}
    By the conditional subgaussianity of $X_t$, we have
    \begin{align*}
      \Ex{D_{t,\psi}\given\mathcal{F}_{t-1}}
      &= \Ex{\exp(\psi X_t)\given\mathcal{F}_{t-1}}
        \cdot \exp\paren[\bigg]{-\frac{1}{2}\sigma_t^2\psi^2}
      \le \exp\paren[\bigg]{\frac{1}{2}\sigma_t^2\psi^2}
        \cdot \exp\paren[\bigg]{-\frac{1}{2}\sigma_t^2\psi^2}
      = 1.
    \end{align*}
    It is also clear that $D_{t,\psi}$ is $\mathcal{F}_t$-measurable
    (since $X_t,\sigma_t \in \mathcal{F}_t$), and therefore so is
    $M_{t,\psi}$.  Furthermore,
    \begin{align*}
      \Ex{M_{t,\psi}\given\mathcal{F}_{t-1}}
      &= \Ex{M_{t-1,\psi}\cdot D_{t,\psi}\given\mathcal{F}_{t-1}}
        = M_{t-1,\psi} \Ex{D_{t,\psi}\given\mathcal{F}_{t-1}}
        \le M_{t-1,\psi},
    \end{align*}
    showing that $M_{t,\psi}$ is indeed a supermartingale and therefore
    by Doob's optional stopping theorem
    $\Ex{M_{\tau,\psi}} \le \Ex{M_{0,\psi}} = 1$.

    Now, we argue that $M_{\tau,\psi}$ is well-defined.  By the
    convergence theorem for non-negative supermartingales,
    $M_{\infty,\psi} = \lim_{t\to\infty}M_{t,\psi}$ is almost surely
    well-defined.  Hence, $M_{\tau,\psi}$ is almost surely
    well-defined independently of whether $\tau<\infty$ holds or not.
    Next, we show that $\Ex{M_{\tau,\psi}} \le 1$.  For this, let
    $Q_{t,\psi} \defeq M_{\min\{\tau,t\},\psi}$ be a stopped
    version of ${(M_{t,\psi})}_t$.  By Fatou's lemma,
    $\Ex{M_{\tau,\psi}} = \Ex{\liminf_{t\to\infty}Q_{t,\psi}} \le
    \liminf_{t\to\infty}\Ex{Q_{t,\psi}} \le 1$, showing the
    $\Ex{M_{\tau,\psi}} \le 1$ indeed holds.
  \end{proof}
\end{lemma}

\begin{lemma}
  Let $n\in\Nat$ and $\varepsilon>0$ and $\sigma^2>0$.  Let
  $X_1,X_2,\dotsc,X_n\in\Real$ and
  $\sigma_1,\sigma_2,\dotsc,\sigma_n\in\Real$ be sequences of random
  variables and let
  $\mathcal{F}_1 \subset \mathcal{F}_2 \subset \dotsb \subset
  \mathcal{F}_n$ be a filtration.  Suppose that each
  $X_t\in\mathcal{F}_t$ and almost surely each $\sigma_t \le \sigma$
  and each $X_t$ is conditionally $\sigma_t$-subgaussian given
  $\mathcal{F}_{t-1}$.  Then
  \begin{align*}
    \Prob[\Big]{\exists t \le n. \sum_{s=1}^t X_s \ge
    \sqrt{2\gamma_nV_t\log(N/\delta)}}
    &\le \delta,
  \end{align*}
  where
  \begin{align*}
    V_t &\defeq \max\set[\Big]{\varepsilon,\sum_{s=1}^t \sigma_s^2}, &
    \gamma_n &\defeq 1 + \frac{1}{\log(n)}, &
    N &\defeq 1 + \ceil*{\frac{\log(n\sigma^2/\varepsilon)}{\log(\gamma_n)}}.
  \end{align*}

  \begin{proof}
    For $\psi\in\Real$ define
    \begin{align*}
      M_{t,\psi} &\defeq \exp\paren[\bigg]{\sum_{s=1}^t \psi X_s - \frac{1}{2}\psi^2\sigma_s^2}.
    \end{align*}
    If $\tau \le n$ is a stopping time with respect to $\mathcal{F}$,
    then $\Ex{M_{\tau,\psi}} \le 1$ by \cref{lemma:martingale-mgf}.
    Therefore by Markov's inequality,
    \begin{align*}
      \delta/N
      &\ge \Prob{M_{\tau,\psi} \ge N/\delta} \\
      &= \Prob[\bigg]{\exp\paren[\Big]{\sum_{s=1}^\tau \psi X_s - \frac{\psi^2\sigma_s^2}{2}} \ge N/\delta} \\
      &= \Prob[\bigg]{\psi \sum_{s=1}^\tau X_s - \frac{\psi\sigma_s^2}{2} \ge \log(N/\delta)} \\
      &\ge \Prob[\bigg]{\sum_{s=1}^\tau X_s \ge \frac{\log(N/\delta)}{\psi} + \frac{\psi V_\tau}{2}},
      &\text{since } V_\tau \ge \sum_{s=1}^\tau \sigma_s^2.
    \end{align*}
    To get the tightest tail bound, we want to minimize the quantity
    on the right side of the inequality; it attains a minimum of
    $\sqrt{2V_\tau \log(1/\delta)}$ at
    $\psi_{\min} \defeq \sqrt{2\log(N/\delta)/V_\tau}$.  Furthermore,
    for any $\psi\ge\psi_{\min}$ that quantity is at most
    $\psi V_\tau$.

    However, since $V_\tau$ is a random quantity, we cannot simply set
    $\psi \defeq \psi_{\min}$.  Instead, we recognize that
    $V_\tau \in [\varepsilon, n\sigma^2]$ almost surely, and we can
    logarithmically cover this range with the $N$ values
    $\varepsilon\gamma_n^{k-1}$ for $k=1,2,\dotsc,N$; at least one of
    these must lie in the interval $[V_\tau/\gamma_n, V_\tau]$.  Then
    we can define the corresponding values
    \begin{align*}
      \psi_k &\defeq \sqrt{\frac{2\log(N/\delta)}{\varepsilon\gamma_n^{k-1}}},
              &\text{for } k=1,2,\dotsc,N;
    \end{align*}
    there must be some value of $k$ for which
    $\psi_k \in [\psi_{\min}, \gamma_n\psi_{\min}]$.  Using a union bound
    over these $N$ values, we get
    \begin{align*}
      \delta
      &\ge \Prob[\bigg]{\exists k\in[N]. \sum_{s=1}^\tau X_s \ge \frac{\log(N/\delta)}{\psi_k} + \frac{\psi_k V_\tau}{2}} \\
      &\ge \Prob[\bigg]{\sum_{s=1}^\tau X_s \ge \gamma_n\psi_{\min}V_\tau} \\
      &= \Prob[\bigg]{\sum_{s=1}^\tau X_s \ge \sqrt{2\gamma_nV_\tau\log(N/\delta)}}.
    \end{align*}
    To complete the proof, define the stopping time $\tau =
    \min\set{n,\tau_n}$ where
    \begin{align*}
      \tau_n &\defeq \min\set[\Big]{t \le n \given \sum_{s=1}^tX_s \ge \sqrt{2\gamma_nV_t\log(N/\delta)}}.
              \qedhere
    \end{align*}
  \end{proof}
\end{lemma}


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
