\documentclass{article}
\usepackage[utf8]{inputenc}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018
\PassOptionsToPackage{round,colon}{natbib}

% ready for submission
\usepackage{nips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
% \usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\title{Differentially Private Contextual Linear Bandits}
\author{
  Roshan Shariff
  \And
  Or Sheffet
}

\usepackage{newtxtext}
%\usepackage[semibold]{libertine} % a bit lighter than Times--no osf in math
\usepackage[T1]{fontenc} % best for Western European languages
%\usepackage{textcomp} % required to get special symbols
%\usepackage[varqu,varl]{inconsolata} % a typewriter font must be defined
\usepackage{amsmath,mathtools}
\usepackage{amsthm,thmtools,thm-restate}
\usepackage{amssymb} % loads amsfonts
\usepackage{nicefrac}
%\usepackage[libertine,cmintegrals,bigdelims,vvarbb]{newtxmath} %
%replaces some amssymb symbols
\usepackage{newtxmath}
%\usepackage[scr=rsfso]{mathalfa} % Use rsfso to provide mathscr
\usepackage{dsfont}
\usepackage{upgreek}
\usepackage{bm} % load after all math to give access to bold math

\usepackage{microtype}
\usepackage[inline,shortlabels]{enumitem}
\usepackage{setspace}
\usepackage{tikz-cd}
\usepackage{booktabs}
\usepackage[boxed]{algorithm}
\usepackage{algpseudocode}
%\usepackage{fullpage}
\usepackage{color}
\newcommand{\os}[1]{\textcolor{red}{Or's comment:~}#1}
%\usepackage{dblfloatfix}

%% Bibliography/References
%\usepackage[round,colon]{natbib}

%% Cross-references
\usepackage{varioref}
\usepackage[hidelinks]{hyperref}
\usepackage[capitalise]{cleveref}

%% To-do notes
\usepackage[obeyFinal]{todonotes}

% \newcommand{\tinytodo}[2][]{\todo[size=\tiny]{#2}}
\newcommand{\tinytodo}[2][]{\todo[size=\tiny, #1]{\begin{spacing}{1.0}#2\end{spacing}}}
\newcommand{\RStodo}[2][]{\tinytodo[color=red!20, #1]{R:\@#2}} % Roshan
\newcommand{\OStodo}[2][]{\tinytodo[color=blue!20, #1]{Cs: #2}} % Or
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%% Macros

\newcommand{\wildcard}{\mathinner{\,{\cdot}\,}}
\newcommand{\defeq}{\coloneq}
\newcommand{\eqdef}{\eqcolon}
\newcommand{\inv}[1]{#1^{-1}}
\newcommand{\Real}{\mathds{R}}
\newcommand{\Nat}{\mathds{N}}
\newcommand{\Int}{\mathds{Z}}
\newcommand{\mgf}{\mathrm{mgf}}
\newcommand{\UCB}{\operatorname{UCB}}
\renewcommand\mid{\mathinner{\vert}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\rank}{rank}

\newcommand\given[1][\delimsize]{%
  \providecommand{\delimsize}{}
  \nonscript\:#1\vert\allowbreak\nonscript\:\mathopen{}
}

\DeclarePairedDelimiter{\abs}||
\DeclarePairedDelimiter{\paren}()
\DeclarePairedDelimiter{\brck}{[}{]}
\DeclarePairedDelimiterX{\set}[1]\lbrace\rbrace{#1}
\DeclarePairedDelimiter{\ceil}\lceil\rceil
\DeclarePairedDelimiterX{\innerp}[2]\langle\rangle{#1,#2}
\DeclarePairedDelimiterXPP{\Prob}[1]{\mathds{P}}(){}{#1}
\DeclarePairedDelimiterXPP{\PrSet}[1]{\mathds{P}}\{\}{}{#1}
\DeclarePairedDelimiterXPP{\Ex}[1]{\mathds{E}}{[}{]}{}{#1}
\DeclarePairedDelimiterXPP{\Exx}[2]{\mathds{E}_{#1}}{[}{]}{}{#2}
\DeclarePairedDelimiterXPP{\Var}[1]{\mathrm{Var}}{[}{]}{}{#1}
\DeclarePairedDelimiterXPP{\One}[1]{\mathds{1}}\{\}{}{#1}
\DeclarePairedDelimiterXPP{\norm}[2]{}\Vert\Vert{_{#1}}{#2}

%% Other symbols
\newcommand{\A}{\mathcal{A}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\E}{\mathcal{E}}
\providecommand\transp{\top}
\let\transpsymbol\transp
\renewcommand{\transp}[1]{#1^\transpsymbol}
\newcommand{\Aset}[1]{\mathcal{A}_{#1}}
\newcommand{\Dset}[1]{\mathcal{D}_{#1}}
\newcommand{\Cset}[1]{\mathcal{C}_{#1}}

% Theorem environments

\declaretheorem[style=definition]{definition}
\declaretheorem[style=definition]{assumption}
\declaretheorem[style=definition]{theorem}
\declaretheorem[style=definition]{lemma}
\declaretheorem[style=definition]{claim}
\declaretheorem[style=definition]{corollary}

\Crefname{assumption}{Assumption}{Assumptions}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\onehalfspacing

\begin{document}

\maketitle

\begin{abstract}
  We study the contextual linear bandit problem, where an algorithm
  sequentially selects actions that maximize an associated reward,
  which may depend on a provided per-round \emph{context}.  In
  particular, the reward is assumed to be a linear function of a
  \emph{feature vector} that encodes the context and selected action,
  with added stochastic noise.

  In many real-world applications, it is important to maintain the
  privacy of the provided contexts and rewards.  Specifically, the
  algorithm's choice of actions should not reveal too much information
  about the previous contexts and rewards it has learned from.  We
  address this concern by providing a \emph{differentially private}
  algorithm for the contextual linear bandit problem, along with
  bounds on its performance.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

The \emph{multi-armed bandit} is a well-known sequential
decision-making task: in every round $t$, an algorithm chooses an
action (or arm) $a_t\in\A$ and receives a corresponding reward $r_t$.
The objective is to maximize the cumulative reward $\sum_tr_t$ by
learning from the past rewards associated with the different actions.
Many real-world applications, however, are instances of the more
general \emph{contextual} bandit problem, where an additional piece of
context $c_t\in\C$ is available at the beginning of each round.  The
reward is then assumed to depend not only on the selected action but
also on the context.

% Since the agent only observes the reward for actions it takes, this
% is an online learning task with partial information.  Learning the
% rewards associated with all the arms requires an agent to
% \emph{explore} them.  Too much exploration, however, would be
% counter-productive; eventually the algorithm should \emph{exploit}
% the best arms it has found.  The main challenge of bandit strategies
% is to balance exploration and exploitation.

As a motivating example, consider the problem of choosing an
advertisement to be displayed on a web site.  The algorithm receives a
reward when a user clicks on an ad.  The probability of this click can
vary greatly, however, depending on how well the chosen advertisement
matches the contents of the web page and the expressed interests of
the user.  Using this additional context allows an algorithm to make
vastly better decisions than would be possible by observing the
rewards alone, which can only quantify the effectiveness of each ad
without regard to any particular situation.

This example highlights the need to maintain the privacy of the
context and reward: users' interests, web site visits, and clicks are
sensitive personal information.  An algorithm may pick an ad for a
user based on their own information, and it may even learn from
aggregated information to choose better in the future, but an
adversary analyzing these ad choices must not be able to learn too
much about any individual user.

We present algorithms for the contextual bandit problem that are
\emph{differentially private}, a widely adopted notion of privacy
preservation in both academia and industry.  In this framework, the
output of the algorithm does not change in a statistically significant
way when any single context and reward is changed.  An adversary
analyzing this output therefore cannot learn the private information
received by the algorithm on any particular round.  We will see,
however, that contextual bandit algorithms that are differentially
private must essentially ignore their context.  We therefore adopt the
more relaxed notion of \emph{joint differential privacy} which allows
the algorithm to reveal the provided context, but only via its output
for the same round.  All future outputs must still maintain the
privacy of the context and reward.  Intuitively, it is reasonable that
an advertisement shown to a user reveals their own interests, but
those shown to other users do not.

\todo[inline]{Consider non-contextual bandits; only the rewards are
  private.  Does this help with the counter-example against optimism
  in Tor's paper?}

\todo[inline]{Looks like a $d^2$ dependence is necessary: consider
  $\theta^*=\paren{\pm 1/d, \dotsc}$ and the actions are $e_i$.  Then
  you need $d^2\log d$ samples?}

\subsection{Related Work}
\label{sec:related-work}

Differential privacy was first introduced by
\citet{DworkCalibratingNoiseSensitivity2006} and its modern
formulation in \citet{DworkDifferentialPrivacy2006}.
\Citet{DworkAlgorithmicFoundationsDifferential2014} is a comprehensive
overview of the field.

\todo[inline]{Finish this section.}

\subsection{Contributions}
\label{sec:contributions}

\todo[inline]{Finish this section.}


\section{Background and Problem Definition}
\label{sec:background}


\subsection{Stochastic Contextual Linear Bandits}
\label{sec:intro-contextual}

A multi-armed bandit is a sequential decision making task in which, at
every round $t$, an algorithm selects an action $a_t$ from a set $\A$.
Usually, $\A = \set{1,\dotsc,K}$ (the $K$-armed bandit) but we do not
require this; our algorithms can handle other action sets that may be
infinite or uncountable.  The algorithm then receives a \emph{reward}
corresponding to the action it chose.  In the (stationary)
\emph{stochastic multi-armed bandit}, the reward is of the form $r_t =
\mu_{a_t} + \eta_t$, where $\mu_a$ is the expected reward of action $a$
and $\eta_t$ is zero-mean noise.  In other words, the reward is sampled
independently from a distribution that depends only on the selected
action.

In the contextual bandit problem, the algorithm also receives a
\emph{context} $c_t\in\C$ at the beginning of each round.  The reward
is assumed to depend on both the reward and the context.  We will
represent such an algorithm by a function
$A : (\Cset{}\times\Aset{}\times\Real)^* \times \Cset{} \to
\Delta_{\Aset{}}$ whose input is a history of context-action-reward
triples along with a current context, and whose output is a
probability distribution over actions.  The context usually gives
additional information about the actions, in a sense that we will
expand upon below.

To take advantage of the context while choosing actions, we will
assume that the context affects the reward in a linear way, in the
following sense.  Suppose there is a known \emph{feature function}
$\phi:\Cset{}\times\Aset{}\to\Real^d$ that maps every context-action
pair to a $d$-dimensional \emph{feature vector}.  We will assume that
the reward is a linear function of the feature vector, with some added
noise: $r_t = \innerp{\theta^*}{\phi(c_t,a_t)} + \eta_t$ for some
unknown vector $\theta^*\in\Real^d$.

Since all the information the algorithm needs is encoded in the
feature vector of each action, we can now formulate the contextual
bandit problem in the following equivalent way.  Define the decision
set
$\Dset{t} \defeq \set{\phi(c_t,a)\given a\in\Aset{}} \subset \Real^d$.
The context is implicitly encoded in the decision set, and choosing
$x_t\in\Dset{t}$ is equivalent to choosing an action $a_t\in\Aset{}$.
In the contextual stochastic linear bandit framework, at each round:
\begin{enumerate}
\item The agent receives a \emph{decision set} $\mathcal{D}_t \subset
  \Real^d$.
\item The agent chooses an \emph{action} $X_t \in \mathcal{D}_t$.
\item The agent receives a \emph{reward} $Y_t = \innerp{X_t}{\theta^*} + \eta_t$.
\end{enumerate}
The vector $\theta^*\in\Real^d$ is an unknown parameter of the
environment which the agent learns so that it can maximize its reward.

\begin{assumption}[Subgaussian noise]\label{assumption:subgaussian-noise}
  We denote by
  $\mathcal{F}_t =
  \sigma(\mathcal{D}_1,X_1,Y_1,\dotsc,\mathcal{D}_{t-1},X_{t-1},Y_{t-1},\mathcal{D}_t,X_t)$
  all the information available just before the noise $\eta_t$ is
  observed.  We assume that $\eta_t$ is \emph{conditionally
    $\sigma^2$-subgaussian:}
  \begin{align*}
    \Ex{\exp(\lambda\eta_t)\given \mathcal{F}_t} &\le \exp(\lambda^2\sigma^2/2),
    &\text{for all } \lambda\in\Real.
  \end{align*}
\end{assumption}

\subsection{Privacy for Contextual Bandits}

In many applications, the context and reward may be considered private
information about the users of the bandit algorithm.  For example, a
search engine might have a context consisting of a user's search
query, identity, interests, and physical location, while the reward
indicates which search result the user clicked on.  The search engine
should, of course, use the context to answer each query; furthermore,
it should learn from the reward to better respond to future queries
from other users.  However, it should also maintain privacy: its
responses to queries should not reveal \emph{too much} information
about the context and rewards it has learned from.  More precisely, we
want algorithms that are \emph{jointly differentially private} in the
following sense:

\begin{definition}
  A randomized contextual bandit algorithm $A$ is
  \emph{$(\varepsilon,\delta)$-differentially private} if the
  following holds for every $t^*\in\set{1,\dotsc,n}$.  Let
  $H = (c_t,a_t,r_t)_{t=1}^n$ and $H' = (c'_t,a'_t,r'_t)_{t=1}^n$ be
  two sequences of context-action-reward triples that differ only on
  round $t^*$; in other words, $c_t=c'_t$, $a_t=a'_t$, $r_t=r'_t$ for
  $t\neq t^*$.  \todo[inline]{Complete this definition.}
\end{definition}

\todo[inline]{Complete this section.}

\subsection{Differentially Private Linear Regression}
\label{sec:dp-regression}

\subsection{The Tree Mechanism}
\label{sec:tree-mechanism}

\section{Linear UCB with Changing Regularizers}

\todo[inline]{Clean up notation in this section, including time
  indexes for $X$, $Y$, and $\hat\theta$.}

The basis for our differentially private contextual linear bandit
algorithms will be the well-studied LinUCB \todo{citation} algorithm.
At every round, LinUCB constructs an ellipsoidal \emph{confidence set}
$\E_t$ that contains the unknown parameter vector $\theta^*$ with high
probability.  Since $y_t = \innerp{\theta^*}{x_t} + \eta_t$, the center of
the confidence ellipsoid is taken to be the solution of the
regularized linear regression problem
\begin{align*}
  &\min_{\hat\theta \in \Real^d} \frac{1}{2}\norm{}{X\hat\theta - y}^2 + \frac{1}{2}\norm{H_t}{\hat\theta}^2,
  && \text{i.e., } \hat\theta = \inv{(\transp{X}X + H_t)}\transp{X}y,
\end{align*}
where $y\in\Real^t$ is a vector of observed rewards and
$X\in\Real^{t\times d}$ is a matrix of actions selected so far.
Defining $V_t = \transp{X}X + H_t$, the form of the ellipsoid is
\begin{align*}
  \E_t &\defeq \set{\theta\in\Real^d \given \norm{\inv{V_t}}{\theta-\hat\theta}^2 < \beta_t}
\end{align*}
where $\beta_t$ will be defined later in such a way that the ellipsoid
contains $\theta^*$ with high probability.

By using a regularizer matrix $H_t$ that changes at each round, we
allow ourselves to use random regularizers to achieve differential
privacy (as outlined in \cref{sec:dp-regression}).  We start by
analyzing the regret of this algorithm.

\begin{algorithm}
  \caption{Linear UCB with changing regularizers}\label{alg:linucb}
  \begin{algorithmic}
    \For{$t \in 1,2,\dotsc,n$}
    \State Receive decision set $\Dset{t} \subset \Real^d$ and
    regularizer $H_t \in \Real^{d\times d}$.
    \State $V_t \gets \transp{X}X + H_t$
    \State $\hat\theta \gets \inv{V_t}\transp{X}y$
    \State For each $x\in\Dset{t}$, compute
    \begin{align*}
      \UCB_t(x) &\defeq \innerp{\hat\theta}{x} + \sqrt{\beta_t}\norm{\inv{V_t}}{x}
    \end{align*}
    \State $X_t \gets \argmax_{x\in\Dset{t}} \UCB_t(x)$
    \State Choose action $X_t$ and receive reward $Y_t$
    \EndFor
  \end{algorithmic}
\end{algorithm}

We will show a generic regret bound that will depend on certain
assumptions about the decision sets $\Dset{t} \subset \Real^d$,
unknown parameter vector $\theta^*\in\Real^d$, and confidence sets
$\Cset{t}$.

\begin{assumption}\label{assumption:linucb}
  We assume that
  \begin{itemize}
  \item The mean reward is bounded: $\abs{\innerp{x}{\theta^*}} \le B$
    for any $x\in\bigcup_t\Dset{t}$.
  \item The actions are bounded: $\norm{}{x} \le L$ for all
    $x\in\bigcup_t\Dset{t}$.
  \item \os{This isn't an assumption. This is more of the nature of things you want to prove. My suggestion: call a sequence of $\beta$s and $V$s $\delta$-good if for any $t$ and blah-blah it holds that...} The confidence intervals hold with high probability: with
    probability $1-\delta$, for all $t\in[n]$, $x\in\Dset{t}$,
    \begin{align*}
      \UCB_t(x) &\ge \innerp{x}{\theta^*} \ge \UCB_t(x) - 2\sqrt{\beta_{t-1}}\norm{\inv{V_{t-1}}}{x}
      \shortintertext{where}
      \UCB_t(x) &\defeq \max_{\theta\in\Cset{t}} \innerp{x}{\theta}
    \end{align*}
    and $\beta_0,\dotsc,\beta_{n-1}\in\Real$ is a non-decreasing sequence
    with $\beta_{n-1} \ge 1$ and $0 \prec V_0,\dotsc,V_{n-1} \in
    \Real^{d \times d}$ is a sequence of symmetric positive definite matrices.
  \end{itemize}
\end{assumption}

\begin{lemma}\label{lemma:linucb-regret}
  Suppose \cref{assumption:linucb} holds with some
  ${(\beta_t, V_t)}_{t=0}^{n-1}$.  Then the pseudo-regret of Linear
  UCB satisfies
  \begin{align*}
    \widehat{R}_n &\le \sqrt{4n\beta_{n-1} \sum_{t=1}^n\min\set{B^2, \norm{\inv{V_{t-1}}}{X_t}^2}}.
  \end{align*}

  \begin{proof}
    Let $r_t = \max_{x\in\Dset{t}} \innerp{x-X_t}{\theta^*}$ be the
    immediate pseudo-regret suffered at round $t\in[n]$ and let
    $X_t^* = \argmax_{x\in\Dset{t}}\innerp{x}{\theta^*}$ be an optimal
    action for round $t$.  We know that the confidence interval holds
    for $X_t^*$, and also that we choose the action with highest UCB,
    so
    \begin{align*}
      \innerp{X_t^*}{\theta^*} &\le \UCB_t(X_t^*) \le \UCB_t(X_t).
    \end{align*}
    Since the lower confidence bound also holds by our assumption,
    \begin{align*}
      \innerp{X_t}{\theta^*} &\ge \UCB_t(X_t) - 2\sqrt{\beta_{t-1}}\norm{\inv{V_{t-1}}}{X_t}.
    \end{align*}
    Combining these inequalities we get
    \begin{align*}
      r_t &= \innerp{X_t^*}{\theta^*} - \innerp{X_t}{\theta^*} \\
         &\le \UCB_t(X_t) - \innerp{X_t}{\theta^*} \\
         &\le 2\sqrt{\beta_{t-1}}\norm{\inv{V_{t-1}}}{X_t}.
    \end{align*}
    Since we assumed that the mean absolute reward is bounded by $B$
    we also get that $r_t \le 2B$, and since $\beta_{n-1} \ge \max\set{\beta_t, 1}$
    we have
    \begin{align*}
      r_t &\le \min\set{2B, 2\sqrt{\beta_{t-1}}\norm{\inv{V_{t-1}}}{X_t}}
           \le 2\sqrt{\beta_{n-1}} \min\set{B, \norm{\inv{V_{t-1}}}{X_t}}.
    \end{align*}

    Now we apply Jensen's inequality as follows:
    \begin{align*}
      \widehat{R}_n^2 &= n^2 \paren[\Big]{\sum_{t=1}^n \frac{r_t}{n}}^2
                      \le n^2 \sum_{t=1}^n \frac{r_t^2}{n} = n\sum_{t=1}^nr_t^2 \\
      \widehat{R}_n &\le \sqrt{n\sum_{t=1}^n r_t^2} \\
                    &= \sqrt{4n\beta_{n-1} \sum_{t=1}^n\min\set{B^2, \norm{\inv{V_{t-1}}}{X_t}^2}}.
                      \qedhere
    \end{align*}
  \end{proof}
\end{lemma}

\begin{lemma}[Elliptical Potential]\label{lemma:elliptical-potential}
  Let $x_1,\dotsc,x_n \in \Real^d$ and for all $t\in[n]$,
  $U_t = U_0 + \sum_{s=1}^t x_s \transp{x_s}$ and
  $L \ge \max_t\norm{}{x_t}$. Then for any $B > 0$
  \begin{align*}
    \sum_{t=1}^n\min\set{B^2, \norm{\inv{U_{t-1}}}{x_t}^2}
    &\le \max\set{2, B^2}\log\frac{\det U_n}{\det U_0}
      \le \max\set{2, B^2} d\log\frac{\tr U_0+nL^2}{d\det^{1/d} U_0}.
  \end{align*}

  \begin{proof}
    We use the fact that $u \wedge B \le \max\set{2, B}\log(1+u)$ for
    any $u \ge 0$, so that
    \begin{align*}
      \sum_{t=1}^n\min\set{B^2 \wedge \norm{\inv{U_{t-1}}}{x_t}^2}
      &\le \max\set{2, B^2} \sum_{t=1}^n \log(1 + \norm{\inv{U_{t-1}}}{x_t}^2).
    \end{align*}
    We will show that this last summation is $\log(\det U_n/\det
    U_0)$.  For $t \ge 1$ we have
    \begin{align*}
      U_t &= U_{t-1} + x_t\transp{x_t}
           = U_{t-1}^{1/2} (I + U_{t-1}^{-1/2}x_t\transp{x_t}U_{t-1}^{-1/2}) U_{t-1}^{1/2} \\
      \det U_t &= \det U_{t-1}\det(I + U_{t-1}^{-1/2}x_t\transp{x_t}U_{t-1}^{-1/2}).
    \end{align*}

    Consider the eigenvectors of the matrix $I+y\transp{y}$ for an
    arbitrary vector $y\in\Real^d$.  We know that $y$ itself is an
    eigenvector with eigenvalue $1+\norm{}{y}^2$:
    \begin{align*}
      (I + y\transp{y})y &= y + y\innerp{y}{y} = (1+\norm{}{y}^2)y.
    \end{align*}
    Moreover, since $I+y\transp{y}$ is symmetric, every other
    eigenvector $u$ is orthogonal to $y$, so that
    \begin{align*}
      (I + y\transp{y})u &= u + u\innerp{y}{u} = u.
    \end{align*}
    Therefore the only eigenvalues of $I+y\transp{y}$ are
    $1+\norm{}{y}^2$ (with eigenvector $y$) and 1.

    In our case $y = U_{t-1}^{-1/2}x_t$ and $\norm{}{y}^2 =
    \transp{x_t}\inv{U_{t-1}}x_t = \norm{\inv{U_{t-1}}}{x_t}^2$, so we get our
    first inequality:
    \begin{align*}
      \det U_n &= \det U_0 \prod_{t=1}^n(1 + \norm{\inv{U_{t-1}}}{x_t}^2) \\
      \log\frac{\det U_n}{\det U_0} &= \sum_{t=1}^n\log(1+\norm{\inv{U_{t-1}}}{x_t}^2).
    \end{align*}

    To get the second inequality, we apply the arithmetic-geometric
    mean inequality to the eigenvalues $\lambda_i$ of $U_n$:
    \begin{align*}
      \det U_n &= \prod_{i=1}^d \lambda_i
                \le \paren[\Big]{\frac{1}{d} \sum_{i=1}^d \lambda_i}^d
                = \paren{(1/d)\tr U_n}^d
                \le \paren{(\tr U_0 + nL^2)/d}^d \\
      \log\frac{\det U_n}{\det U_0}
              &\le d\log\frac{\tr U_0 + nL^2}{d\det^{1/d}U_0}
                \qedhere
    \end{align*}
  \end{proof}
\end{lemma}


\begin{theorem}[Generic Linear UCB Regret]\label{thm:linucb-regret}
  Suppose \cref{assumption:linucb} holds with some $\beta_t$ and
  $V_t \defeq G_t + H_t$, where
  $G_t \defeq \sum_{s=1}^t X_s\transp{X_s}$ and
  $H_t \succeq H \succ 0$ for some symmetric
  $H_t, H \in \Real^{d\times d}$.  Then the pseudo-regret of Linear UCB
  \begin{align*}
    \widehat{R}_n &\defeq \sum_{t=1}^n \max_{x\in\Dset{t}} \innerp{x}{\theta^*} - \innerp{X_t}{\theta*}
    \shortintertext{satisfies}
    \widehat{R}_n &\le \sqrt{4\max\set{2,B^2}n\beta_{n-1} \log\frac{\det V_n}{\det H}} \\
    \widehat{R}_n &\le \sqrt{4\max\set{2,B^2}dn\beta_{n-1}\log\frac{\tr H + nL^2}{d\det^{1/d} H}}.
  \end{align*}

  \begin{proof}
    From our assumptions and \cref{lemma:linucb-regret}, we know that
    the Linear UCB regret is bounded by
    \begin{align*}
      \widehat{R}_n &\le \sqrt{4n\beta_{n-1} \sum_{t=1}^n\min\set{B^2, \norm{\inv{V_{t-1}}}{X_t}^2}}.
    \end{align*}
    Define $U_t \defeq G_t + H$, so that $U_t \preceq V_t$ and by
    \citet[Theorem~7.8]{ZhangMatrixTheory2011} it follows that
    $\inv{V_t} \preceq \inv{U_t}$, which means $\norm{\inv{V_{t-1}}}{X_t}^2
    \le \norm{\inv{U_{t-1}}}{X_t}^2$.  We now apply
    \cref{lemma:elliptical-potential} to the sequence $U_t$ to get
    \begin{align*}
      \sum_{t=1}^n\min\set{B^2,\norm{\inv{V_{t-1}}}{X_t}^2}
      &\le \sum_{t=1}^n\min\set{B^2,\norm{\inv{U_{t-1}}}{X_t}^2} \\
      &\le \max\set{2, B^2}\log\det\frac{U_n}{U_0} \\
      &\le \max\set{2, B^2}d\log\frac{\tr U_0 + nL^2}{d \det^{1/d} U_0}.
    \end{align*}
    To get the required bounds, it only remains to see that $U_0 = H$
    and that $\det U_n \le \det V_n$ \citep[again
    by][Theorem~7.8]{ZhangMatrixTheory2011}.
  \end{proof}

\end{theorem}


\section{Ellipsoidal Confidence Sets}
\label{sec:ellips-conf-bounds}

Throughout this section we will fix some particular round $t \le n$
and suppress the subscripted $t$.

We will consider confidence sets for the value of $\theta^*$ that are
ellipsoidal with centre $\hat{\theta}$, of the form
$\set{\theta\in\Real^d \given \norm{V}{\theta-\hat{\theta}}^2 \le
  \beta}$.  Then the upper confidence bound of an action $x$ is
\begin{align*}
  \UCB(x) &= \max_{\theta: \norm{V}{\theta-\hat\theta}^2 \le \beta} \innerp{x}{\theta}
\end{align*}
We solve this constrained optimization problem by the method of
Lagrange multipliers:
\begin{align*}
  \mathcal{L}(\theta, \lambda) &= \innerp{x}{\theta} - \lambda(\norm{V}{\theta-\hat\theta}^2 - \beta) \\
  \nabla_\theta\mathcal{L}(\theta, \lambda) &= \transp{x} - \lambda\transp{(\theta-\hat\theta)}V = 0 \\
  \theta &= \hat\theta + \frac{{\inv{V}x}}{\lambda}
  \shortintertext{To find $\lambda$:}
  \beta &= \norm{V}{\theta-\hat\theta}^2
          = \norm[\bigg]{V}{\frac{\inv{V}x}{\lambda}}^2
          = \frac{1}{\lambda^2}\transp{x}\inv{V}x \\
  \lambda &= \norm{\inv{V}}{x} / \sqrt\beta
  \intertext{Which we substitute into the expression above:}
  \UCB(x) &= \innerp[\bigg]{x}{\hat\theta + \frac{\sqrt\beta}{\norm{\inv{V}}{x}}\inv{V}x} \\
          &= \innerp{x}{\hat\theta} + \sqrt\beta \norm{\inv{V}}{x}.
\end{align*}
We have therefore shown that if we construct an ellipsoidal confidence
set with an appropriate $V$ and $\beta$, then the requirements of
\cref{assumption:linucb} will be satisfied with the above $\UCB$
index.

Let $X_s \in \Real^d$ be the action vector selected by the algorithm at
time $s$, so that $X$ is a $t \times d$ matrix.  The \emph{Gram matrix} is given by
$G \defeq \transp{X}X \in \Real^{d \times d}$.  Let $y_s$ be the
reward received at time $s$, so that $y\in\Real^t$.

We will take $\hat{\theta}$ to be a regularized solution of the
least-squares system
\begin{align*}
  X\theta &\approx y,
\end{align*}
regularized by the symmetric positive definite matrix $H \succeq 0$:
\begin{align*}
  \hat{\theta} &= \argmin_\theta \frac{1}{2}\norm{}{X\theta - y}^2 + \frac{1}{2}\norm{H}{\theta}^2 \\
               &= \inv{\paren{\transp{X}X + H}} \transp{X}y \\
               &= \inv{\paren{\transp{X}X + H}} \transp{X} \paren{X\theta^* + \eta} \\
               &= \theta^* + \inv{\paren{\transp{X}X + H}}\paren{\transp{X}\eta - H\theta^*} \\
               &= \theta^* + \inv{V}Z - \inv{V}H\theta^*
\end{align*}
where we used the fact that $y = X\theta^* + \eta$ (and $\eta_s$ is the
noise in the reward at round $s$) and we defined $V \defeq \transp{X}X
+ H$ and $Z\defeq X^T\eta$.

\begin{lemma}\label{lemma:subgaussian-z}
  Under \cref{assumption:subgaussian-noise} and for any
$\lambda\in\Real^d$, the random variable $Z = \transp{X}\eta$ satisfies
  \begin{align*}
    \Ex{\exp(\transp{\lambda}Z)} \le \exp\paren{\norm{G}{\lambda}^2/2}.
  \end{align*}

  \begin{proof}
    We have
    \begin{align*}
      \Ex{\exp(\transp{\lambda}Z)}
      &= \Ex{\exp(\transp{\lambda}\transp{X}\eta)} \\
      &= \Ex[\Big]{\Ex[\Big]{\prod_{s=1}^t \exp\paren{\transp{\lambda}X_s\eta_s} \given \mathcal{F}_t}} \\
      &= \Ex[\Big]{\Ex[\Big]{\exp\paren{\transp{\lambda}X_t\eta_t} \given \mathcal{F}_t} \prod_{s=1}^{t-1} \exp\paren{\transp{\lambda}X_s\eta_s}} \\
      &\le \Ex[\Big]{\exp\paren{{(\transp{\lambda}X_t)}^2/2}
        \prod_{s=1}^{t-1} \exp\paren{\transp{\lambda}X_s\eta_s}}
      &\text{since $\eta_t$ is conditionally 1-subgaussian given $\mathcal{F}_t$}\\
      &\le \Ex[\Big]{\prod_{s=1}^t\exp({(\transp{\lambda}X_s)}^2/2)}
      &\text{similarly conditioning on $\mathcal{F}_{t-1},\mathcal{F}_{t-2},\dotsc$}\\
      &= \Ex[\Big]{\exp\paren[\Big]{\sum_{s=1}^t\frac{\transp{\lambda}X_s\transp{X_s}\lambda}{2}}} \\
      &= \exp\paren[\Big]{\frac{1}{2}\transp{\lambda}\transp{X}X\lambda}
        = \exp(\norm{G}{\lambda}^2/2)
        \qedhere
    \end{align*}
  \end{proof}
\end{lemma}


\begin{lemma}\label{lemma:z-norm-bounded-whp}
  Suppose for any $\lambda\in\Real^d$ the random variable
  $Z=\transp{X}\eta$ satisfies
  $\Ex{\exp(\transp{\lambda}Z)} \le \exp(\norm{G}{\lambda}^2/2)$ (see,
  for example, \cref{lemma:subgaussian-z}).  Let
  $0 \prec H \in \Real^{d\times d}$ be any symmetric positive definite
  matrix.  Then for any $0 < \delta \le 1$, we have
  \begin{align*}
    \Prob[\Bigg]{\norm{\inv{(G+H)}}{Z} \ge \sqrt{2\log\frac{1}{\delta} + \log\frac{\det(G+H)}{\det H}}} &\le \delta.
  \end{align*}

  \begin{proof}
    We define
    \begin{align*}
      M_\lambda &\defeq \exp\paren[\Big]{\transp{\lambda}Z - \frac{1}{2}\transp{\lambda}G\lambda}.
    \end{align*}
    Now consider $\lambda\sim\mathcal{N}(0, \inv{H})$ to be a random
    variable with the density function $h(\lambda)$.  Define
    \begin{align*}
      \bar{M}
      &\defeq \int M_\lambda \,h(\lambda)\,d\lambda \\
      &= \frac{1}{\sqrt{{(2\pi)}^d \det\inv{H}}} \int\exp\paren[\Big]{\transp{\lambda}Z
        - \frac{1}{2}\transp{\lambda}G\lambda
        - \frac{1}{2}\transp{\lambda}H\lambda
        } \,d\lambda.
    \end{align*}
    We complete the square in the integrand:
    \begin{align*}
      \transp{\lambda}Z - \frac{1}{2}\transp{\lambda}G\lambda - \frac{1}{2}\transp{\lambda}H\lambda
      &= \frac{1}{2}\transp{Z}\inv{(G+H)}Z - \frac{1}{2}\transp{(\lambda - \inv{(G+H)}Z)}(G+H)(\lambda-\inv{(G+H)}Z) \\
      &= \frac{1}{2} \norm{\inv{(G+H)}}{Z}^2 - \frac{1}{2}\norm{G+H}{\lambda-\inv{(G+H)}Z}^2
    \end{align*}
    which we substitute into the previous equation to get
    \begin{align*}
      \bar{M}
      &= \frac{\exp\paren[\big]{\frac{1}{2} \norm{\inv{(G+H)}}{Z}^2}}{\sqrt{{(2\pi)}^d \det\inv{H}}}
        \int \exp\paren[\Big]{-\frac{1}{2} \norm{G+H}{\lambda-\inv{(G+H)}Z}^2} \,d\lambda \\
      &= \sqrt{\frac{\det H}{\det(G+H)}} \exp\paren[\Big]{\frac{1}{2}\norm{\inv{(G+H)}}{Z}^2}, \\
      \log\bar{M} &= \frac{1}{2}\norm{\inv{(G+H)}}{Z}^2 + \frac{1}{2}\log\frac{\det H}{\det(G+H)}.
    \end{align*}
    Our assumption gives us
    \begin{align*}
      \Ex{\exp(\transp{\lambda}Z)} &\le \exp\paren[\Big]{\frac{1}{2}\transp{\lambda}G\lambda} \\
      \Ex[\bigg]{\frac{\exp(\transp{\lambda}Z)}{\exp(\frac{1}{2}\transp{\lambda}G\lambda)}}
      &= \Ex{M_\lambda} \le 1, &\text{for all }\lambda\in\Real^d.
    \end{align*}
    Now, by Fubini's theorem we can exchange the expectation with the
    integral over $\lambda$:
    \begin{align*}
      \Ex{\bar{M}} &= \Ex[\Big]{\int M_\lambda\,h(\lambda)\,d\lambda} = \int \Ex{M_\lambda} \,h(\lambda)\,d\lambda \le 1
    \end{align*}
    and use a Chernoff bound:
    \begin{align*}
      \Prob{\log(\bar{M}) \ge u} &\le \exp(-u) \\
      \Prob[\Big]{\frac{1}{2}\norm{\inv{(G+H)}}{Z}^2 \ge u + \frac{1}{2}\log\frac{\det(G+H)}{\det H}} &\le \exp(-u) \\
      \Prob[\Big]{\norm{\inv{(G+H)}}{Z} \ge \sqrt{2u + \log\frac{\det(G+H)}{\det H}}} &\le \exp(-u).
    \end{align*}
    Substituting $u = \log(1/\delta)$ completes the proof.
  \end{proof}

\end{lemma}

%===============================================================================
% \hrule

% We see that
% \begin{align*}
%   \max_{\lambda\in\Real^d} \exp\paren[\Big]{\transp{\lambda}Z - \frac{1}{2}\transp{\lambda}G\lambda}
%   &= \exp\paren[\Big]{\max_{\lambda\in\Real^d} \transp{\lambda}Z - \frac{1}{2}\transp{\lambda}G\lambda}.
% \end{align*}
% The maximizer of this expression is given by taking the gradient and
% setting it to zero:
% \begin{align*}
%   \nabla_\lambda\brck[\Big]{\transp{\lambda}Z - \frac{1}{2}\transp{\lambda}G\lambda}_{\lambda=\lambda^*} = Z - G\lambda^* &= 0 \\
%   \lambda^* &= \inv{G}Z
% \end{align*}
% and thus
% \begin{align*}
%   \max_{\lambda\in\Real^d} \exp\paren[\Big]{\transp{\lambda}Z - \frac{1}{2}\transp{\lambda}G\lambda}
%   &= \exp\paren[\Big]{\transp{Z}\inv{G}Z - \frac{1}{2}\transp{Z}\inv{G}Z} \\
%   &= \exp\paren[\Big]{\frac{1}{2}\norm{\inv{G}}{Z}^2}.
% \end{align*}
% We define
% \begin{align*}
%   M_\lambda \defeq \exp\paren[\Big]{\transp{\lambda}Z - \frac{1}{2}\transp{\lambda}G\lambda}
% \end{align*}
% and use Chernoff's bound:
% \begin{align*}
%   \Pr\paren[\Big]{\frac{1}{2}\norm{\inv{G}}{Z}^2 > u}
%   &= \Pr\paren[\Big]{\max_{\lambda\in\Real^d}M_\lambda > u} \\
%   &\le \exp(-u) \Ex[\Big]{\max_{\lambda\in\Real^d} M_\lambda}.
% \end{align*}
% \hrule
%===============================================================================

\subsection{Ridge Regression}
\label{sec:ridge-regression}

We will now instantiate a concrete algorithm based on \emph{ridge
  regression}, i.e.\ using the regularizer $H=\rho I$ for some
$\rho>0$.  Thus we will have
\begin{align*}
  V_t &\defeq \rho I + \sum_{s=1}^t X_s\transp{X_s}
  \shortintertext{and}
  \hat{\theta}_t - \theta^* &= \inv{V_t}Z_t - \inv{V_t}H\theta^* \\
                          &= \inv{V_t}Z_t - \rho\inv{V_t}\theta^* \\
  V_t^{1/2}(\hat{\theta}_t - \theta^*) &= V_t^{-1/2}Z - \rho V_t^{-1/2}\theta^* \\
  \norm{V_t}{\hat{\theta}_t - \theta^*} &= \norm{}{V_t^{-1/2}Z - \rho V_t^{-1/2}\theta^*} \\
  &\le \norm{\inv{V_t}}{Z} + \rho\norm{\inv{V_t}}{\theta^*}.\
                          &\text{Triangle inequality} \\
  &\le \norm{\inv{V_t}}{Z} + \sqrt\rho \norm{}{\theta^*} &\text{Since } V_t \succeq \rho I.
\end{align*}
We now use \cref{lemma:subgaussian-z,lemma:z-norm-bounded-whp} in our
confidence bound:
\begin{align*}
  \Prob[\Bigg]{\norm{V_t}{\hat{\theta}_t - \theta^*} > \sqrt\rho\norm{}{\theta^*} + \sqrt{2\log\frac{1}{\delta} + \log\frac{\det V_t}{\det \rho I}}}
  &\le \delta.
\end{align*}
We assume that $\norm{}{\theta^*} \le S$.  We use the union bound,
replacing $\delta$ with $\delta/n$ to get a bound that holds uniformly
at every round:
\begin{align*}
  \sqrt{\beta_t} &= \sqrt\rho S + \sqrt{2\log\frac{n}{\delta} + \log\frac{\det V_t}{\det \rho I}}.
\end{align*}
Applying \cref{thm:linucb-regret} gives us the regret bound
\begin{align*}
  \widehat{R}_n
  &\le \sqrt{8dn\beta_{n-1}\log\frac{\tr V_0 + nL^2}{d\det^{1/d} V_0}}
    = \sqrt{8dn\beta_{n-1}\log\frac{d\rho + nL^2}{d\rho}}
  \shortintertext{where}
  \sqrt{\beta_{n-1}}
  &= \sqrt\rho S + \sqrt{2\log\frac{n}{\delta} + \log\frac{\det V_{n-1}}{\det \rho I}} \\
  &\le \sqrt\rho S + \sqrt{2\log\frac{n}{\delta} + d\log\paren*{1 + \frac{nL^2}{d\rho}}}.
\end{align*}
Choosing $\delta=1/n$ and constant $\rho$ gives $\sqrt{\beta_{n-1}} =
O(d^{1/2}\log^{1/2}(n/d))$ and thus the expected regret of Linear UCB
with ellipsoidal confidence sets satisfies
\begin{align*}
  \widehat{R}_n &= O(\beta_n^{1/2}\sqrt{dn\log(n/d)}) = O(d\log(n/d)\sqrt{n}).
\end{align*}


\section{Privacy-Preserving Linear UCB}

The Linear UCB algorithm records its history of actions vectors and
rewards, which it uses to select its next action at every round.  We
will now examine the effects of recording this history in a
privacy-preserving manner.

In particular, Linear UCB uses its history of arms and rewards through
the Gram matrix $\transp{X}X$ and the vector $\transp{X}y$.  If these
quantities preserve privacy, then the Linear UCB algorithm must also
since it is simply post-processing them.

\appendix

% \section{Facts About Random Variables}

% \begin{definition}[Subgaussian random variable]\label{def:subG}
%   A real-valued random variable $X$ is \emph{$\sigma^2$-subgaussian}
%   if its moment-generating function satisfies:
%   \begin{align*}
%     \mgf_X(s) \defeq \Ex{\exp(sX)} &\le \exp(s^2\sigma^2/2).
%   \end{align*}
% \end{definition}

% \begin{definition}[Subexponential random variable]\label{def:subExp}
%   A real-valued random variable $X$ is \emph{$\lambda$-subexponential}
%   if its moment-generating function satisfies:
%   \begin{align*}
%     \mgf_X(s) \defeq \Ex{\exp(sX)} &\le \exp(s^2\lambda^2/2),
%     &\text{for all } \abs{s}\le 1/\lambda.
%   \end{align*}
% \end{definition}

% \begin{definition}[Subgaussian/subexponential random vector]
%   A random vector $X\in\Real^b$ is subgaussian (subexponential) if the
%   random variable $\innerp{v}{X}$ is subgaussian (subexponential) for
%   any $v\in\Real^n$ with $\norm{}{v}=1$.
% \end{definition}

% \begin{definition}[Subgaussian/subexponential random matrix]
%   A random matrix $W\in\Real^{n\times m}$ is subgaussian (subexponential)
%   if the random vector $Wu\in\Real^n$ is subgaussian
%   (subexponential) for any $u\in\Real^m$ with $\norm{}{u}=1$.
% \end{definition}

\section{Proofs}

\begin{lemma}[{\citealp[Lemma~9]{AbbasiYadkoriImprovedAlgorithmsLinear2011}}]\label{lemma:martingale-mgf}
  Let $X_1, X_2, \dotsc \in \Real$ and
  $\sigma_1, \sigma_2, \dotsc \in \Real$ be sequences of random
  variables and let
  $\mathcal{F}_1 \subset \mathcal{F}_2 \subset \dotsb$ be a
  filtration.  Suppose that each $X_t \in \mathcal{F}_t$ and almost
  surely
  $\Ex{\exp(\lambda X_t)\given \mathcal{F}_{t-1}} \le
  \exp(\sigma_t^2\lambda^2/2)$ (i.e. each
  $\sigma_t \in \mathcal{F}_{t-1}$ and $X_t$ is conditionally
  $\sigma_t$-subgaussian given $\mathcal{F}_{t-1}$).  For any
  $\lambda\in\Real$, define
  \begin{align*}
    M_{t,\psi} &\defeq \exp\paren[\bigg]{\sum_{s=1}^t \psi X_s - \frac{1}{2}\sigma_s^2\psi^2}.
  \end{align*}
  Let $\tau$ be a stopping time with respect to
  ${(\mathcal{F}_t)}_{t=1}^\infty$.  Then $M_{\tau,\psi}$ is almost surely
  well-defined and $\Ex{M_{\tau,\psi}} \le 1$.

  \begin{proof}
    We will show that ${(M_{t,\psi})}_{t=1}^\infty$ is a
    supermartingale.  Let
    \begin{align*}
      D_{t,\psi} &\defeq \exp\paren[\bigg]{\psi X_t - \frac{1}{2}\sigma_t^2\psi^2}.
    \end{align*}
    By the conditional subgaussianity of $X_t$, we have
    \begin{align*}
      \Ex{D_{t,\psi}\given\mathcal{F}_{t-1}}
      &= \Ex{\exp(\psi X_t)\given\mathcal{F}_{t-1}}
        \cdot \exp\paren[\bigg]{-\frac{1}{2}\sigma_t^2\psi^2}
      \le \exp\paren[\bigg]{\frac{1}{2}\sigma_t^2\psi^2}
        \cdot \exp\paren[\bigg]{-\frac{1}{2}\sigma_t^2\psi^2}
      = 1.
    \end{align*}
    It is also clear that $D_{t,\psi}$ is $\mathcal{F}_t$-measurable
    (since $X_t,\sigma_t \in \mathcal{F}_t$), and therefore so is
    $M_{t,\psi}$.  Furthermore,
    \begin{align*}
      \Ex{M_{t,\psi}\given\mathcal{F}_{t-1}}
      &= \Ex{M_{t-1,\psi}\cdot D_{t,\psi}\given\mathcal{F}_{t-1}}
        = M_{t-1,\psi} \Ex{D_{t,\psi}\given\mathcal{F}_{t-1}}
        \le M_{t-1,\psi},
    \end{align*}
    showing that $M_{t,\psi}$ is indeed a supermartingale and therefore
    by Doob's optional stopping theorem
    $\Ex{M_{\tau,\psi}} \le \Ex{M_{0,\psi}} = 1$.

    Now, we argue that $M_{\tau,\psi}$ is well-defined.  By the
    convergence theorem for non-negative supermartingales,
    $M_{\infty,\psi} = \lim_{t\to\infty}M_{t,\psi}$ is almost surely
    well-defined.  Hence, $M_{\tau,\psi}$ is almost surely
    well-defined independently of whether $\tau<\infty$ holds or not.
    Next, we show that $\Ex{M_{\tau,\psi}} \le 1$.  For this, let
    $Q_{t,\psi} \defeq M_{\min\{\tau,t\},\psi}$ be a stopped
    version of ${(M_{t,\psi})}_t$.  By Fatou's lemma,
    $\Ex{M_{\tau,\psi}} = \Ex{\liminf_{t\to\infty}Q_{t,\psi}} \le
    \liminf_{t\to\infty}\Ex{Q_{t,\psi}} \le 1$, showing the
    $\Ex{M_{\tau,\psi}} \le 1$ indeed holds.
  \end{proof}
\end{lemma}

\begin{lemma}
  Let $n\in\Nat$ and $\varepsilon>0$ and $\sigma^2>0$.  Let
  $X_1,X_2,\dotsc,X_n\in\Real$ and
  $\sigma_1,\sigma_2,\dotsc,\sigma_n\in\Real$ be sequences of random
  variables and let
  $\mathcal{F}_1 \subset \mathcal{F}_2 \subset \dotsb \subset
  \mathcal{F}_n$ be a filtration.  Suppose that each
  $X_t\in\mathcal{F}_t$ and almost surely each $\sigma_t \le \sigma$
  and each $X_t$ is conditionally $\sigma_t$-subgaussian given
  $\mathcal{F}_{t-1}$.  Then
  \begin{align*}
    \Prob[\Big]{\exists t \le n. \sum_{s=1}^t X_s \ge
    \sqrt{2\gamma_nV_t\log(N/\delta)}}
    &\le \delta,
  \end{align*}
  where
  \begin{align*}
    V_t &\defeq \max\set[\Big]{\varepsilon,\sum_{s=1}^t \sigma_s^2}, &
    \gamma_n &\defeq 1 + \frac{1}{\log(n)}, &
    N &\defeq 1 + \ceil*{\frac{\log(n\sigma^2/\varepsilon)}{\log(\gamma_n)}}.
  \end{align*}

  \begin{proof}
    For $\psi\in\Real$ define
    \begin{align*}
      M_{t,\psi} &\defeq \exp\paren[\bigg]{\sum_{s=1}^t \psi X_s - \frac{1}{2}\psi^2\sigma_s^2}.
    \end{align*}
    If $\tau \le n$ is a stopping time with respect to $\mathcal{F}$,
    then $\Ex{M_{\tau,\psi}} \le 1$ by \cref{lemma:martingale-mgf}.
    Therefore by Markov's inequality,
    \begin{align*}
      \delta/N
      &\ge \Prob{M_{\tau,\psi} \ge N/\delta} \\
      &= \Prob[\bigg]{\exp\paren[\Big]{\sum_{s=1}^\tau \psi X_s - \frac{\psi^2\sigma_s^2}{2}} \ge N/\delta} \\
      &= \Prob[\bigg]{\psi \sum_{s=1}^\tau X_s - \frac{\psi\sigma_s^2}{2} \ge \log(N/\delta)} \\
      &\ge \Prob[\bigg]{\sum_{s=1}^\tau X_s \ge \frac{\log(N/\delta)}{\psi} + \frac{\psi V_\tau}{2}},
      &\text{since } V_\tau \ge \sum_{s=1}^\tau \sigma_s^2.
    \end{align*}
    To get the tightest tail bound, we want to minimize the quantity
    on the right side of the inequality; it attains a minimum of
    $\sqrt{2V_\tau \log(1/\delta)}$ at
    $\psi_{\min} \defeq \sqrt{2\log(N/\delta)/V_\tau}$.  Furthermore,
    for any $\psi\ge\psi_{\min}$ that quantity is at most
    $\psi V_\tau$.

    However, since $V_\tau$ is a random quantity, we cannot simply set
    $\psi \defeq \psi_{\min}$.  Instead, we recognize that
    $V_\tau \in [\varepsilon, n\sigma^2]$ almost surely, and we can
    logarithmically cover this range with the $N$ values
    $\varepsilon\gamma_n^{k-1}$ for $k=1,2,\dotsc,N$; at least one of
    these must lie in the interval $[V_\tau/\gamma_n, V_\tau]$.  Then
    we can define the corresponding values
    \begin{align*}
      \psi_k &\defeq \sqrt{\frac{2\log(N/\delta)}{\varepsilon\gamma_n^{k-1}}},
              &\text{for } k=1,2,\dotsc,N;
    \end{align*}
    there must be some value of $k$ for which
    $\psi_k \in [\psi_{\min}, \gamma_n\psi_{\min}]$.  Using a union bound
    over these $N$ values, we get
    \begin{align*}
      \delta
      &\ge \Prob[\bigg]{\exists k\in[N]. \sum_{s=1}^\tau X_s \ge \frac{\log(N/\delta)}{\psi_k} + \frac{\psi_k V_\tau}{2}} \\
      &\ge \Prob[\bigg]{\sum_{s=1}^\tau X_s \ge \gamma_n\psi_{\min}V_\tau} \\
      &= \Prob[\bigg]{\sum_{s=1}^\tau X_s \ge \sqrt{2\gamma_nV_\tau\log(N/\delta)}}.
    \end{align*}
    To complete the proof, define the stopping time $\tau =
    \min\set{n,\tau_n}$ where
    \begin{align*}
      \tau_n &\defeq \min\set[\Big]{t \le n \given \sum_{s=1}^tX_s \ge \sqrt{2\gamma_nV_t\log(N/\delta)}}.
              \qedhere
    \end{align*}
  \end{proof}
\end{lemma}


\begin{claim}[{\citealp[Theorem~7.8]{ZhangMatrixTheory2011}}]
  If $A \succeq B \succeq 0$, then
  \begin{enumerate}
  \item $\rank(A) \ge \rank(B)$
  \item $\det A \ge \det B$
  \item $\inv{B} \succeq \inv{A}$ if $A$ and $B$ are nonsingular.
  \end{enumerate}
\end{claim}

\bibliographystyle{plainnat}
\bibliography{references,zotero-references}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
