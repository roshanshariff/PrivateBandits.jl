\documentclass{article}
\usepackage[utf8]{inputenc}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018
\PassOptionsToPackage{round,colon}{natbib}

% ready for submission
\usepackage{nips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
% \usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\title{Differentially Private Contextual Linear Bandits}
\author{
  Roshan Shariff
  \And
  Or Sheffet
}

\usepackage{newtxtext}
%\usepackage[semibold]{libertine} % a bit lighter than Times--no osf in math
\usepackage[T1]{fontenc} % best for Western European languages
%\usepackage{textcomp} % required to get special symbols
%\usepackage[varqu,varl]{inconsolata} % a typewriter font must be defined
\usepackage{amsmath,mathtools}
\usepackage{amsthm,thmtools,thm-restate}
\usepackage{amssymb} % loads amsfonts
\usepackage{nicefrac}
%\usepackage[libertine,cmintegrals,bigdelims,vvarbb]{newtxmath} %
%replaces some amssymb symbols
\usepackage{newtxmath}
%\usepackage[scr=rsfso]{mathalfa} % Use rsfso to provide mathscr
\usepackage{dsfont}
\usepackage{upgreek}
\usepackage{bm} % load after all math to give access to bold math

\usepackage{microtype}
\usepackage[inline,shortlabels]{enumitem}
\usepackage{setspace}
\usepackage{tikz-cd}
\usepackage{booktabs}
\usepackage[boxed]{algorithm}
\usepackage{algpseudocode}
%\usepackage{fullpage}
\usepackage{color}
\newcommand{\os}[1]{\textcolor{red}{Or's comment:~\textbf{#1}}}
%\usepackage{dblfloatfix}

%% Bibliography/References
%\usepackage[round,colon]{natbib}

%% Cross-references
\usepackage{varioref}
\usepackage[hidelinks]{hyperref}
\usepackage[capitalise]{cleveref}

%% To-do notes
\usepackage[obeyFinal]{todonotes}

% \newcommand{\tinytodo}[2][]{\todo[size=\tiny]{#2}}
\newcommand{\tinytodo}[2][]{\todo[size=\tiny, #1]{\begin{spacing}{1.0}#2\end{spacing}}}
\newcommand{\RStodo}[2][]{\tinytodo[color=red!20, #1]{R:\@#2}} % Roshan
\newcommand{\OStodo}[2][]{\tinytodo[color=blue!20, #1]{Cs: #2}} % Or
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%% Macros

\newcommand{\wildcard}{\mathinner{\,{\cdot}\,}}
\newcommand{\defeq}{\coloneq}
\newcommand{\eqdef}{\eqcolon}
\newcommand{\inv}[1]{#1^{-1}}
\newcommand{\Real}{\mathds{R}}
\newcommand{\Nat}{\mathds{N}}
\newcommand{\Int}{\mathds{Z}}
\newcommand{\mgf}{\mathrm{mgf}}
\newcommand{\UCB}{\operatorname{UCB}}
\renewcommand\mid{\mathinner{\vert}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\rank}{rank}

\newcommand\given[1][\delimsize]{%
  \providecommand{\delimsize}{}
  \nonscript\:#1\vert\allowbreak\nonscript\:\mathopen{}
}

\DeclarePairedDelimiter{\abs}||
\DeclarePairedDelimiter{\paren}()
\DeclarePairedDelimiter{\brck}{[}{]}
\DeclarePairedDelimiterX{\set}[1]\lbrace\rbrace{#1}
\DeclarePairedDelimiter{\ceil}\lceil\rceil
\DeclarePairedDelimiterX{\innerp}[2]\langle\rangle{#1,#2}
\DeclarePairedDelimiterXPP{\Prob}[1]{\mathds{P}}(){}{#1}
\DeclarePairedDelimiterXPP{\PrSet}[1]{\mathds{P}}\{\}{}{#1}
\DeclarePairedDelimiterXPP{\Ex}[1]{\mathds{E}}{[}{]}{}{#1}
\DeclarePairedDelimiterXPP{\Exx}[2]{\mathds{E}_{#1}}{[}{]}{}{#2}
\DeclarePairedDelimiterXPP{\Var}[1]{\mathrm{Var}}{[}{]}{}{#1}
\DeclarePairedDelimiterXPP{\One}[1]{\mathds{1}}\{\}{}{#1}
\DeclarePairedDelimiterXPP{\norm}[2]{}\Vert\Vert{_{#1}}{#2}

%% Other symbols
\newcommand{\A}{\mathcal{A}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\E}{\mathcal{E}}
\providecommand\transp{\top}
\let\transpsymbol\transp
\renewcommand{\transp}[1]{#1^\transpsymbol}
\newcommand{\Aset}[1]{\mathcal{A}_{#1}}
\newcommand{\Dset}[1]{\mathcal{D}_{#1}}
\newcommand{\Cset}[1]{\mathcal{C}_{#1}}
\newcommand{\Eset}[1]{\mathcal{E}_{#1}}
% Theorem environments

\declaretheorem[style=definition]{definition}
\declaretheorem[style=definition]{assumption}
\declaretheorem[style=definition]{theorem}
\declaretheorem[style=definition]{lemma}
\declaretheorem[style=definition]{claim}
\declaretheorem[style=definition]{corollary}
\declaretheorem[style=definition,numbered=no]{note}

\Crefname{assumption}{Assumption}{Assumptions}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\onehalfspacing

\begin{document}

\maketitle

\begin{abstract}
  We study the contextual linear bandit problem, where an algorithm
  sequentially selects actions that maximize an associated reward,
  which may depend on a provided per-round \emph{context}.  In
  particular, the reward is assumed to be a linear function of a
  \emph{feature vector} that encodes the context and selected action,
  with added stochastic noise.

  In many real-world applications, it is important to maintain the
  privacy of the provided contexts and rewards.  Specifically, the
  algorithm's choice of actions should not reveal too much information
  about the previous contexts and rewards it has learned from.  We
  address this concern by providing a \emph{differentially private}
  algorithm for the contextual linear bandit problem, along with
  bounds on its performance.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

The \emph{stochastic multi-armed bandit} (MAB) is a well-known sequential
decision-making task: in every round $t$, an algorithm chooses an
action (or arm) $a_t\in\A$ and receives a corresponding reward $r_t$ drawn iid from distrbution associate with the action $a_t$.
The objective is to maximize the cumulative reward $\sum_t r_t$ by
learning from the past rewards associated with the different actions.
The \emph{contextual} bandit problem is an extension of MAB, where each day we are also given an arbitrary
context $C_t$, and the reward is now drawn from a distribution that depends on \emph{both} the context and the selected action.

% Since the agent only observes the reward for actions it takes, this
% is an online learning task with partial information.  Learning the
% rewards associated with all the arms requires an agent to
% \emph{explore} them.  Too much exploration, however, would be
% counter-productive; eventually the algorithm should \emph{exploit}
% the best arms it has found.  The main challenge of bandit strategies
% is to balance exploration and exploitation.

As a motivating example, consider the problem of online advertisments. The user provides a context (composed of query word, past history of clicks, website visits etc), and in response the site surfaces a potential ad and receives a reward if the user clicks the ad. Ignoring the context and modeling the problem as a standard MAB (with each possible ad as an action) suffers from the drawback of ignoring the variety of users' preferences; whereas modeling the problem as an independent learning task per user prohibits the ability generalize between users. As a result, we model the problem as a contextual linear-bandit problem. Based on the user-given context, each ad (action) is mapped to some feature vector; yet the click-probability is modeled the same way across all users --- it is assumed to be proportional to the inner-product between the action's feature vector and some target vector $\theta^*$.

The above example motivates the need for privacy in the contextual bandit setting: users' interests, web site visits, and clicks are
sensitive personal information, yet they are all required for making a reasonable prediction.
% An algorithm may pick an ad for a
% user based on their own information, and it may even learn from
% aggregated information to choose better in the future, but an
% adversary analyzing these ad choices must not be able to learn too
% much about any individual user.
In this work, we give the first upper- and lower-bounds for the problem of (joint) \emph{differential private} contextual linear bandits. We present algorithms for the contextual bandit problem that are
\emph{differentially private}, the de-facto gold-standard of privacy
preserving data-analysis in both academia and industry. Standard differential privacy (under continuous observation with event-level privacy) states the
output of the algorithm does not change in a statistically significant
way when any single context and reward is changed. However, as illustrate below, in the contextual bandit setting differential
private states we must essentially ignore each day's context and thus incur linear regret. We therefore adopt the
more relaxed notion of \emph{joint differential privacy} \citep{KearnsMechanismDesign2014} which allows
for the output of time $t$ depend arbitrarily on the context of day $t$, yet have only limited dependency with the context or reward of any other day $t'$. Intuitively, we allow the $t$-th user to observe ads corresponding to her prefernces, without revealing (much) about those preferences to all users at times $t'>t$.\footnote{The guarantee of differential privacy under continuous observation assures that the entire sequence of actions and rewards preserse privacy. Hence, even if all later user collude in an effort to learn user $t$'s context, they still have very limited advantage over a random guess.}

\todo[inline]{Consider non-contextual bandits; only the rewards are
  private.  Does this help with the counter-example against optimism
  in Tor's paper?}

\todo[inline]{Looks like a $d^2$ dependence is necessary: consider
  $\theta^*=\paren{\pm 1/d, \dotsc}$ and the actions are $e_i$.  Then
  you need $d^2\log d$ samples?\\Why? I need further explanation here.}

\subsection{Background and Problem Formulation}
\label{subsec:intro_background}

\paragraph{Stochastic Contextual Linear Bandits.}
The classic MAB problem is a sequential decision making task in which, at
every round $t$, an algorithm selects an action $a_t$ from a fixed set $\A$.
%Usually, $\A = \set{1,\dotsc,K}$ (the $K$-armed bandit) but we do not
%require this; our algorithms can handle other action sets that may be
%infinite or uncountable.
The algorithm then receives a \emph{reward}
corresponding to the action it chose.  In the (stationary)
\emph{stochastic multi-armed bandit}, the reward is of the form $r_t =
\mu_{a_t} + \eta_t$, where $\mu_a$ is the expected reward of action $a$
and $\eta_t$ is zero-mean noise.
%In other words, the reward is sampled independently from a distribution that depends only on the selected action.

In the contextual bandit problem, the algorithm also receives a
\emph{context} $c_t\in\C$ at the beginning of each round, and the reward
is assumed to depend on both the reward and the context.
\os{Is the functional representation necessary?}  We represent such an algorithm by a function
$A : (\Cset{}\times\Aset{}\times\Real)^* \times \Cset{} \to
\Delta_{\Aset{}}$ whose input is a history of context-action-reward
triples along with a current context, and whose output is a
probability distribution over actions.
To take advantage of the context while choosing actions, we will
assume that the context affects the reward in a linear way, in the
following sense.  Suppose there is a known \emph{mapping}
$\phi:\Cset{}\times\Aset{}\to\Real^d$ that maps every context-action
pair to a $d$-dimensional \emph{feature vector}.  We will assume that
the reward is a linear function of the feature vector, with some added
noise: $r_t = \innerp{\theta^*}{\phi(c_t,a_t)} + \eta_t$ for some
unknown vector $\theta^*\in\Real^d$. In fact, we simplify the problem statement to include the \emph{decision set}
$\Dset{t} \defeq \set{\phi(c_t,a)\given a\in\Aset{}} \subset \Real^d$
(where the context is implicitly encoded in the decision set), and so, rather than choosing an action out of $\Aset{}$ the algorithm chooses some
$x_t\in\Dset{t}$.
Thus, the contextual stochastic linear bandit framework is composed of repeated rounds in which
\begin{enumerate}
\item The learner receives an arbitrarily chosen \emph{decision set} $\mathcal{D}_t \subset
  \Real^d$.
\item The learner chooses an \emph{action} $X_t \in \mathcal{D}_t$.
\item The learner receives a  stochastically drawn\emph{reward}  $Y_t = \innerp{X_t}{\theta^*} + \eta_t$.
\end{enumerate}
\os{nitpicking comment: do we want to distinguish scalars/coordinates from vectors and make vectors bold or with bars?} The vector $\theta^*\in\Real^d$ is the key unknown parameter of the
environment which the agent aims to learn so that she can maximize reward. The goal of the learner is thus to minimize her expected regret...\os{Define the objective: minimizing expected regret --- perhaps justify why we chose this objective (standard, implies similar objectives etc)}

\begin{assumption}[Subgaussian noise]\label{assumption:subgaussian-noise}
  We denote by
  $\mathcal{F}_t =
  \sigma(\mathcal{D}_1,X_1,Y_1,\dotsc,\mathcal{D}_{t-1},X_{t-1},Y_{t-1},\mathcal{D}_t,X_t)$
  all the information available just before the noise $\eta_t$ is
  observed.  We assume that $\eta_t$ is \emph{conditionally
    $\sigma^2$-subgaussian:}
  $
    \Ex{\exp(\lambda\eta_t)\given \mathcal{F}_t} \le \exp(\lambda^2\sigma^2/2)$,
    for all  $\lambda\in\Real$
\end{assumption}

\paragraph{Joint Differential Privacy.}
As discussed above, the context and reward may be considered private
information about the users which we wish to keep private from all \emph{other} users. We thus introduce the notion of joint differentially private learners under continuous observation \citep[a combination of the two definitions given in][]{KearnsMechanismDesign2014,DworkContinualObservation2010}. First, we say two sequences $S = \langle (\mathcal{D}_1, Y_1), (\mathcal{D}_2, Y_2), ..., (\mathcal{D}_T, Y_T) \rangle$ and $S' = \langle (\mathcal{D}'_1, Y'_1), ..., (\mathcal{D}'_T, Y'_T) \rangle$ are \emph{$t$-neighbors} if for all $t'\neq t$ it holds that $(\mathcal{D}_t,Y_t) = (\mathcal{D}'_t, Y_t)$.
\os{maybe omit?} We say $S$ and $S'$ are \emph{neighbors} if there exists a $t$ such that the two sequences are $t$-neighbors.
% For example, a
% search engine might have a context consisting of a user's search
% query, identity, interests, and physical location, while the reward
% indicates which search result the user clicked on.  The search engine
% should, of course, use the context to answer each query; furthermore,
% it should learn from the reward to better respond to future queries
% from other users.  However, it should also maintain privacy: its
% responses to queries should not reveal \emph{too much} information
% about the context and rewards it has learned from.  More precisely, we
% want algorithms that are \emph{jointly differentially private} in the
% following sense:

\begin{definition}
\label{def:JDP_continual_observation}
  A randomized algorithm $A$ for the contextual bandit problem is
  \emph{$(\varepsilon,\delta)$-jointly differentially private} (JDP) under continual observation if for any $t$ and any pair of $t$-neighboring sequences $S$ and $S'$, and any subset ${\cal S}_{>t} \subset \mathcal{D}_{t+1} \times \mathcal{D}_{t+2} \times \cdots \times \mathcal{D}_{T}$ of sequence of actions ranging from day $t+1$ to the end of the sequence, it holds that $\Pr[A(S)\in \mathcal{S}_{>t}] \leq e^\varepsilon\Pr[A(S')\in \mathcal{S}'_{>t}] +\delta$.
\end{definition}
Note that the standard definition of differential privacy requires the distribution proximity to hold for any subset of sequences of actions ranging from day $t$ to the end of sequence. However, in our problem formulation, with given decision sets, this notion isn't not even well-defined --- as the decision set of day $t$ is different under $S$ and under $S'$. Therefore, when we discuss the impossibility of regret-minimization under standard differential privacy, we result back to the setting of different contexts with fixed action set. See further details in Section~\ref{sec:lower_bounds}.

\subsection{Our Contribution}
\label{sec:contributions}

In this work, in addition to formulating the definition of JDP under continual observation, we also present a framework for implimenting JDP algorithms for the contextual bandit problem. Not surprisingly, our framework combines the tree-based algorithm of.... with the linear upper-confidence bound (UCB) algorithm. However, for modularity, we chose to analyze a family of linear UCB algorithms where in each day one uses a different regularizer, under the premise that all regularizers



\todo[inline]{Complete this section.}
\section{Preliminaries and Background}
\label{sec:background}

Differential privacy was first introduced by
\citet{DworkCalibratingNoiseSensitivity2006} and its modern
formulation in \citet{DworkDifferentialPrivacy2006}.
\Citet{DworkAlgorithmicFoundationsDifferential2014} is a comprehensive
overview of the field.

\todo[inline]{Finish this section.}


\todo[inline]{Finish this section.}


\subsection{Differentially Private Linear Regression}
\label{sec:dp-regression}

\subsection{The Tree Mechanism}
\label{sec:tree-mechanism}

\section{Differentially Private Linear UCB}

\subsection{Related Work}
\label{sec:related-work}


Our differentially private contextual linear bandit algorithm is based
on the well-studied LinUCB, an adaptation of the Upper Confidence
Bound (UCB) algorithm to stochastic linear bandits
\citep{DaniStochasticLinearOptimization2008,RusmevichientongLinearlyParameterizedBandits2010,AbbasiYadkoriImprovedAlgorithmsLinear2011}.
At every round, LinUCB constructs a \emph{confidence set} $\E_t$
containing the unknown parameter vector $\theta^*$ with high
probability.  It then computes a UCB for the reward of each action in
the decision set $\Dset{t}$, and ``optimistically'' chooses the action
with the highest UCB:
\begin{align}\label{eq:def-ucb}
  x_t &\gets \argmax_{x\in\Dset{t}} \UCB_t(x),
  &\text{where }
  \UCB_t(x) &\defeq \max_{\theta\in\E_t} \innerp{\theta}{x}.
\end{align}
With the rewards being linear with added subgaussian noise (i.e.,
$y_s = \innerp{\theta^*}{X_s} + \eta_s$ for $s=1,\dotsc,t$) it is
natural to centre the confidence set $\E_{t+1}$ on the (regularized)
linear regression estimate:
\begin{align*}
  \hat\theta_t &\defeq \min_{\hat\theta \in \Real^d} \norm{}{X_{1:t}\hat\theta - y_{1:t}}^2 + \norm{H_t}{\hat\theta}^2
  = \inv{(\transp{X_{1:t}}X_{1:t} + H_t)}\transp{X_{1:t}}y_{1:t}.
\end{align*}
The matrix $V_t \defeq G_t + H_t \in \Real^{d\times d}$ is the regularized
version of the Gram matrix $G_t \defeq \transp{X_{1:t}}X_{1:t}$.  As
with LinUCB, we use ellipsoidal confidence sets of the form
\begin{align}\label{eq:def-ellip}
  \E_{t+1} &\defeq \set{\theta\in\Real^d \given \norm{\inv{V_t}}{\theta-\hat\theta_t}^2 \le \beta_t},
\end{align}
where $\beta_t$ scales the size of the ellipsoid and will be calculated
later to yield high-probability confidence sets.

We now discuss our modifications to LinUCB to achieve differential
privacy.  Notice that the history of actions and rewards up to round
$t$ is used only via the confidence set $\E_{t+1}$, which is to say
the Gram matrix $G_t$ and the vector
$u_t \defeq \transp{X_{1:t}}y_{1:t}$ (these also determine $\beta_t$,
as we will see).  By recording this history with differential privacy,
we obtain a linear UCB algorithm that is also differentially private
because it simply post-processes $G_t$ and $u_t$
\citep[see][Proposition~2.1]{DworkAlgorithmicFoundationsDifferential2014}.

Our first modification is to use a regularizer matrix $H_t$ that
changes at each round, which to our knowledge has not been considered
in previous work.  If $H_t$ is a noise matrix appropriately chosen to
ensure differential privacy, then $V_t$ becomes a ``privacy-preserving
approximation'' of $G_t$.  Similarly, our second modification
maintains the privacy of the observed rewards (not just the action
vectors): we replace $u_t \defeq \transp{X_{1:t}}y_{1:t}$ with a
private approximation $\tilde u_t = u_t + h_t$.  These two
modifications result in \cref{alg:linucb}, with confidence sets
$\E_{t+1}$ centered at $\tilde\theta_t \defeq \inv{V_t} \tilde u_t$.

\begin{algorithm}
  \caption{Linear UCB with changing regularizers}\label{alg:linucb}
  \begin{algorithmic}
    \For{each round $t \in 1,2,\dotsc,n$}
    \State $\Dset{t} \gets{}$ decision set ${} \subset \Real^d$.
    \State $V_{t-1} \gets G_{t-1} + H_{t-1}$
    \Comment{privacy-preserving ${} \approx G_{t-1}$}
    \State $\tilde u_{t-1} \gets u_{t-1} + h_{t-1}$
    \Comment{privacy-preserving ${} \approx \transp{X}y$}
    \State $\tilde\theta_{t-1} \gets \inv{V_{t-1}}\tilde u_{t-1}$
    \Comment{privacy-preserving ${} \approx \hat\theta_{t-1}$}
    \For{each action $x\in\Dset{t}$}
    \State $\UCB_t(x) \gets \innerp{\tilde\theta_{t-1}}{x} +
    \sqrt{\beta_{t-1}}\norm{\inv{V_{t-1}}}{x}$
    \Comment{closed form of \eqref{eq:def-ucb} for ellipsoids \eqref{eq:def-ellip}}
    \EndFor
    \State $X_t \gets \argmax_{x\in\Dset{t}} \UCB_t(x)$
    \Comment{Choose action.}
    \State $G_t \gets G_{t-1} + X_t\transp{X_t},
    \quad u_t \gets u_{t-1} + X_ty_t$
    \Comment{Record in privacy-preserving history.}
    \EndFor
  \end{algorithmic}
\end{algorithm}

In the rest of this section, we will consider how to choose $H_t$ and
$h_t$ to ensure differential privacy.  For now, we satisfy ourselves
with the following claim, which is a straightforward application of
the post-processing lemma of differential privacy
\citep[Proposition~2.1]{DworkAlgorithmicFoundationsDifferential2014}:
\begin{claim}
  If the sequence $(V_t,\tilde u_t)_{t=1}^{T-1}$ is
  $(\varepsilon,\delta)$-differentially private with respect to
  $(X_t,y_t)_{t=1}^{T-1}$, then \cref{alg:linucb} is
  $(\varepsilon,\delta)$-jointly differentially private.
\end{claim}

\begin{note}
  \Cref{alg:linucb} is only jointly differentially private even though
  the history maintains full differential privacy.  This is because
  the chosen action depends not only on the past contexts $c_s$
  ($s < t$, via the differentially private history of $X_s$) but also
  on the current context $c_t$ via the decision set $\Dset{t}$.  Since
  this use of $c_t$ is not differentially private, it is revealed by
  the algorithm's chosen $x_t$ (as discussed in
  \cref{sec:priv-contextual}).
\end{note}

\subsection{Regret Bounds}
\label{sec:regret-bounds}

The utility of \cref{alg:linucb} comes from it having a small regret,
which depends on constructing accurate confidence sets that
nevertheless tightly concentrate around the unknown $\theta^*$.  In
other words, the $\beta_t$ should be as small as possible while
accounting for the noise $\eta_t$ and the perturbations caused by
$H_t$ and $h_t$.  We start by giving a ``generic'' regret bound that
holds if the confidence sets are accurate.

\begin{restatable}[Regret of \Cref{alg:linucb}]{theorem}{thmlinucbregret}
  \label{thm:linucb-regret}%
  Suppose the mean rewards are bounded:
  $\abs{\innerp{\theta^*}{x}} \le B$ for all $x\in\bigcup_t\Dset{t}$;
  and $\bar\beta_n \ge \max\set{\beta_0,\dotsc,\beta_{n-1},1}$.  In
  the event that for all rounds $t\in\set{1,\dotsc,n}$ the confidence
  sets $\Eset{t}$ contain $\theta^*$ (i.e.,
  $\norm{V_{t-1}}{\theta^* - \tilde\theta_{t-1}} \le
  \sqrt{\beta_{t-1}}$) and each $V_{t-1} = G_{t-1} + H_{t-1}$ for some
  symmetric $H,H_0,\dotsc,H_{n-1} \in \Real^{d\times d}$ with all
  $H_{t-1} \succeq H \succ 0$, the pseudo-regret of \cref{alg:linucb}
  \begin{align*}
    \widehat R_n
    &\defeq \sum_{t=1}^n \max_{x\in\Dset{t}} \innerp{x}{\theta^*} - \innerp{X_t}{\theta*} \\
    \shortintertext{satisfies}
    \widehat R_n
    &\le 2B\sqrt{2n \bar\beta_n \log\frac{\det V_n}{\det H}}. \\
    \shortintertext{Furthermore, if all $\norm{}{x_t} \le L$}
    \widehat R_n
    &\le 2B\sqrt{2dn \bar\beta_n \log\frac{\tr H + nL^2}{d\det^{1/d} H}}.
  \end{align*}
\end{restatable}


\section{Ellipsoidal Confidence Sets}
\label{sec:ellips-conf-bounds}

Throughout this section we will fix some particular round $t \le n$
and suppress the subscripted $t$.

We will consider confidence sets for the value of $\theta^*$ that are
ellipsoidal with centre $\hat{\theta}$, of the form
$\set{\theta\in\Real^d \given \norm{V}{\theta-\hat{\theta}}^2 \le
  \beta}$.  Then the upper confidence bound of an action $x$ is
\begin{align*}
  \UCB(x) &= \max_{\theta: \norm{V}{\theta-\hat\theta}^2 \le \beta} \innerp{x}{\theta}
\end{align*}
We solve this constrained optimization problem by the method of
Lagrange multipliers:
\begin{align*}
  \mathcal{L}(\theta, \lambda) &= \innerp{x}{\theta} - \lambda(\norm{V}{\theta-\hat\theta}^2 - \beta) \\
  \nabla_\theta\mathcal{L}(\theta, \lambda) &= \transp{x} - \lambda\transp{(\theta-\hat\theta)}V = 0 \\
  \theta &= \hat\theta + \frac{{\inv{V}x}}{\lambda}
  \shortintertext{To find $\lambda$:}
  \beta &= \norm{V}{\theta-\hat\theta}^2
          = \norm[\bigg]{V}{\frac{\inv{V}x}{\lambda}}^2
          = \frac{1}{\lambda^2}\transp{x}\inv{V}x \\
  \lambda &= \norm{\inv{V}}{x} / \sqrt\beta
  \intertext{Which we substitute into the expression above:}
  \UCB(x) &= \innerp[\bigg]{x}{\hat\theta + \frac{\sqrt\beta}{\norm{\inv{V}}{x}}\inv{V}x} \\
          &= \innerp{x}{\hat\theta} + \sqrt\beta \norm{\inv{V}}{x}.
\end{align*}
We have therefore shown that if we construct an ellipsoidal confidence
set with an appropriate $V$ and $\beta$, then the requirements of
\cref{assumption:linucb} will be satisfied with the above $\UCB$
index.

Let $X_s \in \Real^d$ be the action vector selected by the algorithm at
time $s$, so that $X$ is a $t \times d$ matrix.  The \emph{Gram matrix} is given by
$G \defeq \transp{X}X \in \Real^{d \times d}$.  Let $y_s$ be the
reward received at time $s$, so that $y\in\Real^t$.

We will take $\hat{\theta}$ to be a regularized solution of the
least-squares system
\begin{align*}
  X\theta &\approx y,
\end{align*}
regularized by the symmetric positive definite matrix $H \succeq 0$:
\begin{align*}
  \hat{\theta} &= \argmin_\theta \frac{1}{2}\norm{}{X\theta - y}^2 + \frac{1}{2}\norm{H}{\theta}^2 \\
               &= \inv{\paren{\transp{X}X + H}} \transp{X}y \\
               &= \inv{\paren{\transp{X}X + H}} \transp{X} \paren{X\theta^* + \eta} \\
               &= \theta^* + \inv{\paren{\transp{X}X + H}}\paren{\transp{X}\eta - H\theta^*} \\
               &= \theta^* + \inv{V}Z - \inv{V}H\theta^*
\end{align*}
where we used the fact that $y = X\theta^* + \eta$ (and $\eta_s$ is the
noise in the reward at round $s$) and we defined $V \defeq \transp{X}X
+ H$ and $Z\defeq X^T\eta$.

\begin{lemma}\label{lemma:subgaussian-z}
  Under \cref{assumption:subgaussian-noise} and for any
$\lambda\in\Real^d$, the random variable $Z = \transp{X}\eta$ satisfies
  \begin{align*}
    \Ex{\exp(\transp{\lambda}Z)} \le \exp\paren{\norm{G}{\lambda}^2/2}.
  \end{align*}

  \begin{proof}
    We have
    \begin{align*}
      \Ex{\exp(\transp{\lambda}Z)}
      &= \Ex{\exp(\transp{\lambda}\transp{X}\eta)} \\
      &= \Ex[\Big]{\Ex[\Big]{\prod_{s=1}^t \exp\paren{\transp{\lambda}X_s\eta_s} \given \mathcal{F}_t}} \\
      &= \Ex[\Big]{\Ex[\Big]{\exp\paren{\transp{\lambda}X_t\eta_t} \given \mathcal{F}_t} \prod_{s=1}^{t-1} \exp\paren{\transp{\lambda}X_s\eta_s}} \\
      &\le \Ex[\Big]{\exp\paren{{(\transp{\lambda}X_t)}^2/2}
        \prod_{s=1}^{t-1} \exp\paren{\transp{\lambda}X_s\eta_s}}
      &\text{since $\eta_t$ is conditionally 1-subgaussian given $\mathcal{F}_t$}\\
      &\le \Ex[\Big]{\prod_{s=1}^t\exp({(\transp{\lambda}X_s)}^2/2)}
      &\text{similarly conditioning on $\mathcal{F}_{t-1},\mathcal{F}_{t-2},\dotsc$}\\
      &= \Ex[\Big]{\exp\paren[\Big]{\sum_{s=1}^t\frac{\transp{\lambda}X_s\transp{X_s}\lambda}{2}}} \\
      &= \exp\paren[\Big]{\frac{1}{2}\transp{\lambda}\transp{X}X\lambda}
        = \exp(\norm{G}{\lambda}^2/2)
        \qedhere
    \end{align*}
  \end{proof}
\end{lemma}


\begin{lemma}\label{lemma:z-norm-bounded-whp}
  Suppose for any $\lambda\in\Real^d$ the random variable
  $Z=\transp{X}\eta$ satisfies
  $\Ex{\exp(\transp{\lambda}Z)} \le \exp(\norm{G}{\lambda}^2/2)$ (see,
  for example, \cref{lemma:subgaussian-z}).  Let
  $0 \prec H \in \Real^{d\times d}$ be any symmetric positive definite
  matrix.  Then for any $0 < \delta \le 1$, we have
  \begin{align*}
    \Prob[\Bigg]{\norm{\inv{(G+H)}}{Z} \ge \sqrt{2\log\frac{1}{\delta} + \log\frac{\det(G+H)}{\det H}}} &\le \delta.
  \end{align*}

  \begin{proof}
    We define
    \begin{align*}
      M_\lambda &\defeq \exp\paren[\Big]{\transp{\lambda}Z - \frac{1}{2}\transp{\lambda}G\lambda}.
    \end{align*}
    Now consider $\lambda\sim\mathcal{N}(0, \inv{H})$ to be a random
    variable with the density function $h(\lambda)$.  Define
    \begin{align*}
      \bar{M}
      &\defeq \int M_\lambda \,h(\lambda)\,d\lambda \\
      &= \frac{1}{\sqrt{{(2\pi)}^d \det\inv{H}}} \int\exp\paren[\Big]{\transp{\lambda}Z
        - \frac{1}{2}\transp{\lambda}G\lambda
        - \frac{1}{2}\transp{\lambda}H\lambda
        } \,d\lambda.
    \end{align*}
    We complete the square in the integrand:
    \begin{align*}
      \transp{\lambda}Z - \frac{1}{2}\transp{\lambda}G\lambda - \frac{1}{2}\transp{\lambda}H\lambda
      &= \frac{1}{2}\transp{Z}\inv{(G+H)}Z - \frac{1}{2}\transp{(\lambda - \inv{(G+H)}Z)}(G+H)(\lambda-\inv{(G+H)}Z) \\
      &= \frac{1}{2} \norm{\inv{(G+H)}}{Z}^2 - \frac{1}{2}\norm{G+H}{\lambda-\inv{(G+H)}Z}^2
    \end{align*}
    which we substitute into the previous equation to get
    \begin{align*}
      \bar{M}
      &= \frac{\exp\paren[\big]{\frac{1}{2} \norm{\inv{(G+H)}}{Z}^2}}{\sqrt{{(2\pi)}^d \det\inv{H}}}
        \int \exp\paren[\Big]{-\frac{1}{2} \norm{G+H}{\lambda-\inv{(G+H)}Z}^2} \,d\lambda \\
      &= \sqrt{\frac{\det H}{\det(G+H)}} \exp\paren[\Big]{\frac{1}{2}\norm{\inv{(G+H)}}{Z}^2}, \\
      \log\bar{M} &= \frac{1}{2}\norm{\inv{(G+H)}}{Z}^2 + \frac{1}{2}\log\frac{\det H}{\det(G+H)}.
    \end{align*}
    Our assumption gives us
    \begin{align*}
      \Ex{\exp(\transp{\lambda}Z)} &\le \exp\paren[\Big]{\frac{1}{2}\transp{\lambda}G\lambda} \\
      \Ex[\bigg]{\frac{\exp(\transp{\lambda}Z)}{\exp(\frac{1}{2}\transp{\lambda}G\lambda)}}
      &= \Ex{M_\lambda} \le 1, &\text{for all }\lambda\in\Real^d.
    \end{align*}
    Now, by Fubini's theorem we can exchange the expectation with the
    integral over $\lambda$:
    \begin{align*}
      \Ex{\bar{M}} &= \Ex[\Big]{\int M_\lambda\,h(\lambda)\,d\lambda} = \int \Ex{M_\lambda} \,h(\lambda)\,d\lambda \le 1
    \end{align*}
    and use a Chernoff bound:
    \begin{align*}
      \Prob{\log(\bar{M}) \ge u} &\le \exp(-u) \\
      \Prob[\Big]{\frac{1}{2}\norm{\inv{(G+H)}}{Z}^2 \ge u + \frac{1}{2}\log\frac{\det(G+H)}{\det H}} &\le \exp(-u) \\
      \Prob[\Big]{\norm{\inv{(G+H)}}{Z} \ge \sqrt{2u + \log\frac{\det(G+H)}{\det H}}} &\le \exp(-u).
    \end{align*}
    Substituting $u = \log(1/\delta)$ completes the proof.
  \end{proof}

\end{lemma}

%===============================================================================
% \hrule

% We see that
% \begin{align*}
%   \max_{\lambda\in\Real^d} \exp\paren[\Big]{\transp{\lambda}Z - \frac{1}{2}\transp{\lambda}G\lambda}
%   &= \exp\paren[\Big]{\max_{\lambda\in\Real^d} \transp{\lambda}Z - \frac{1}{2}\transp{\lambda}G\lambda}.
% \end{align*}
% The maximizer of this expression is given by taking the gradient and
% setting it to zero:
% \begin{align*}
%   \nabla_\lambda\brck[\Big]{\transp{\lambda}Z - \frac{1}{2}\transp{\lambda}G\lambda}_{\lambda=\lambda^*} = Z - G\lambda^* &= 0 \\
%   \lambda^* &= \inv{G}Z
% \end{align*}
% and thus
% \begin{align*}
%   \max_{\lambda\in\Real^d} \exp\paren[\Big]{\transp{\lambda}Z - \frac{1}{2}\transp{\lambda}G\lambda}
%   &= \exp\paren[\Big]{\transp{Z}\inv{G}Z - \frac{1}{2}\transp{Z}\inv{G}Z} \\
%   &= \exp\paren[\Big]{\frac{1}{2}\norm{\inv{G}}{Z}^2}.
% \end{align*}
% We define
% \begin{align*}
%   M_\lambda \defeq \exp\paren[\Big]{\transp{\lambda}Z - \frac{1}{2}\transp{\lambda}G\lambda}
% \end{align*}
% and use Chernoff's bound:
% \begin{align*}
%   \Pr\paren[\Big]{\frac{1}{2}\norm{\inv{G}}{Z}^2 > u}
%   &= \Pr\paren[\Big]{\max_{\lambda\in\Real^d}M_\lambda > u} \\
%   &\le \exp(-u) \Ex[\Big]{\max_{\lambda\in\Real^d} M_\lambda}.
% \end{align*}
% \hrule
%===============================================================================

\subsection{Ridge Regression}
\label{sec:ridge-regression}

We will now instantiate a concrete algorithm based on \emph{ridge
  regression}, i.e.\ using the regularizer $H=\rho I$ for some
$\rho>0$.  Thus we will have
\begin{align*}
  V_t &\defeq \rho I + \sum_{s=1}^t X_s\transp{X_s}
  \shortintertext{and}
  \hat{\theta}_t - \theta^* &= \inv{V_t}Z_t - \inv{V_t}H\theta^* \\
                          &= \inv{V_t}Z_t - \rho\inv{V_t}\theta^* \\
  V_t^{1/2}(\hat{\theta}_t - \theta^*) &= V_t^{-1/2}Z - \rho V_t^{-1/2}\theta^* \\
  \norm{V_t}{\hat{\theta}_t - \theta^*} &= \norm{}{V_t^{-1/2}Z - \rho V_t^{-1/2}\theta^*} \\
  &\le \norm{\inv{V_t}}{Z} + \rho\norm{\inv{V_t}}{\theta^*}.\
                          &\text{Triangle inequality} \\
  &\le \norm{\inv{V_t}}{Z} + \sqrt\rho \norm{}{\theta^*} &\text{Since } V_t \succeq \rho I.
\end{align*}
We now use \cref{lemma:subgaussian-z,lemma:z-norm-bounded-whp} in our
confidence bound:
\begin{align*}
  \Prob[\Bigg]{\norm{V_t}{\hat{\theta}_t - \theta^*} > \sqrt\rho\norm{}{\theta^*} + \sqrt{2\log\frac{1}{\delta} + \log\frac{\det V_t}{\det \rho I}}}
  &\le \delta.
\end{align*}
We assume that $\norm{}{\theta^*} \le S$.  We use the union bound,
replacing $\delta$ with $\delta/n$ to get a bound that holds uniformly
at every round:
\begin{align*}
  \sqrt{\beta_t} &= \sqrt\rho S + \sqrt{2\log\frac{n}{\delta} + \log\frac{\det V_t}{\det \rho I}}.
\end{align*}
Applying \cref{thm:linucb-regret} gives us the regret bound
\begin{align*}
  \widehat{R}_n
  &\le \sqrt{8dn\beta_{n-1}\log\frac{\tr V_0 + nL^2}{d\det^{1/d} V_0}}
    = \sqrt{8dn\beta_{n-1}\log\frac{d\rho + nL^2}{d\rho}}
  \shortintertext{where}
  \sqrt{\beta_{n-1}}
  &= \sqrt\rho S + \sqrt{2\log\frac{n}{\delta} + \log\frac{\det V_{n-1}}{\det \rho I}} \\
  &\le \sqrt\rho S + \sqrt{2\log\frac{n}{\delta} + d\log\paren*{1 + \frac{nL^2}{d\rho}}}.
\end{align*}
Choosing $\delta=1/n$ and constant $\rho$ gives $\sqrt{\beta_{n-1}} =
O(d^{1/2}\log^{1/2}(n/d))$ and thus the expected regret of Linear UCB
with ellipsoidal confidence sets satisfies
\begin{align*}
  \widehat{R}_n &= O(\beta_n^{1/2}\sqrt{dn\log(n/d)}) = O(d\log(n/d)\sqrt{n}).
\end{align*}


\section{Privacy-Preserving Linear UCB}

The Linear UCB algorithm records its history of actions vectors and
rewards, which it uses to select its next action at every round.  We
will now examine the effects of recording this history in a
privacy-preserving manner.

In particular, Linear UCB uses its history of arms and rewards through
the Gram matrix $\transp{X}X$ and the vector $\transp{X}y$.  If these
quantities preserve privacy, then the Linear UCB algorithm must also
since it is simply post-processing them.

\bibliographystyle{plainnat}
\bibliography{references,zotero-references}

\newpage

\appendix

\section{Supplementary Material}

\subsection{Proof of \Cref{thm:linucb-regret}}

\thmlinucbregret*

\begin{lemma}\label{lemma:linucb-regret}
  Suppose the mean rewards are bounded:
  $\abs{\innerp{\theta^*}{x}} \le B$ for all $x\in\bigcup_t\Dset{t}$;
  all the $V_t \succ 0$ are symmetric; and
  $\bar\beta_n \ge \max\set{\beta_0,\dotsc,\beta_{n-1},1}$.  In the
  event that for all rounds $t\in\set{1,\dotsc,n}$ the confidence sets
  $\Eset{t}$ contain $\theta^*$ (i.e.,
  $\norm{V_{t-1}}{\theta^* - \tilde\theta_{t-1}} \le \sqrt{\beta_{t-1}}$),
  the pseudo-regret of \cref{alg:linucb} satisfies
  \begin{align*}
    \widehat{R}_n &\le \sqrt{4n\bar\beta_n \sum_{t=1}^n\min\set{B^2, \norm{\inv{V_{t-1}}}{x_t}^2}}.
  \end{align*}
\end{lemma}

\begin{proof}
  At every round $t$, \cref{alg:linucb} selects the ``optimistic''
  action $x_t$ satisfying
  \begin{align}\label{eq:optimistic-action}
    (x_t,\bar\theta_t) &= \argmax_{x\in\Dset{t},\theta\in\Eset{t}} \innerp{\theta}{x}.
  \end{align}
  Let $x_t^* = \argmax_{x\in\Dset{t}}\innerp{\theta^*}{x}$ be an optimal
  action and $r_t = \innerp{\theta^*}{x_t^*-x_t}$ be the immediate
  pseudo-regret suffered for round $t$:
  \begin{align*}
    r_t &= \innerp{\theta^*}{x_t^*} - \innerp{\theta^*}{x_t} \\
        &\le \innerp{\bar\theta_t}{x_t} - \innerp{\theta^*}{x_t}
        &\text{from \eqref{eq:optimistic-action} since }
          x_t^*\in\Dset{t}, \theta^*\in\Eset{t} \\
        &= \innerp{\bar\theta_t - \theta^*}{x_t} \\
        &= \innerp{V_{t-1}^{1/2}(\bar\theta_t - \theta^*)}{V_{t-1}^{-1/2}x_t} \\
        &\le \norm{V_{t-1}}{\bar\theta_t - \theta^*}\norm{\inv{V_{t-1}}}{x_t}
        &\text{by Cauchy-Schwarz}\\
        &\le \paren[\big]{\norm{V_{t-1}}{\bar\theta_t - \tilde\theta_{t-1}}
          + \norm{V_{t-1}}{\theta^* - \tilde\theta_{t-1}}}\norm{\inv{V_{t-1}}}{x_t}
        &\text{by the triangle inequality}\\
        &\le 2\sqrt{\beta_{t-1}}\norm{\inv{V_{t-1}}}{x_t}
        &\text{since } \bar\theta_t, \theta^* \in \Eset{t} \\
        &\le 2\sqrt{\beta_{n-1}}\norm{\inv{V_{t-1}}}{x_t}
        &\text{since } (\beta_t)_{t=0}^{n-1} \text{ is non-decreasing.}
  \end{align*}
  From our assumptions that the mean absolute reward is bounded by $B$
  and $\beta_{n-1} \ge 1$, we also get that
  $r_t \le 2B \le 2B\sqrt{\beta_{n-1}}$.  Putting these together,
  \begin{align*}
    r_t &\le 2\sqrt{\beta_{n-1}} \min\set{B, \norm{\inv{V_{t-1}}}{x_t}}.
  \end{align*}
  Now we apply Jensen's inequality as follows:
  \begin{align*}
    \widehat R_n^2 &= n^2 \paren[\Big]{\sum_{t=1}^n \frac{r_t}{n}}^2
                   \le n^2 \sum_{t=1}^n \frac{r_t^2}{n} = n\sum_{t=1}^nr_t^2 \\
    \widehat R_n &\le \sqrt{n\sum_{t=1}^n r_t^2}
                   = \sqrt{4n\beta_{n-1} \sum_{t=1}^n\min\set{B^2, \norm{\inv{V_{t-1}}}{x_t}^2}}.
                   \qedhere
  \end{align*}
\end{proof}

\begin{lemma}[Elliptical Potential]\label{lemma:elliptical-potential}
  Let $x_1,\dotsc,x_n \in \Real^d$ be bounded vectors with
  $\norm{}{x_t} \le L$, $U_0\in\Real^{d\times d}$ a positive definite
  matrix, and $B \ge 0$.  Define
  $U_t \defeq U_0 + \sum_{s=1}^t x_s \transp{x_s}$ for all
  $t\in\set{1,\dotsc,n}$.  Then
  \begin{align*}
    \sum_{t=1}^n\min\set{B^2, \norm{\inv{U_{t-1}}}{x_t}^2}
    &\le 2B^2 \log\frac{\det U_n}{\det U_0}
      \le 2B^2d \log\frac{\tr U_0+nL^2}{d\det^{1/d} U_0}.
  \end{align*}
\end{lemma}

\begin{proof}
  We begin by deriving the first inequality with $B=1$, using the fact
  that $\min\set{1, u} \le 2\log(1+u)$ for any $u \ge 0$:
  \begin{align*}
    \sum_{t=1}^n\min\set{1, \norm{\inv{U_{t-1}}}{x_t}^2}
    &\le 2 \sum_{t=1}^n \log(1 + \norm{\inv{U_{t-1}}}{x_t}^2).
  \end{align*}
  We will show that this last summation is $2\log(\det U_n/\det
  U_0)$.  For $t \ge 1$ we have
  \begin{align*}
    U_t &= U_{t-1} + x_t\transp{x_t}
         = U_{t-1}^{1/2}
         \paren[\big]{I + U_{t-1}^{-1/2}x_t\transp{x_t}U_{t-1}^{-1/2}}
         U_{t-1}^{1/2} \\
    \det U_t &=\det U_{t-1}
              \det\paren[\big]{I + U_{t-1}^{-1/2}x_t\transp{x_t}U_{t-1}^{-1/2}}.
  \end{align*}
  Consider the eigenvectors of the matrix $I+y\transp{y}$ for an
  arbitrary vector $y\in\Real^d$.  We know that $y$ itself is an
  eigenvector with eigenvalue $1+\norm{}{y}^2$:
  \begin{align*}
    (I + y\transp{y})y &= y + y\innerp{y}{y} = (1+\norm{}{y}^2)y.
  \end{align*}
  Moreover, since $I+y\transp{y}$ is symmetric, every other
  eigenvector $u$ is orthogonal to $y$, so that
  \begin{align*}
    (I + y\transp{y})u &= u + u\innerp{y}{u} = u.
  \end{align*}
  Therefore the only eigenvalues of $I+y\transp{y}$ are
  $1+\norm{}{y}^2$ (with eigenvector $y$) and 1.  In our case
  $y = U_{t-1}^{-1/2}x_t$ and
  $\norm{}{y}^2 = \transp{x_t}\inv{U_{t-1}}x_t =
  \norm{\inv{U_{t-1}}}{x_t}^2$, so we get our first inequality:
  \begin{align*}
    \det U_n &= \det U_0 \prod_{t=1}^n(1 + \norm{\inv{U_{t-1}}}{x_t}^2) \\
    2\log\frac{\det U_n}{\det U_0} &= 2\sum_{t=1}^n\log(1+\norm{\inv{U_{t-1}}}{x_t}^2).
  \end{align*}
  For $B \neq 1$, we apply the above result with each $x_t$ scaled by
  $B^{-1}$ and $U_0,\dotsc,U_n$ scaled by $B^{-2}$:
  \begin{align*}
    \sum_{t=1}^n\min\set{B^2,\norm{\inv{U_{t-1}}}{x_t}^2}
    &= B^2 \sum_{t=1}^n\min\set{1,\norm{\inv{U_{t-1}}}{B^{-1}x_t}^2}
      \le 2B^2 \log\frac{\det(B^{-2}U_n)}{\det(B^{-2}U_0)}.
  \end{align*}
  To get the second inequality, we apply the arithmetic-geometric
  mean inequality to the eigenvalues $\lambda_i$ of $U_n$:
  \begin{align*}
    \det U_n &= \prod_{i=1}^d \lambda_i
              \le \paren[\Big]{\frac{1}{d} \sum_{i=1}^d \lambda_i}^d
              = \paren{(1/d)\tr U_n}^d
              \le \paren{(\tr U_0 + nL^2)/d}^d \\
    2B^2 \log\frac{\det U_n}{\det U_0}
            &\le 2B^2d \log\frac{\tr U_0 + nL^2}{d\det^{1/d}U_0}
              \qedhere
  \end{align*}
\end{proof}

\begin{lemma}[{\citealp[Theorem~7.8]{ZhangMatrixTheory2011}}]
  \label{claim:psd-matrix-props}%
  If $A \succeq B \succeq 0$, then
  \begin{enumerate}
  \item $\rank(A) \ge \rank(B)$
  \item $\det A \ge \det B$
  \item $\inv{B} \succeq \inv{A}$ if $A$ and $B$ are nonsingular.
  \end{enumerate}
\end{lemma}

\begin{proof}[Proof of \Cref{thm:linucb-regret}]
  From our assumptions and \cref{lemma:linucb-regret}, we know that
  the Linear UCB regret is bounded by
  \begin{align*}
    \widehat{R}_n &\le \sqrt{4n \bar\beta_n \sum_{t=1}^n\min\set{B^2, \norm{\inv{V_{t-1}}}{X_t}^2}}.
  \end{align*}
  Define $U_t \defeq G_t + H$, so that $U_t \preceq V_t$ and by
  \cref{claim:psd-matrix-props} it follows that
  $\inv{V_t} \preceq \inv{U_t}$, which means
  $\norm{\inv{V_{t-1}}}{X_t}^2 \le \norm{\inv{U_{t-1}}}{X_t}^2$.  We
  now apply \cref{lemma:elliptical-potential} to the sequence $U_t$ to
  get
  \begin{align*}
    \sum_{t=1}^n\min\set{B^2,\norm{\inv{V_{t-1}}}{X_t}^2}
    &\le \sum_{t=1}^n\min\set{B^2,\norm{\inv{U_{t-1}}}{X_t}^2} \\
    &\le 2B^2 \log\det\frac{U_n}{U_0} \\
    &\le 2B^2d \log\frac{\tr U_0 + nL^2}{d \det^{1/d} U_0}.
  \end{align*}
  To get the required bounds, it only remains to see that $U_0 = H$ and
  that $\det U_n \le \det V_n$ (again by
  \cref{claim:psd-matrix-props}).
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \section{Facts About Random Variables}

% \begin{definition}[Subgaussian random variable]\label{def:subG}
%   A real-valued random variable $X$ is \emph{$\sigma^2$-subgaussian}
%   if its moment-generating function satisfies:
%   \begin{align*}
%     \mgf_X(s) \defeq \Ex{\exp(sX)} &\le \exp(s^2\sigma^2/2).
%   \end{align*}
% \end{definition}

% \begin{definition}[Subexponential random variable]\label{def:subExp}
%   A real-valued random variable $X$ is \emph{$\lambda$-subexponential}
%   if its moment-generating function satisfies:
%   \begin{align*}
%     \mgf_X(s) \defeq \Ex{\exp(sX)} &\le \exp(s^2\lambda^2/2),
%     &\text{for all } \abs{s}\le 1/\lambda.
%   \end{align*}
% \end{definition}

% \begin{definition}[Subgaussian/subexponential random vector]
%   A random vector $X\in\Real^b$ is subgaussian (subexponential) if the
%   random variable $\innerp{v}{X}$ is subgaussian (subexponential) for
%   any $v\in\Real^n$ with $\norm{}{v}=1$.
% \end{definition}

% \begin{definition}[Subgaussian/subexponential random matrix]
%   A random matrix $W\in\Real^{n\times m}$ is subgaussian (subexponential)
%   if the random vector $Wu\in\Real^n$ is subgaussian
%   (subexponential) for any $u\in\Real^m$ with $\norm{}{u}=1$.
% \end{definition}

\section{Proofs}

\begin{lemma}[{\citealp[Lemma~9]{AbbasiYadkoriImprovedAlgorithmsLinear2011}}]\label{lemma:martingale-mgf}
  Let $X_1, X_2, \dotsc \in \Real$ and
  $\sigma_1, \sigma_2, \dotsc \in \Real$ be sequences of random
  variables and let
  $\mathcal{F}_1 \subset \mathcal{F}_2 \subset \dotsb$ be a
  filtration.  Suppose that each $X_t \in \mathcal{F}_t$ and almost
  surely
  $\Ex{\exp(\lambda X_t)\given \mathcal{F}_{t-1}} \le
  \exp(\sigma_t^2\lambda^2/2)$ (i.e. each
  $\sigma_t \in \mathcal{F}_{t-1}$ and $X_t$ is conditionally
  $\sigma_t$-subgaussian given $\mathcal{F}_{t-1}$).  For any
  $\lambda\in\Real$, define
  \begin{align*}
    M_{t,\psi} &\defeq \exp\paren[\bigg]{\sum_{s=1}^t \psi X_s - \frac{1}{2}\sigma_s^2\psi^2}.
  \end{align*}
  Let $\tau$ be a stopping time with respect to
  ${(\mathcal{F}_t)}_{t=1}^\infty$.  Then $M_{\tau,\psi}$ is almost surely
  well-defined and $\Ex{M_{\tau,\psi}} \le 1$.

  \begin{proof}
    We will show that ${(M_{t,\psi})}_{t=1}^\infty$ is a
    supermartingale.  Let
    \begin{align*}
      D_{t,\psi} &\defeq \exp\paren[\bigg]{\psi X_t - \frac{1}{2}\sigma_t^2\psi^2}.
    \end{align*}
    By the conditional subgaussianity of $X_t$, we have
    \begin{align*}
      \Ex{D_{t,\psi}\given\mathcal{F}_{t-1}}
      &= \Ex{\exp(\psi X_t)\given\mathcal{F}_{t-1}}
        \cdot \exp\paren[\bigg]{-\frac{1}{2}\sigma_t^2\psi^2}
      \le \exp\paren[\bigg]{\frac{1}{2}\sigma_t^2\psi^2}
        \cdot \exp\paren[\bigg]{-\frac{1}{2}\sigma_t^2\psi^2}
      = 1.
    \end{align*}
    It is also clear that $D_{t,\psi}$ is $\mathcal{F}_t$-measurable
    (since $X_t,\sigma_t \in \mathcal{F}_t$), and therefore so is
    $M_{t,\psi}$.  Furthermore,
    \begin{align*}
      \Ex{M_{t,\psi}\given\mathcal{F}_{t-1}}
      &= \Ex{M_{t-1,\psi}\cdot D_{t,\psi}\given\mathcal{F}_{t-1}}
        = M_{t-1,\psi} \Ex{D_{t,\psi}\given\mathcal{F}_{t-1}}
        \le M_{t-1,\psi},
    \end{align*}
    showing that $M_{t,\psi}$ is indeed a supermartingale and therefore
    by Doob's optional stopping theorem
    $\Ex{M_{\tau,\psi}} \le \Ex{M_{0,\psi}} = 1$.

    Now, we argue that $M_{\tau,\psi}$ is well-defined.  By the
    convergence theorem for non-negative supermartingales,
    $M_{\infty,\psi} = \lim_{t\to\infty}M_{t,\psi}$ is almost surely
    well-defined.  Hence, $M_{\tau,\psi}$ is almost surely
    well-defined independently of whether $\tau<\infty$ holds or not.
    Next, we show that $\Ex{M_{\tau,\psi}} \le 1$.  For this, let
    $Q_{t,\psi} \defeq M_{\min\{\tau,t\},\psi}$ be a stopped
    version of ${(M_{t,\psi})}_t$.  By Fatou's lemma,
    $\Ex{M_{\tau,\psi}} = \Ex{\liminf_{t\to\infty}Q_{t,\psi}} \le
    \liminf_{t\to\infty}\Ex{Q_{t,\psi}} \le 1$, showing the
    $\Ex{M_{\tau,\psi}} \le 1$ indeed holds.
  \end{proof}
\end{lemma}

\begin{lemma}
  Let $n\in\Nat$ and $\varepsilon>0$ and $\sigma^2>0$.  Let
  $X_1,X_2,\dotsc,X_n\in\Real$ and
  $\sigma_1,\sigma_2,\dotsc,\sigma_n\in\Real$ be sequences of random
  variables and let
  $\mathcal{F}_1 \subset \mathcal{F}_2 \subset \dotsb \subset
  \mathcal{F}_n$ be a filtration.  Suppose that each
  $X_t\in\mathcal{F}_t$ and almost surely each $\sigma_t \le \sigma$
  and each $X_t$ is conditionally $\sigma_t$-subgaussian given
  $\mathcal{F}_{t-1}$.  Then
  \begin{align*}
    \Prob[\Big]{\exists t \le n. \sum_{s=1}^t X_s \ge
    \sqrt{2\gamma_nV_t\log(N/\delta)}}
    &\le \delta,
  \end{align*}
  where
  \begin{align*}
    V_t &\defeq \max\set[\Big]{\varepsilon,\sum_{s=1}^t \sigma_s^2}, &
    \gamma_n &\defeq 1 + \frac{1}{\log(n)}, &
    N &\defeq 1 + \ceil*{\frac{\log(n\sigma^2/\varepsilon)}{\log(\gamma_n)}}.
  \end{align*}

  \begin{proof}
    For $\psi\in\Real$ define
    \begin{align*}
      M_{t,\psi} &\defeq \exp\paren[\bigg]{\sum_{s=1}^t \psi X_s - \frac{1}{2}\psi^2\sigma_s^2}.
    \end{align*}
    If $\tau \le n$ is a stopping time with respect to $\mathcal{F}$,
    then $\Ex{M_{\tau,\psi}} \le 1$ by \cref{lemma:martingale-mgf}.
    Therefore by Markov's inequality,
    \begin{align*}
      \delta/N
      &\ge \Prob{M_{\tau,\psi} \ge N/\delta} \\
      &= \Prob[\bigg]{\exp\paren[\Big]{\sum_{s=1}^\tau \psi X_s - \frac{\psi^2\sigma_s^2}{2}} \ge N/\delta} \\
      &= \Prob[\bigg]{\psi \sum_{s=1}^\tau X_s - \frac{\psi\sigma_s^2}{2} \ge \log(N/\delta)} \\
      &\ge \Prob[\bigg]{\sum_{s=1}^\tau X_s \ge \frac{\log(N/\delta)}{\psi} + \frac{\psi V_\tau}{2}},
      &\text{since } V_\tau \ge \sum_{s=1}^\tau \sigma_s^2.
    \end{align*}
    To get the tightest tail bound, we want to minimize the quantity
    on the right side of the inequality; it attains a minimum of
    $\sqrt{2V_\tau \log(1/\delta)}$ at
    $\psi_{\min} \defeq \sqrt{2\log(N/\delta)/V_\tau}$.  Furthermore,
    for any $\psi\ge\psi_{\min}$ that quantity is at most
    $\psi V_\tau$.

    However, since $V_\tau$ is a random quantity, we cannot simply set
    $\psi \defeq \psi_{\min}$.  Instead, we recognize that
    $V_\tau \in [\varepsilon, n\sigma^2]$ almost surely, and we can
    logarithmically cover this range with the $N$ values
    $\varepsilon\gamma_n^{k-1}$ for $k=1,2,\dotsc,N$; at least one of
    these must lie in the interval $[V_\tau/\gamma_n, V_\tau]$.  Then
    we can define the corresponding values
    \begin{align*}
      \psi_k &\defeq \sqrt{\frac{2\log(N/\delta)}{\varepsilon\gamma_n^{k-1}}},
              &\text{for } k=1,2,\dotsc,N;
    \end{align*}
    there must be some value of $k$ for which
    $\psi_k \in [\psi_{\min}, \gamma_n\psi_{\min}]$.  Using a union bound
    over these $N$ values, we get
    \begin{align*}
      \delta
      &\ge \Prob[\bigg]{\exists k\in[N]. \sum_{s=1}^\tau X_s \ge \frac{\log(N/\delta)}{\psi_k} + \frac{\psi_k V_\tau}{2}} \\
      &\ge \Prob[\bigg]{\sum_{s=1}^\tau X_s \ge \gamma_n\psi_{\min}V_\tau} \\
      &= \Prob[\bigg]{\sum_{s=1}^\tau X_s \ge \sqrt{2\gamma_nV_\tau\log(N/\delta)}}.
    \end{align*}
    To complete the proof, define the stopping time $\tau =
    \min\set{n,\tau_n}$ where
    \begin{align*}
      \tau_n &\defeq \min\set[\Big]{t \le n \given \sum_{s=1}^tX_s \ge \sqrt{2\gamma_nV_t\log(N/\delta)}}.
              \qedhere
    \end{align*}
  \end{proof}
\end{lemma}





\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
